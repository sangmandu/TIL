# 16 Tue

## \[AI 스쿨 1기\] 10주차 DAY 2

### NLP : 언어모델 I ~ II

언어 모델

* 목표 : 문장이 일어날 확률을 구하는 것
  * 다음 문장 다음에 이어질 단어는?
  * 다음 두 문장 중 나타날 확률이 높은 것은?
* 왜 필요한가?
  * 기계 번역
    * **high** winds tonight =&gt; **large** winds tonight
  * 맞춤법 검사
    * about fifteen **minuets** from =&gt; about fifteen **minutes** from
  * 음성 인식
    * **I saw a van** =&gt; **eyes awe of an**
* 연속적인 단어들에 확률을 부여하는 모델
  * P\(W\) = P\(w1, w2, w3, ... , wn\)
* 관련된 일 : 연속적인 단어들이 주어졌을 때 그 다음 단어의 확률을 구하는 것
  * P\(wn\) = p\(w1, w2, ... , wn-1\)

P\(W\) 구하기

* 결합확률 구하기
  * P\(its, water, is ,so ,transparent, that\)

조건부 확률 P\(w \| h\)

* P\(the \| its water is so transparent that\) = Count\(its water is so transparent that the\) / Count\(its water is so transparent that\)
* 문제는?
  * 가능한 문장의 개수가 너무 많음
  * 이것을 계산할 수 있는 충분한 양의 데이터를 갖기 어려움
* Markov Assumption
  * 한 단어의 확률은 그 단어 앞에 나타나는 몇 개의 단어들에만 의존한다
  * P\(the \| its water is so transparent that \) $$ \approx $$P\(the \| that\)
  * 또는 P\(the \| its water is so transparent that \) $$ \approx $$P\(the \| transparent that\)

![](../../.gitbook/assets/image%20%28202%29.png)

Unigram 모델

![](../../.gitbook/assets/image%20%2850%29.png)

*  Markov의 가정을 극단적으로 적용한 예
* 생성된 문장예제들이 부자연스러움

Bigram 모델

![](../../.gitbook/assets/image%20%28216%29.png)

* 비교적 Unigram 보다는 자연스러움

N-gram 모델

* trigram, 4-grams, 5-grams
* 멀리 떨어진 단어들간의 관계를 완벽하게 모델링하진 못한다
* 하지만 많은 경우에도 n-gram만으로도 좋은 결과를 얻을 수 있다.

Bigram 확률 계산

![](../../.gitbook/assets/image%20%28181%29.png)

모델 평가

* 외재적 평가
  * extrinsic evaluation
  * 존재 자체가 목표이기에 특정 과제를 위한 부분으로서 사용됨
  * 언어모델이 좋은지를 판단하기 위해 그 과제의 평가지표를 사용
  * ex\) 맞춤법 오류 수정
* 내재적 평가
  * intrinsic evaluation
  * 외재적 평가는 시간이 많이 걸리는 단점이 있음
  * 언어모델이 학습하는 확률자체를 평가할 수 있다 : Perplexity
  * 이 기준으로 얻은 최적의 언어모델이 실제로 최적이 아닐 수도 있다.
  * 학습과정에 버그가 있었는지 빠르게 알 수 있는 용도로 사용

Perplexity

* 좋은 언어 모델이란
  * 테스트 데이터를 높은 확률로 예측하는 모델
  * Perplexity : 확률의 역수를 단어의 개수로 정규화한 값
  * Pexplexity를 최소화하는 것이 확률을 최대화 하는 것

![3&#xBC88;&#xC9F8;&#xB294; bigram &#xD655;&#xB960;](../../.gitbook/assets/image%20%28215%29.png)





