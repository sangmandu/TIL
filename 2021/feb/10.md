# 10 Wed

## \[파이썬 딥러닝 파이토치\] PART 04 Computer Vision

### 03 Data Augmentation <a id="01-cnn"></a>

Data Augmentation

* CNN과 같은 딥러닝 모델은 Graphical Feature를 학습시키는 것이 주목적
* 이러한 목적을 위해 복잡한 문제를 해결하는 복잡한 모델을 많이 사용
* 그러나 데이터는 한정적이기 마련인데 이를 보완하기 위한 방법
* 원본 이미지를 회전, 반전, 확대/축소, 자르기 등의 변형을 통해 데이터의 수를 늘린다,
* 성능이 소폭 상승한다.

Cutout/Cutmix

* Cutout : 원본 이미지의 일부를 검은색으로 칠하는 기법 =&gt; 일정 영역의 픽셀을 0으로 채움 =&gt; Dropout의 효과
* Cutmix : 원본 이미지의 일부를 다른 라벨의 이미지로 채우는 기법 =&gt; 해당 이미지를 확률로 라벨링해 학습을 진행
* Data Augmentation 보다 성능이 뛰어나다.

### 04 CNN Architecture

ImageNet

* 이미지 분류 모델을 측정하기 위한 데이터로 가장 많이 사용
* 학습 데이터만 총 138GB
* 2 만 개 이상의 클래스와 약 1,400만 장의 이미지로 구성

Network Architecture

* 네트워크를 깊게 쌓으면서 과적합을 방지하고자 하는 연구가 계속되어왔음.
* LeNet
  * 최초의 CNN 모델
* AlexNet
  * 2012 ILSVRC 대회 우승 모델
  * LeNet과 크게 다르지 않음
  * ReLU, Dropout, Data Augmentation 사용
* VGG
  * 2014 ILSVRC 대회 준우승 모델
  * 3 X 3 Conv. 를 깊게 중첩한다는 것이 특징
  * 깊이에 따라 VGG16, VGG19 등
* GoogLeNet
  * 2014 ILSVRC 대회 우승 모델
  * Inception Model 로도 불리며 CNN + Inception 개념을 도입한 복잡한 형태의 모델
  * 구글에서 제안한 모델로 Google + LeNet을 합친 말
  * 일반적인 CNN은 Conv. 뒤 Pool.을 거치는 것이 일반적인데 Inception Model은 한 층에서 서로 다른 연산을 거친 후 Feature Map을 다시 합치는 방식 =&gt; 작은 규모의 특징과 큰 규모의 특징을 한번에 학습할 수 있다는 장점
  * 마지막 FC 에서 Global Average Pooling\(GAP\)으로 대체해 파라미터의 수를 크게 줄이는 효과를 얻었다. GAP는 Feature Map에 대해 각각의 값을 평균내 연결해주는 것.
* ResNet
  * 'Residual Network'의 약자로 MS에서 제안
  * 2015 ILSVRC 대회 우승 모델
  * 현재까지도 이미지 분류의 기본 모델
  * Residual Block이라는 개념을 도입했으며 이전 층의 특징맵을 다음 층의 특징 맵에 더해주는 개념 =&gt; Skip Connection이라 한다. Gradient Vanishing 문제를 완화
* DenseNet
  * ResNet의 확장된 버전
  * Skip Connection을 모든 층에 적용
  * 학술적으로는 ResNet이 더 가치 있다고 여겨짐

