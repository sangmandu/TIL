# 18 Thu

## \[AI 스쿨 1기\] 10주차 DAY 4

### NLP : 단어 임베딩 I

단어의 의미를 어떻게 나타낼 것인가?

* 글자의 나열
* One hot encoding
* 좋은 표현방식 : 단어관의 관계를 잘 표현할 수 있어야함

단어의 의미

* 어근\(lemma\)
* 의미\(sense\)
* ex\) mouse
  * 쥐
  * 컴퓨터 장치
* ex\) mouse, mice
  * 하나의 어근을 가지고있음
* 컴퓨터에서 사용하기 쉬운 형태가 아님

동의어

* 동의어가 되기 위해서는 사전적으로 같은 의미
* 문장 속에 단어를 다른 단어로 대체 했을 때 의미가 달라지지않고 자연스럽다면 동의어
* 그렇지만 항상 그 단어로 대체할 수 있는 것은 아니다
* ex\) H2O - water

유사성

* 유사한 의미를 가짐
* ex\) car - bicycle, cow - horse
* 기계적으로 유사도를 비교하는 것은 쉽지 않음

연관성

* 단어들은 의미의 유사성 외에도 다양한 방식으로 연관될 수 있음
* Semantic Field
  * 특정한 주제나 영역을 공유하는 단어들
  * ex\) hospital : surgeon, scalpel, nurse
  * ex\) restaurants : waiter, menu, plate
* Semantic Frame
  * 특정 행위에 참여하는 주체들의 역할에 관한 단어들
  * ex\) 상거래 행위에 참여하는 주체들 : buy, sell, pay

벡터로 의미 표현하기

* 단어들은 주변의 환경에 의해 의미가 결정된다
  * 주변의 환경 : 주변의 단어들의 분포
* 만약 A와 B가 동일한 주변 단어들의 분포를 가지고 있다면 두 단어는 유사어이다.
* 단어의 의미를 분포적 유사성을 사용해 표현하고자 한다.
* 벡터로 표현된 단어를 임베딩이라고 부른다. 보통은 밀집벡터인 경우를 임베딩이라고 부른다.
  * 밀집되었다는 뜻은 값이 0이 아니라는 것
  * 반대로 희소벡터는 대부분의 벡터가 0이란 뜻
* 최근 NLP 방법들은 모두 임베딩을 사용해서 단어의 의미를 표현한다.

왜 임베딩을 사용하는가?

* 임베딩을 사용하지 않는 경우
  * 각 속성은 한 단어의 존재 유무
  * 학습데이터와 테스트데이터에 동일한 단어가 나타나지 않으면 예측 결과가 좋지 못함
* 임베딩을 사용하는 경우
  * 각 속성은 단어임베딩 벡터
  * 단어의 존재유무보다 벡터의 속성이 중요하다
  * 학습 데이터
    * GOOD : \[0.9, 0.1, -0.5\]
    * NICE : \[1.1, 0.9, 1.2\]
    * 첫번째 벡터의 값이 둘 다 높다 = &gt; 좋다라는 클래스 C1과 관련이 있을 가능성이 높음
  * 테스트 데이터
    * TERRIFIC : \[0.95, -0.2, 0.1\]
    * 학습 데이터에는 한번도 나타나지 않은 단어일 경우, 이 단어의 의미가 긍정적인지 부정적인지는 알 수 없지만 임베딩으로 학습하는 경우에는 C1의 값이 1에 가까우므로 좋은 의미라는 것을 알 수 있음

임베딩의 종류

* 희소벡터
  * tf-idf
  * Vector propagation : 검색엔진을 위한 질의어, 문서 표현
* 밀집벡터
  * Word2vec : 학습이 간편함 
  * Glove





### NLP : 단어 임베딩 II

Term-document 행렬

* 각 문서는 단어들의 벡터로 표현된다

벡터의 유사도 계산하기

![](../../.gitbook/assets/image%20%28269%29.png)

 TF-IDF

* 단어의 빈도수를 그대로 사용할 때의 문제점
  * the, it, they등의 단어들은 의미를 구별하는데 도움이 되지 않는다
* tf-idf
  * 기존 빈도수만을 구하는 방법에서 보정을 하는 방법
  * 문서 d내에 단어 t의 새로운 가중치 값을 계산한다

![](../../.gitbook/assets/image%20%28270%29.png)

![](../../.gitbook/assets/image%20%28271%29.png)

* 문서에 나오는 단어의 빈도수
* 근데 이것을 그대로 사용하기보다는 로그값을 적용해서 정규화\(값이 너무 크게 올라가는 것을 방지\)
* +1은 무한대로 올라가는 것을 방지

![](../../.gitbook/assets/image%20%28267%29.png)

* df\_t = 단어 t를 포함하는 문서들의 개수
* idf\_t = df\_t _의 역수_

![](../../.gitbook/assets/image%20%28266%29.png)

tf-idf vector

* 길다 : 20,000 ~ 50,000
* 희소성 \(대부분의 원소가 0\)

Word2vec, Glove

* 짧다 : 50 ~ 1,000
* 밀집성 \(대부분의 원소가 0이 아님\)

Dense Vectors

* dense가 선호되는 이유
  * 더 적은 개수의 학습 파라미터 수반
  * 더 나은 일반화 능력
  * 동의어와 유사어를 더 잘 표현



### NLP : 단어 임베딩 III

Word2vec

* 주어진 단어 w를 인접한 단어들의 빈도수로 나타내는 대신, 주변 단어를 예측하는 분류기를 학습하자!
  * 단어 w가 주어졌을 때 단어 c가 주변에 나타날 확률은?
* 목표는 모델의 최종예측값이 아니라 모델 내 단어 w의 가중치벡터
* Self-supervision
  * 이 모델을 학습하기 위한 목표값은 이미 데이터내에 존재
  * 사람이 수동으로 레이블을 생성할 필요가 없음

Skip-Gram

* 한 단어가 주어졌을 때 그 주변 단어를 예측할 확률을 최대화하는 모델
* 다음과 같은 확률모델을 가정한다

![](../../.gitbook/assets/image%20%28268%29.png)

* 문제점은 분모 계산량이 많다
* 해결책
  * Noise-constrastive estimation : Normalization constant를 하나의 파라미터로 학습한다. 이진 분류 문제에 해당하는 새로운 목표함수를 최적화 시킨다. 이렇게 해서 얻어지는 파라미터들이 원래 likelihood의 최적해를 근사한다는 것이 증명된다.
  * 이것을 조금 더 단순화시키면 negative sampling이 된다.
  * Word2vec은 negative sampling을 사용한다.

### NLP : 단어 임베딩 IV

Word2vec 학습과정 요약

* \|V\| 개의 d차원 임베딩을 랜덤하게 초기화
* 주변 단어들의 쌍을 positive example로 생성
* 빈도수에 의해 추출된 단어들의 쌍을 negative example로 생성
* 위 데이터를 사용해 분류기 학습
* 학습된 임베딩 w가 최종결과물

