# 18 Thu

## \[AI 스쿨 1기\] 10주차 DAY 4

### NLP : 단어 임베딩 I

단어의 의미를 어떻게 나타낼 것인가?

* 글자의 나열
* One hot encoding
* 좋은 표현방식 : 단어관의 관계를 잘 표현할 수 있어야함

단어의 의미

* 어근\(lemma\)
* 의미\(sense\)
* ex\) mouse
  * 쥐
  * 컴퓨터 장치
* ex\) mouse, mice
  * 하나의 어근을 가지고있음
* 컴퓨터에서 사용하기 쉬운 형태가 아님

동의어

* 동의어가 되기 위해서는 사전적으로 같은 의미
* 문장 속에 단어를 다른 단어로 대체 했을 때 의미가 달라지지않고 자연스럽다면 동의어
* 그렇지만 항상 그 단어로 대체할 수 있는 것은 아니다
* ex\) H2O - water

유사성

* 유사한 의미를 가짐
* ex\) car - bicycle, cow - horse
* 기계적으로 유사도를 비교하는 것은 쉽지 않음

연관성

* 단어들은 의미의 유사성 외에도 다양한 방식으로 연관될 수 있음
* Semantic Field
  * 특정한 주제나 영역을 공유하는 단어들
  * ex\) hospital : surgeon, scalpel, nurse
  * ex\) restaurants : waiter, menu, plate
* Semantic Frame
  * 특정 행위에 참여하는 주체들의 역할에 관한 단어들
  * ex\) 상거래 행위에 참여하는 주체들 : buy, sell, pay

벡터로 의미 표현하기

* 단어들은 주변의 환경에 의해 의미가 결정된다
  * 주변의 환경 : 주변의 단어들의 분포
* 만약 A와 B가 동일한 주변 단어들의 분포를 가지고 있다면 두 단어는 유사어이다.
* 단어의 의미를 분포적 유사성을 사용해 표현하고자 한다.
* 벡터로 표현된 단어를 임베딩이라고 부른다. 보통은 밀집벡터인 경우를 임베딩이라고 부른다.
  * 밀집되었다는 뜻은 값이 0이 아니라는 것
  * 반대로 희소벡터는 대부분의 벡터가 0이란 뜻
* 최근 NLP 방법들은 모두 임베딩을 사용해서 단어의 의미를 표현한다.

왜 임베딩을 사용하는가?

* 임베딩을 사용하지 않는 경우
  * 각 속성은 한 단어의 존재 유무
  * 학습데이터와 테스트데이터에 동일한 단어가 나타나지 않으면 예측 결과가 좋지 못함
* 임베딩을 사용하는 경우
  * 각 속성은 단어임베딩 벡터
  * 단어의 존재유무보다 벡터의 속성이 중요하다
  * 학습 데이터
    * GOOD : \[0.9, 0.1, -0.5\]
    * NICE : \[1.1, 0.9, 1.2\]
    * 첫번째 벡터의 값이 둘 다 높다 = &gt; 좋다라는 클래스 C1과 관련이 있을 가능성이 높음
  * 테스트 데이터
    * TERRIFIC : \[0.95, -0.2, 0.1\]
    * 학습 데이터에는 한번도 나타나지 않은 단어일 경우, 이 단어의 의미가 긍정적인지 부정적인지는 알 수 없지만 임베딩으로 학습하는 경우에는 C1의 값이 1에 가까우므로 좋은 의미라는 것을 알 수 있음

임베딩의 종류

* 희소벡터
  * tf-idf
  * Vector propagation : 검색엔진을 위한 질의어, 문서 표현
* 밀집벡터
  * Word2vec : 학습이 간편함 
  * Glove





### NLP : 단어 임베딩 II

Term-document 행렬

* 각 문서는 단어들의 벡터로 표현된다

벡터의 유사도 계산하기

![](../../.gitbook/assets/image%20%28268%29.png)

 TF-IDF

* 단어의 빈도수를 그대로 사용할 때의 문제점
  * the, it, they등의 단어들은 의미를 구별하는데 도움이 되지 않는다
* tf-idf
  * 기존 빈도수만을 구하는 방법에서 보정을 하는 방법
  * 문서 d내에 단어 t의 새로운 가중치 값을 계산한다

![](../../.gitbook/assets/image%20%28269%29.png)

![](../../.gitbook/assets/image%20%28270%29.png)

* 문서에 나오는 단어의 빈도수
* 근데 이것을 그대로 사용하기보다는 로그값을 적용해서 정규화\(값이 너무 크게 올라가는 것을 방지\)
* +1은 무한대로 올라가는 것을 방지

![](../../.gitbook/assets/image%20%28267%29.png)

* df\_t = 단어 t를 포함하는 문서들의 개수
* idf\_t = df\_t _의 역수_

![](../../.gitbook/assets/image%20%28266%29.png)



### NLP : 단어 임베딩 III





### NLP : 단어 임베딩 IV

