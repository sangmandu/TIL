# 13 Sat

## \[인공지능을 위한 선형대수\] CHAPTER 3. Least Square <a id="ai-1-5-day-3"></a>

### Orthogonal Projection I

Prjection Perspective

* 한 평면으로 수직인 수선의 발

![](../../.gitbook/assets/image%20%28243%29.png)

Orthonormal Sets

* 정의 : 각 벡터가 모두 수직 =&gt; 임의의 두개의 벡터의 내적이 항상 0

Ortho. Basis

* 각 평면의 기저벡터 찾기
* Gram-Schmidt process를 통해 찾을 수 있음

![](../../.gitbook/assets/image%20%28247%29.png)

* 기저벡터식은 위와 같으며 아래 식은 u가 1일 경우\(기저벡터일 경우\(의 식

![](../../.gitbook/assets/image%20%28251%29.png)

* 두 개의 벡터 스페이스가 있을 경우 y\_hat은 위와 같이 표현될 수 있음
* 근데 우리는 y에서 y1-hat 이랑 y2-hat을 구하려는게 아니라 붕 떠있는 점 y'에서 바로 구하고 싶은 것
* y는 단순히 y1-hat과 y2-hat의 합

### Orthogonal Projection II

잘 이해 못했다. 어렵다

Orthogonal Projection

![](../../.gitbook/assets/image%20%28253%29.png)



### 그람-슈미트 직교화와 QR 분해

수직이 아니고 선형적으로 독립한 두 벡터를 수직한 벡터로 나타내기

![](../../.gitbook/assets/image%20%28255%29.png)

* x1은 $$ \sqrt{45} $$로 나누고 x2는 $$ \sqrt{9} $$로 나누기
  * 놈이 1인 기저벡터로 만드는 것
  * 스팬은 그대로 유지

어렵다



