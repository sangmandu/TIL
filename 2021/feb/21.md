# 21 Sun

## \[파이썬 딥러닝 파이토치\] PART 05 NLP

### 01 Data & Task <a id="01-cnn"></a>

자연어 처리

* Natural Language Processing
* 자연어를 이해하는 영역인 자연어 이해와 모델이 자연어를 생성하는 영역인 자연어 생성으로 나눠 표현하기도 한다
  * NLU : Understanding
    * 주어진 Text의 의미를 파악
  * NLG : Generation
    * 주어진 의미에 대한 자연스러운 Text를 만들어냄
  * NLP = NLU + NLG
* NLP는 다양한 Task를 가지고 있고 각 T별로 데이터도 다양하다
  * Task를 먼저 정한 뒤 데이터를 Vectorization 한다

감정 분석

* Sentiment Analysis
* 문장에 대한 특정 감정을 분류해내는 문제
* 긍정/부정을 판단하는 문제지만 5점 척도로 접근하는 경우도 있다

요약

* Text에서 중요한 부분을 찾아내는 Extractive Summarization과 Text의 의미를 완전히 이해해 새로운 문장을 만들어내는 Abstractive Summarization으로 나뉜다.
* 후자가 더 어려운 Task이다.
* 모델링의 어려움 뿐만 아니라 정답 요약문을 만들어야 하는 데이터 수급 문제, 정답 평가의 어려움도 있어 NLP에서 난이도가 높은 편에 속하는 Task이다.

기계 번역

* NLP의 다양한 Task 중 서비스와 직접적인 연관이 있는 영역
* 결과에 대한 평가의 어려움을 가지고 있다
* 언어의 종류에 따른 데이터 불균형이 심해 많은 연구가 필요한 분야

질문 응답

* 주어진 문서를 이해하고 문서 속 정보에 대한 질문의 답을 이끌어 내는 Task

### 02 문자를 숫자로 표현하는 방법

문장의 의미를 만드는 방식

1. 문장을 의미 있는 부분으로 나눈다
2. 나눠진 의미 있는 부분을 숫자로 바꿔 문장을 숫자로 표현한다

* 간단한 방법은 띄어쓰기를 이용하는 것
* "나는 책상 위에 사과를 먹었다"
  * 나는, 책상, 위에, 사과를, 먹었다
* 이와 같이 의미 있는 부분으로 나누는 과정을 Tokenization이라고 하며 쪼개진 부분을 Token이라고 한다. Lexical Analysis라고도 한다. =&gt; 연속된 문자의 나열을 적절하게 의미를 지닌 부분의 나열로 바꾸는 과정

Out of Vocabulary\(OOV\)

* Token을 저장해둔 Vocabulary에 Token이 없어서 처음 본 Token이 나오는 현상을 OOV라고 한다.
* 이럴 때를 대비해 특수한 Token인 &lt;unk&gt;을 만들어 처음 보는 Token은 &lt;unk&gt;로 변환하도록 처리
* Token을 모아 놓은 문장의 모음을 말뭉치, Corpus라고 한다
* 그러나 이러한 말뭉치가 많아지면 메모리와 모델의 사이즈가 커지기 때문에 효율적인 방법이 필요하다
* 예를 들어 현준이에게, 현준한테는 동일한 의미이므로 이러한 중복적인 의미를 가진 여러 단어를 효율적으로 관리하기 위해서는 띄어쓰기보다 더 좋은 Tokenizer가 필요

Byte Pair Encoding\(BPE\)

* Character based tokenization
  * 띄어쓰기가 아니라 글자를 Token으로 사용
  * 장점
    * 사전의 크기가 줄어드는 장점이 있음
    * OOV 현상을 없앨 수 있다.
  * 단점
    * 표현법에 대한 학습이 어렵다
    * 글자 하나는 보통 의미를 가지고 있지 않고 연속된 나열이 특정 의미를 가지기 때문에 패턴으로 학습해 의미를 만들어야함 =&gt; 쉬운 일은 아님
* n-gram Tokenization
  * 1개 단위가 아니라 여러개의 연속된 윈도우를 단위로 살펴보는 개념
  * n에 따라 uni, bi, tri-gram으로 나뉘며 n &gt;= 4일 경우 n-gram으로 나타냄
  * 장점
    * 연속적으로 사용되는 용어를 잘 찾아냄
    * 고유의 의미가 아닌 나열되는 단어로 생성되는 새로운 의미를 잘 파악함
    * 띄어쓰기나 글자로는 잡을 수 없었던 Token을 찾을 수 있음
  * 단점
    * 쓸모 없는 조합이 너무 많이 생성된다
    * 특히 한국어는 어미 변화가 다양해서 의미는 비슷하지만 서로 다른 Token이 매우 많이 생겨날 것
    * Token 사전이 과하게 커지지만 의미가 없거나 자주 사용되지 않는 Token으로 구성되어 비효율적
* Byte Pair Encoding
  * Data Compression 분야에서 사용됐던 개념
  * 반복적으로 나오는 데이터의 연속된 패턴을 치환하는 방식을 사용해 데이터를 좀 더 효율적으로 저장하는 개념
  * 2015년 &lt;Nerual Machine Translation of Rare Word with Subword Units&gt; 논물을 시작으로 NP에서 자주 사용되는 Tokenizer Algorithm이 됨
  * n-gram에서 연속된 글자의 나열이 아닌 여러 번 나타나는 글자의 나열은 의미가 있다고 생각해 따로 Token으로 만드는 방식으로 개선
  * 알고리즘
    1. 단어 횟수를 기록한 사전을 만든다. 이 때 사전의 단어 글자는 모두 띄어 표현
    2. 각 단어에 대해 연속된 2개의 글자의 숫자를 세어 가장 많이 나오는 글자 2개의 조합을 찾는다
    3. 두 글자를 합쳐 기존의 사전의 단어를 수정한다
    4. 미리 정해 놓은 횟수만큼 2~3번의 과정을 반복한다

Word Embeding

* 문장을 숫자로 표현한 것을 날것으로 사용하면 딥러닝 모델의 인자로 사용할 때 문제가 있다.
* 이 숫자는 단순히 인덱스를 의미하기 때문
* Token을 유한한 개수로 가진 변수로 생각하고 범주형 자료로 표현했기 때문에 발생하는 문제
* 이 문제를 해결할 수 있는 범주형 자료 변수를 표현하는 방법 중 한 가지가 바로 원-핫 인코딩
* One-Hot Encoding
  * Corpus를 모두 Tokenization해 Voca를 만들고 각 Tk 마다 Index를 정해야 한다.
* Frequency-based Method
  * 단어의 횟수를 기반으로 표현하는 방식은 말 그대로 문장에 있는 Token의 등장 횟수를 세어 표현하는 방식
  * Token의 횟수에만 집중하기 때문에 주머니 같은 곳에 Token을 모아 놓는다고 해서 Bag of Words\(BoW\)라고 한다.
  * 'a', 'an', 'the', 'in' 등의 단어는 빈도수가 굉장히 높은데 등장 빈도가 적은 Token에 악영향을 끼치게 된다. 이를 해결하기 위해 TF-IDF라는 표현방법이 존재한다

![](../../.gitbook/assets/image%20%28273%29.png)

* TF-IDF
  * Term Frequency - Inverse Documnet Frequency
  *  idf는 Token이 등장하는 수의 비율의 역수이다.
  * a나 the 등의 Token은 TF가 높지만 모든 문장\(문서\)에 등장하므로 idf는 log\(1\)에 가까워 진다. =&gt; 0에 가까운 수
  * 따라서 tf-idf 는 tf와 idf를 곱함으로써 과도하게 높았던 tf수치를 낮춰준다.
  * 반대로 희귀한 Token의 경우에도 부족한 TF를 올려주는 효과를 보인다
  * 횟수 기반의 방법은 문자를 비교적 쉽게 표현할 수 있고 특정 Task에 대해 난이도 대비 좋은 성능을 보인다\(ex 스팸\)
  * 단점은 단어의 순서가 무시된다는 점이며 NLG와 같이 새로운 문장을 생성해야 하는 Task에는 어렵다.
* Dense Representation
  * 원핫 인코딩의 문제점은 변수의 희소성이다. token의 개수만큼 v개의 벡터를 만들어야 하고 대부분은 0인데 극소수의 문서에만 쓰인다
  * 이는 큰 메모리 사용량이 필요하게 하면서 대부분은 0으로 돼 있어 비효율적으로 사용하게 한다.



