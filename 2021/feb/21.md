# 21 Sun

## \[파이썬 딥러닝 파이토치\] PART 05 NLP

### 01 Data & Task <a id="01-cnn"></a>

자연어 처리

* Natural Language Processing
* 자연어를 이해하는 영역인 자연어 이해와 모델이 자연어를 생성하는 영역인 자연어 생성으로 나눠 표현하기도 한다
  * NLU : Understanding
    * 주어진 Text의 의미를 파악
  * NLG : Generation
    * 주어진 의미에 대한 자연스러운 Text를 만들어냄
  * NLP = NLU + NLG
* NLP는 다양한 Task를 가지고 있고 각 T별로 데이터도 다양하다
  * Task를 먼저 정한 뒤 데이터를 Vectorization 한다

감정 분석

* Sentiment Analysis
* 문장에 대한 특정 감정을 분류해내는 문제
* 긍정/부정을 판단하는 문제지만 5점 척도로 접근하는 경우도 있다

요약

* Text에서 중요한 부분을 찾아내는 Extractive Summarization과 Text의 의미를 완전히 이해해 새로운 문장을 만들어내는 Abstractive Summarization으로 나뉜다.
* 후자가 더 어려운 Task이다.
* 모델링의 어려움 뿐만 아니라 정답 요약문을 만들어야 하는 데이터 수급 문제, 정답 평가의 어려움도 있어 NLP에서 난이도가 높은 편에 속하는 Task이다.

기계 번역

* NLP의 다양한 Task 중 서비스와 직접적인 연관이 있는 영역
* 결과에 대한 평가의 어려움을 가지고 있다
* 언어의 종류에 따른 데이터 불균형이 심해 많은 연구가 필요한 분야

질문 응답

* 주어진 문서를 이해하고 문서 속 정보에 대한 질문의 답을 이끌어 내는 Task

### 02 문자를 숫자로 표현하는 방법

문장의 의미를 만드는 방식

1. 문장을 의미 있는 부분으로 나눈다
2. 나눠진 의미 있는 부분을 숫자로 바꿔 문장을 숫자로 표현한다

* 간단한 방법은 띄어쓰기를 이용하는 것
* "나는 책상 위에 사과를 먹었다"
  * 나는, 책상, 위에, 사과를, 먹었다
* 이와 같이 의미 있는 부분으로 나누는 과정을 Tokenization이라고 하며 쪼개진 부분을 Token이라고 한다. Lexical Analysis라고도 한다. =&gt; 연속된 문자의 나열을 적절하게 의미를 지닌 부분의 나열로 바꾸는 과정

Out of Vocabulary\(OOV\)

* Token을 저장해둔 Vocabulary에 Token이 없어서 처음 본 Token이 나오는 현상을 OOV라고 한다.
* 이럴 때를 대비해 특수한 Token인 &lt;unk&gt;을 만들어 처음 보는 Token은 &lt;unk&gt;로 변환하도록 처리
* Token을 모아 놓은 문장의 모음을 말뭉치, Corpus라고 한다
* 그러나 이러한 말뭉치가 많아지면 메모리와 모델의 사이즈가 커지기 때문에 효율적인 방법이 필요하다
* 예를 들어 현준이에게, 현준한테는 동일한 의미이므로 이러한 중복적인 의미를 가진 여러 단어를 효율적으로 관리하기 위해서는 띄어쓰기보다 더 좋은 Tokenizer가 필요

Byte Pair Encoding\(BPE\)

* Character based tokenization
  * 띄어쓰기가 아니라 글자를 Token으로 사용
  * 장점
    * 사전의 크기가 줄어드는 장점이 있음
    * OOV 현상을 없앨 수 있다.
  * 단점
    * 표현법에 대한 학습이 어렵다
    * 글자 하나는 보통 의미를 가지고 있지 않고 연속된 나열이 특정 의미를 가지기 때문에 패턴으로 학습해 의미를 만들어야함 =&gt; 쉬운 일은 아님
* n-gram Tokenization
  * 1개 단위가 아니라 여러개의 연속된 윈도우를 단위로 살펴보는 개념
  * n에 따라 uni, bi, tri-gram으로 나뉘며 n &gt;= 4일 경우 n-gram으로 나타냄
  * 장점
    * 연속적으로 사용되는 용어를 잘 찾아냄
    * 고유의 의미가 아닌 나열되는 단어로 생성되는 새로운 의미를 잘 파악함
    * 띄어쓰기나 글자로는 잡을 수 없었던 Token을 찾을 수 있음
  * 단점
    * 쓸모 없는 조합이 너무 많이 생성된다
    * 특히 한국어는 어미 변화가 다양해서 의미는 비슷하지만 서로 다른 Token이 매우 많이 생겨날 것
    * Token 사전이 과하게 커지지만 의미가 없거나 자주 사용되지 않는 Token으로 구성되어 비효율적
* Byte Pair Encoding
  * Data Compression 분야에서 사용됐던 개념
  * 반복적으로 나오는 데이터의 연속된 패턴을 치환하는 방식을 사용해 데이터를 좀 더 효율적으로 저장하는 개념
  * 2015년 &lt;Nerual Machine Translation of Rare Word with Subword Units&gt; 논물을 시작으로 NP에서 자주 사용되는 Tokenizer Algorithm이 됨
  * n-gram에서 연속된 글자의 나열이 아닌 여러 번 나타나는 글자의 나열은 의미가 있다고 생각해 따로 Token으로 만드는 방식으로 개선
  * 알고리즘
    1. 단어 횟수를 기록한 사전을 만든다. 이 때 사전의 단어 글자는 모두 띄어 표현
    2. 각 단어에 대해 연속된 2개의 글자의 숫자를 세어 가장 많이 나오는 글자 2개의 조합을 찾는다
    3. 두 글자를 합쳐 기존의 사전의 단어를 수정한다
    4. 미리 정해 놓은 횟수만큼 2~3번의 과정을 반복한다
  * 



