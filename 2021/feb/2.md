---
description: TIL
---

# 2 Tue

## \[AI 스쿨 1기\] 9주차 DAY 2

### Big Data : Spark 소개 I

빅데이터의 정의

* 서버 한대로 처리할 수 없는 규모의 데이터
  * 아마존의 DATA SCIENTIST 존라우저가 내린 정의
  * 분산 환경이 필요하느냐에 포커스
* 기존의 소프트웨어로는 처리할 수 없는 규모의 데이터
  * 기존 소프트웨어는 오라클이나 MySQL 등의 관계형 데이터베이스
  * 서버의 사양을 높이지 않는 이상 처리 불가 \(Scale-up\)
  * 서버의 수를 증가시키는 것을 Scale-out 이라고 함
* 4V
  * Volume : 데이터의 크기가 대용량
  * Velocity : 데이터의 처리 속도가 중요
  * Variety : 구조화/비구조화 데이터
  * Veracity : 데이터의 품질

빅데이터의 예 - 웹 페이지

* 수십조개 이상의 웹 페이지 존재 ex\) 구글
* 이를 크롤하여 중요한 페이지를 찾아내고 인덱싱하는 것은 엄청난 크기의 데이터 수집과 계산을 필요로 한다
* 사용자 검색어와 클릭 정보 자체도 대용량
* 이런 문제를 해결하면서 구글이 빅데이터 기술의 발전에 지대한 공헌을 하게 됨

대용량 처리 기술

* 분산 환경 기반 : 2대 이상의 서버로 구성
  * 분산 컴퓨팅과 분산 파일 시스템이 필요
  * 모든 서버들이 가진 디스크를 모두 합쳐서 가상디스크로 사용
* Fault Tolerance : 소수의 서버가 고장나도 동작해야함
* Sacle Out : 확장이 용이해야함

하둡의 등장

* Doug Cutting이 구글랩 발표 논문들에 기반해 만든 오픈소스 프로젝트
  * Doug Cutting의 아들의 코끼리 인형 이름이 하둡
* 처음 시작은 Nutch라는 오픈소스 검색엔진의 하부 프로젝트
* 크게 두 개의 서브 시스템으로 구현됨
  * 분산 파일 시스템인 HDFS
  * 분산 컴퓨팅 시스템인 MapReduce
    * 새로운 프로그래밍 방식으로 대용량 데이터 처리의 효율을 극대화
    * 작업에 따라서는 프로그래밍이 너무 복잡하다 =&gt; 성능에 초점을 맞춤
    * 결국 Hive 처럼 MapReduce로 구현된 SQL 언어들이 다시 각광을 받게 됨
    * 기본적으로 배치 작업에 최적화 되어 있어서 realtime으로 처리하기는 어렵다

하둡의 발전

* 하둡 1.0은 HDFS위에 MapReduce라는 분산컴퓨팅 시스팀이 도는 구조
  * 다른 분산컴퓨팅 시스템은 지원하지 못함
* 하둡 2.0은 아키텍처가 크게 변경됨
  * 하둡은 기반 분산처리 시스템이 되고 그 위에 애플리케이션 레이어가 올라가는 구조
  * Spark는 하둡 2.0위에서 애플리케이션 레이어로 실행됨

HDFS - 분산 파일 시스템

* 데이터를 블록 단위로 저장
  * 블록의 크기는 128MB
* 블록 복제 방식
  * 각 블록은 3군데에 중복 저장됨 =&gt; Fault tolerance를 보장
  * 이것이 의미가 있으려면 서버가 3대는 있어야 됨
* 데이터는 데이터 노드에 저장되어 있으며 이 위치는 네임 노드에 저장되어 있음
  * 데이터 노드는 고장나면 다른 백업 노드를 참조하면 된다
  * 하둡 1.0에서는 네임 노드가 고장나면 치명적
  * 하둡 2.0에서는 2nd 네임 노드를 추가

분산 컴퓨팅 시스템

* 하둡 1.0
  * 하나의 잡 트래커와 다수의 태스크 트래커로 구성
  * 잡 트래커가 일을 나눠서 다수의 태스크 트래커에게 분배
* 하둡 2.0
  * 클라이언트, 리소스 매니저, 노드 매니저, 컨테이너로 역할 세분화

하둡을 이용한 데이터 시스템 구성

* 하둡은 흔히 이야기하는 Data Warehouse
* 웍플로우\(여러 곳으로 데이터 푸시\) 관리로는 Airflow가 대세

하둡 1.0 vs 하둡 2.0

* 하둡 2.0을 YARN 이라고 부름
* YARN이라는 프레임 워크 위에서 독자적인 분산 컴퓨팅 시스템이 작동될 수 있도록 함





### Big Data : Spark 소개 II

Spark의 등장

* 버클리 대학의 AMPLab에서 아파치 오픈소스 프로젝트로 2013년 시작
* 하둡의 뒤를 잇는 2세대 빅데이터 기술
  * 자체 분산황경도 지원하지만 주로 하둡 2.0 위에서 분산환경으로 사용
  * 스칼라로 작성됨
* MapReduce의 단점을 대폭적으로 개선
  * Pandas와 굉장히 흡사
* 현재 버전은 Spark3
  * Scala, Java, Python3으로 프로그래밍이 가능
  * 머신러닝 관련해서 많은 개선이 있음 ex\) GPU 환경

Spark vs MapReduce

* MR은 디스크 기반, S는 메모리 기반
* MR은 하둡 위에서만 동작, S는 하둡 이외의 환경 지원
* MR은 키와 밸류 기반 프로그래밍, S는 판다스와 흡사
* S는 다양한 방식의 컴퓨팅을 지원
  * 배치, 스트리밍, SQL, 머신 러닝 등

Spark의 구조

* 드라이버 프로그램의 존재
* Spark는 하둡 2.0 \(혹은 하둡 3.0\) 위에 올라가는 애플리케이션

Spark 프로그래밍

* RDD
  * 로우레벨 프로그래밍 API =&gt; 세밀한 제어 가능
  * 코딩의 복잡도가 증가한다는 단점
* DataFrame & Dataset
  * 스칼라나 자바는 데이터셋
  * 파이썬은 데이터프레임을 사용\(파이썬은 컴파일이 필요 없기 때문\)
  * 하이레벨 프로그래밍 API로 점점 많이 사용되는 추세
  * SparkSQL을 사용하면 이를 쓰게 된다

판다스

* 파이썬으로 데이터 분석을 하는데 가장 기본이 되는 모듈 중의 하나
  * 엑셀에서 하는 일을 가능하게 해주는 파이썬 모듈
  * MATPLOTLIB\(시각화\)- SCIKIT-LEARN\(머신러닝\)과 같은 모듈과 같이 사용됨
* 소규모의 구조화된 데이터를 다루는데 최적
  * 한 대의 서버에서 다룰 수 있는 데이터로 크기가 제약됨
  * 병렬 처리를 지원하지 않음
  * 위 두가지의 특징이 병렬처리에서 판다스를 쓰지 않는 이유
* 할 수 있는 일
  * 구조화된 데이터를 읽고 저장
  * 다양한 통계 지표 도출
  * 데이터 청소 작업 =&gt; 데이터 전처리
  * 시각화

판다사의 데이터 구조

* 엑셀의 시트에 해당하는 것이 Dataframe
* 엑셀 시트의 컬럼에 해당하는 것이 Series

Spark 세션

* 스파크 프로그램의 시작은 Spark 세션을 만들어야 한다
* 스파크 세션을 통해 스파크가 제공해주는 다양한 기능을 사용
* 스파크 2.0 이전에는 기능에 따라 다른 컨텍스트를 생성해야 했음

![](../../.gitbook/assets/image%20%28201%29.png)

Spark 데이터 구조

* 크게 3가지의 자료 구조가 존재
  * RDD
    * 거의 대부분의 스파크 데이터 구조
    * 서버에 저장된 분산 데이터를 지칭
    * 로우레벨 데이터 이다
    * 구조화/비구조화 데이터를 모두 지원
    * RDD는 다수의 파티션으로 구성되고 스파크 클러스터내 서버들에 나눠 저장된다
    * 이 때 RDD는 바로 클러스터에 저장되지만 일반 파이썬 데이터는 parallelize 함수를 통해 RDD로 변환된 뒤 저장된다
    * ex\) \(물론 이렇게 작은 데이터를 클러스터에 올릴 이유는 없다\)

      ```python
      py_list = [1, 2, 3, 4]
      rdd = sc.parallelize(py_list)
      print(rdd.collect())
      ```
  * Dataframe & Dataset
    * RDD는 컬럼이 없는데에 비해 데이터 프레임과 데이터 셋은 컬럼이 존재 =&gt; 개발자가 유용
    * 데이터프레임은 타입이 없고 데이터셋은 타입이 있다
      * 타입이 없는 파이썬에서는 데이터 프레임 사용
      * 타입이 있는 자바나 스칼라는 데이터셋 사용
      * 더 자세히는 컴파일 하기전에 타입을 미리 알고 있어야 하는지 아닌지에 대한 미묘한 차이가 있다
    * parallelize : data =&gt; rdd
    * collect : rdd =&gt; data
      * 이 때 주의해야 할 점은 collect할 데이터가 작아야 한다. 굉장히 크면 메모리가 부족해서 오류가 날 수 있음



### Big Data : Spark의 데이터 구조

스파크 세션

* 스카프를 실행할 때는 스파크 세션이라는 오브젝트를 먼저 생성해야함
* 데이터에 대한 작업은 모두 스파크 클러스터 위에서 작동하며 파이썬이나 자바 코드는 이 데이터에 대한 조작을 명령만 할뿐이다. 실제로는 엔트리 포인트가 있어야 하는데 이것이 스파크 세션
* spark = SparkSession\(\)
  * appName 
  * config : 다양한 형태의 키와 밸류들을 스파크 클러스터에게 전달 가능
  * getOrCreate\(\) : 스파크를 생성
  * sc = spark.Context
    * sc =&gt; RDD 조작
    * spark = 데이터 프레임 조작

Spark 데이터 구조 - 데이터 프레임 생성 방법

* RDD를 변환해서 생성 : RDD의 toDF 함수 사용
* SQL 쿼리를 기반으로 생성
* 외부 데이터를 로딩하여 생성 EX\) .csv / .jdbc

Spark 개발 환경

* 개인컴퓨터에 설치하고 사용하는 방법
  * 간편하기는 하지만 노트북등을 설치해야함
  * 아니면 spark-submit를 이용해 실행
* 각종 무료 노트북 사용
  * 구글 colab
  * 데이터브릭의 커뮤니티 노트북\(속도가 코랩보다 좀 느린 듯\)
  * 제플린의 무료 노트북 : 한 가지 언어가 아니라 여러 언어로 할 수 있음\(속도가 코랩보다 좀 느린 듯\)
* 실제 협업
  * AWS의 EMR 클러스터 사용







