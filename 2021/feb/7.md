# 7 Sun

## PART 03 Deep Learning

### 01 딥러닝의 정의

딥러닝

* 새로운 모델의 개념이 아닌 신경망이 발전한 모델
* 신경망은 학습하는 알고리즘의 특성상 과적합이 심하게 일어나고 Gradient Vanishing이 발생한다.
* 이를 방지하기 위해 SVM과 Ensemble Learning이 많이 쓰인다.
* 딥러닝은 2개 이상의 은닉층을 지니고 있는 다층 신경망
* 딥러닝이 본격적으로 발전하게 된 것은 Graphical Representation Learning이라는 특징 때문

### 02 딥러닝이 발전하게 된 계기

* 과적합과 Gradient Vanishing을 완화시킬 수 있는 알고리즘이 발전
* GPU를 신경망의 연산에 사용할 수 있게 되면서 학습 시간이 오래 걸리는 문제를 해결

### 03 딥러닝의 종류

* MLP
* CNN : 이미지 관련 분야에서 많이 사용
* RNN : 텍스트같은 시계열 분야에 많이 사용

### 04 딥러닝의 발전을 이끈 알고리즘 - 1

Dropout

* 신경망의 학습 과정 중 Layer의 노드를 랜덤하게 Drop함으로써 Generalization 효과를 가져오게 하는 테크닉

![](../../.gitbook/assets/image%20%28211%29.png)

* 유전 알고리즘에서 아이디어를 차용
* MNIST라는 손글씨 데이터에 Dropout을 적용한 신경망과 적용하지 않은 신경망의 성능을 비교하면 Dropout을 적용한 신경망이 Test Error가 더 낮다.
* Ensemble Learning의 Random Forest의 개념과 비슷하다
  * Ensemble Learning의 기본 개념은 다양한 모델
  * 다양한 모델을 만들기 위해 데이터를 랜덤하게 구성하고 변수도 랜덤하게 구성한 것이 RandomForest
  * Dropout을 랜덤한 변수의 구성으로 보면 비슷한 모델 구성

Activation 함수

* ReLU
  * Rectified Linear Unit
  * 시그모디으 함수와 같은 비선형 활성 함수의 문제점을 어느 정도 해결 =&gt; Gradient Vanishing 완화
  * f\(x\) = max\(0, x\)
  * 이후로 Leaky ReLU, ELU, parametric ReLU, SELU, SERLU 등 다양한 활성함수 등장

Batch Normalization

* 신경망에는 과적합과 GV 외에도 Internal Covariance shift라는 현상이 발생
  * 각 층마다 Input 분포가 달라짐에 따라 학습 속도가 느려지는 현상
  * Batch Normalization은 이를 방지 =&gt; Input 분포를 정규화해 학습 속도를 빠르게 함
  * [여기](https://eehoeskrap.tistory.com/430)를 참고하면 이해에 도움이 될 수 있음\(근데 난 이해 잘 못함\)
  * 대력적인 느낌 : ReLU는 입력값이 0보다 크면 항상 자기 자신을 Output으로 주는데, 이 값의 범위가 너무 가지각색이니 정규화를 통해 일정 범위안의 있는 값으로 통일하겠다라는 것 같다. 이 때 표준분포를 쓰는게 아니라, 각 레이어마다 알파 베타 감마를......... ㅠㅠ

Initialization

* LeCun Initialization
  * CNN 창시자의 이름을 땀
* He Initialization
  * Xavier Initialization을 보완

Optimizer

* SGD이외에도 다양한 Optimizer 존재
* Momentum
  * 미분을 통한 Gradient 방향으로 가되, 일종의 관성을 추가하는 개념
  * 사용하지 않았을 경우보다 최적의 장소로 더 빠르게 수렴하며 걸어가는 보폭이 커진 개념으로 이해 가능
  * 최적 해가 아닌 지역해를 지나칠 수도있다는 장점
* NAG
  * Nesterov Accelerated Gradient
  * Momentum을 약간 변형한 방법
  * 모멘텀으로 이동한 후 기울기를 구해 이동하는 방식
* Adagrad
  * Adaptive Gradient
  * 가보지 않은 곳은 많이 움직이고 가본 곳은 조금씩 움직이자
* RMSProp
  * Adagrad의 단점을 보완한 방법 =&gt; 학습이 오래 진행될수록 step size가 작아지고 부분이 계속 증가 =&gt; G\(갱신된 파라미터\)가 무한히 커지지 않도록 지수 평균을 내 계산
* Adadelta
  * Adaptive Delta
  * Adagrad의 단점을 보완한 방법
  * Gradient의 양이 너무 적어지면 움직임이 멈추는데, 이를 방지하기 위한 방법
* Adam
  * Adaptive Moment Estimation
  * 딥러닝 모델에서 가장 많이 사용하는 기본적인 Optimizer
  * RMSProp과 Momentum 방식의 특징을 결합한 방법
* RAdam
  * Rectified Adam
  * 대부분의 Optimizer는 학습 초기에 전역 최저점이 아닌 지역 최저점에 수렴해 버릴 수 있는 단점이 있는데 이를 교정하기 위한 Optimizer

