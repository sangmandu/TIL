# 6 Sat

## PART 02 AI Background

### 05 인공신경망

퍼셉트론

* 1957년에 개발된 최초의 인공지능 모형
* Feed-Forward Network 모형
* 선형 분류 모형

MLP

* Multi Layer Perceptron
* 퍼셉트론이 지닌 한계점인 비선형 분류 문제를 해결하기 위해 등장

사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계하기

{% file src="../../.gitbook/assets/mlp\_mnist.ipynb" caption="MNIST with MLP" %}

Universal Approximation Theorem

* 신경망 이론 중 가장 유명하고 신경망 학습의 특성을 잘 나타내주는 이론
* 은닉층이 1개 이상인 신경망\(=MLP\)은 학습 데이터 내에서 어떤 함수든 근사 시킬 수 있다라는 이론
* 두 가지 의문점
  * 학습 데이터 내에서 어떤 모델이나 만들 수 있으면 굉장한 것 아닌가?
    * 학습 데이터 내에서는 잘 맞지만 실제 데이터에서는 안맞을 수 있음 =&gt; 과적합
  * 은닉층을 깊게 쌓을 필요가 있는가?
    * 복잡한 문제에 대해 간단한 모델보다 복잡한 모형이 좀 더 효율적으로 모델링이 가능하다

신경망 모형의 단점

* 과적합
* Gradient Vanishing

성능 지표

* MSE : Mean Square Error
  * 회귀 모형에서 많이 사용하는 Loss
  * 상대적인 성능 지표
  * A문제의 모델1의 MSE가 30, B문제의 모델2의 MSE가 300 이라고 하더라도 모델1의 성능이 좋다고 말할 수 없다.
* MAPE : Mean Absolute Percentage Error
  * 절대적인 성능 지표
* 정확도 : Accuracy
  * \(실제 클래스, 예측한 클래스\)에 따라 4가지로 분류
  * \(O, O\) : TP : True Positive
  * \(O, X\) : FP : False Positive
  * \(X, O\) : FN : False Negative
  * \(X, X\) : TN : True Negative
  * 클래스 간의 불균형 현상이 있을 때는 지표로 사용하기 어렵다
    * 정상이 99%, 비정상이 1%인 데이터에서는 정상이라고만 말해도 99%의 성능을 얻기 때문

![](../../.gitbook/assets/image%20%28211%29.png)

* F1-Measure\(F1-Score\) : Class Imbalance 상황에서 많이 사용하는 지표.

![](../../.gitbook/assets/image%20%28213%29.png)

