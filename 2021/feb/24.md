# 24 Wed

## \[파이썬 딥러닝 파이토치\] PART 05 NLP

### 02 문자를 숫자로 표현하는 방법

문장의 의미를 만드는 방식

1. 문장을 의미 있는 부분으로 나눈다
2. 나눠진 의미 있는 부분을 숫자로 바꿔 문장을 숫자로 표현한다

* 간단한 방법은 띄어쓰기를 이용하는 것
* "나는 책상 위에 사과를 먹었다"
  * 나는, 책상, 위에, 사과를, 먹었다
* 이와 같이 의미 있는 부분으로 나누는 과정을 Tokenization이라고 하며 쪼개진 부분을 Token이라고 한다. Lexical Analysis라고도 한다. =&gt; 연속된 문자의 나열을 적절하게 의미를 지닌 부분의 나열로 바꾸는 과정

Out of Vocabulary\(OOV\)

* Token을 저장해둔 Vocabulary에 Token이 없어서 처음 본 Token이 나오는 현상을 OOV라고 한다.
* 이럴 때를 대비해 특수한 Token인 &lt;unk&gt;을 만들어 처음 보는 Token은 &lt;unk&gt;로 변환하도록 처리
* Token을 모아 놓은 문장의 모음을 말뭉치, Corpus라고 한다
* 그러나 이러한 말뭉치가 많아지면 메모리와 모델의 사이즈가 커지기 때문에 효율적인 방법이 필요하다
* 예를 들어 현준이에게, 현준한테는 동일한 의미이므로 이러한 중복적인 의미를 가진 여러 단어를 효율적으로 관리하기 위해서는 띄어쓰기보다 더 좋은 Tokenizer가 필요

Byte Pair Encoding\(BPE\)

* Character based tokenization
  * 띄어쓰기가 아니라 글자를 Token으로 사용
  * 장점
    * 사전의 크기가 줄어드는 장점이 있음
    * OOV 현상을 없앨 수 있다.
  * 단점
    * 표현법에 대한 학습이 어렵다
    * 글자 하나는 보통 의미를 가지고 있지 않고 연속된 나열이 특정 의미를 가지기 때문에 패턴으로 학습해 의미를 만들어야함 =&gt; 쉬운 일은 아님
* n-gram Tokenization
  * 1개 단위가 아니라 여러개의 연속된 윈도우를 단위로 살펴보는 개념
  * n에 따라 uni, bi, tri-gram으로 나뉘며 n &gt;= 4일 경우 n-gram으로 나타냄
  * 장점
    * 연속적으로 사용되는 용어를 잘 찾아냄
    * 고유의 의미가 아닌 나열되는 단어로 생성되는 새로운 의미를 잘 파악함
    * 띄어쓰기나 글자로는 잡을 수 없었던 Token을 찾을 수 있음
  * 단점
    * 쓸모 없는 조합이 너무 많이 생성된다
    * 특히 한국어는 어미 변화가 다양해서 의미는 비슷하지만 서로 다른 Token이 매우 많이 생겨날 것
    * Token 사전이 과하게 커지지만 의미가 없거나 자주 사용되지 않는 Token으로 구성되어 비효율적
* Byte Pair Encoding
  * Data Compression 분야에서 사용됐던 개념
  * 반복적으로 나오는 데이터의 연속된 패턴을 치환하는 방식을 사용해 데이터를 좀 더 효율적으로 저장하는 개념
  * 2015년 &lt;Nerual Machine Translation of Rare Word with Subword Units&gt; 논물을 시작으로 NP에서 자주 사용되는 Tokenizer Algorithm이 됨
  * n-gram에서 연속된 글자의 나열이 아닌 여러 번 나타나는 글자의 나열은 의미가 있다고 생각해 따로 Token으로 만드는 방식으로 개선
  * 알고리즘
    1. 단어 횟수를 기록한 사전을 만든다. 이 때 사전의 단어 글자는 모두 띄어 표현
    2. 각 단어에 대해 연속된 2개의 글자의 숫자를 세어 가장 많이 나오는 글자 2개의 조합을 찾는다
    3. 두 글자를 합쳐 기존의 사전의 단어를 수정한다
    4. 미리 정해 놓은 횟수만큼 2~3번의 과정을 반복한다

Word Embeding

* 문장을 숫자로 표현한 것을 날것으로 사용하면 딥러닝 모델의 인자로 사용할 때 문제가 있다.
* 이 숫자는 단순히 인덱스를 의미하기 때문
* Token을 유한한 개수로 가진 변수로 생각하고 범주형 자료로 표현했기 때문에 발생하는 문제
* 이 문제를 해결할 수 있는 범주형 자료 변수를 표현하는 방법 중 한 가지가 바로 원-핫 인코딩
* One-Hot Encoding
  * Corpus를 모두 Tokenization해 Voca를 만들고 각 Tk 마다 Index를 정해야 한다.
* Frequency-based Method
  * 단어의 횟수를 기반으로 표현하는 방식은 말 그대로 문장에 있는 Token의 등장 횟수를 세어 표현하는 방식
  * Token의 횟수에만 집중하기 때문에 주머니 같은 곳에 Token을 모아 놓는다고 해서 Bag of Words\(BoW\)라고 한다.
  * 'a', 'an', 'the', 'in' 등의 단어는 빈도수가 굉장히 높은데 등장 빈도가 적은 Token에 악영향을 끼치게 된다. 이를 해결하기 위해 TF-IDF라는 표현방법이 존재한다

![](../../.gitbook/assets/image%20%28273%29.png)

* TF-IDF
  * Term Frequency - Inverse Documnet Frequency
  *  idf는 Token이 등장하는 수의 비율의 역수이다.
  * a나 the 등의 Token은 TF가 높지만 모든 문장\(문서\)에 등장하므로 idf는 log\(1\)에 가까워 진다. =&gt; 0에 가까운 수
  * 따라서 tf-idf 는 tf와 idf를 곱함으로써 과도하게 높았던 tf수치를 낮춰준다.
  * 반대로 희귀한 Token의 경우에도 부족한 TF를 올려주는 효과를 보인다
  * 횟수 기반의 방법은 문자를 비교적 쉽게 표현할 수 있고 특정 Task에 대해 난이도 대비 좋은 성능을 보인다\(ex 스팸\)
  * 단점은 단어의 순서가 무시된다는 점이며 NLG와 같이 새로운 문장을 생성해야 하는 Task에는 어렵다.
* Dense Representation
  * 원핫 인코딩의 문제점은 변수의 희소성이다. token의 개수만큼 v개의 벡터를 만들어야 하고 대부분은 0인데 극소수의 문서에만 쓰인다
  * 이는 큰 메모리 사용량이 필요하게 하면서 대부분은 0으로 돼 있어 비효율적으로 사용하게 한다.
* Word2vec : idea
  * 이 방법은 관계를 연산으로 설명할 수 있는 벡터 표현이 가능하다는 것을 보여준다.
  * 모델 학습의 가정
    * Token의 의미는 주변 Token의 정보로 표현된다
  * 학습하는 과정
    * CBOW : Continuous Bag-of-Words Model
    * Skip-Gram
    * 두 가지 방법 모두 공통적으로 문장을 윈도우 형태로 부분만 보는 것을 기본으로 한다
    * CBOW는 주변 Token을 벡터로 변환해 더한 후 Target을 맞추는 것이고 Skip-Gram의 목적은 Target을 벡터로 변환해 주변 Token을 맞추는 것

