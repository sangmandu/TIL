# 17 Wed

## \[AI 스쿨 1기\] 10주차 DAY 2

### NLP : 문서분류 I

문서 분류

* 텍스트를 입력으로 받아, 텍스트가 어떤 종류의 범주에 속하는지를 구분하는 작업

다양한 문서 분류 문제들

* 문서의 범주, 주제 분류
* 이메일 스팸 분류
* 감성 분류 : 긍정적인지 부정적인지
* 언어 분류 : 주어진 문제가 어떤 언어로 쓰여 있는가?

주제 분류

* CS논문을 가지고 CS 주제를 분류하는 것

감성 분류

* 주어진 문서에 대해서 해당 문서가 긍정적인지 부정적인지 파악
* 영화 리뷰가 긍/부 정적인지 파악하기
  * 긍정적 : richly, great, awesome, love
  * 부정적 : pathetic, worst, awful, ridiculously
* 제품에 대한 대중의 반응
* 정치인에 대한 사람들의 생각
* 감성 분류를 기반으로 선거 결과를 예측
* 감정적\(방황, 슬픔, 기쁨\), 태도적\(좋아함, 사랑함, 싫어함\), 성격적\(불안함, 적대적, 호의적\) 측면이 있지만 긍/부정적인 태도만을 파악하는 간단한 작업을 다룰 것.

문서 분류 : 정의

* 문서를 주로 d 라고 함
* 가능한 모든 클래스를 C ={c1, c2, ...} 라고 한다
* 출력 결과는 : 예측된 클래스 cn

문서 분류 방법 - 규칙기반 모델

* 단어들의 조합을 사용한 규칙들을 사용
  * spam : black-list, dollars & you have been selected, etc
* Precision은 높지만 recall이 낮음
  * 사람이 규칙을 만들기 때문에 정확도는 높음
  * 지속적인 업데이트로 정확도에 기여
  * 커버하지 못하는 경우도 굉장히 많다
  * 따라서 사람이 만든 규칙보다 머신러닝 학습을 통해 생성된 규칙을 따르는 것이 좋음
  * 머신러닝도 사실은 복잡한 규칙을 만든다고 볼 수 있다. 하지만 사람이 이를 만들기는 쉽지 않음
* Snorkel
  * 모델 중 하나
  * 규칙기반 모델과 머신러닝을 함께 사용하는 방식
  * 각각의 규칙을 labeling function으로 간주
    * 전문가가 labeling 하는 것이 아닌 특정 규칙안에서 labeling을 한다.
  * factor graph를 가지고 확률적 목표값을 생성한다
    * generative model
    * 긍정 1, 부정 0 이 아닌 0.7, 0.2 등의 확률적 수치를 사용한다
    * 이는 labeling되지 않은 unlabed 데이터들에 대해서 labeling해주는 작업이다
  * labeled data가 부족하거나 클래스 정의 자체가 애매한 경우 매우 유용하다 =&gt; 감성 분류 자체는 굉장히 애매한 클래스에 속하는 경우가 많다.
  * 문서 하나하나에 labeling 하기는 어렵다.
  * 확실하 규칙 생성은 쉬운 편에 속함.
  * 이 후 다양한 모델을 사용할 수 있다

문서 분류 방법들 - 지도학습

* input : 문서 d
* classes : C
* training set : m = \(d1, c1\) ,,, \(dm, cm\)
* classifier\(output\) : y:d -&gt; c
* 다양한 모델 사용 가능
  * Naive Bayes
  * Logistic regression
  * Nerual networks
  * k-Nearest Neighbors

### 

### NLP : 문서분류 II

Naive Bayes 분류기

* 가정 이름이 Naive Bayes임
  * 그래서 이름이 나이브 베이즈 분류기
* 이 모델은 Bag of Words 표현에 기반함
  * 텍스트 표현 방법이 이 방법

![](../../.gitbook/assets/image%20%28239%29.png)

* 순서를 고려하지 않은 단어들의 집합이라고 생각하면 된다
* \(0, 5, 0, 3, ... , 0\)
  * 각각의 vocaburary의 한단어 =&gt; \|v\|개의 원소를 가진 벡터
  * 원소의 값은 빈도수를 의미

Naive Bayes 분류기 - 수식화

![](../../.gitbook/assets/image%20%28236%29.png)

* 베이즈 정리를 이용해서 식 생성

![](../../.gitbook/assets/image%20%28233%29.png)

* 가장 확률이 높은 클래스를 찾는 것이 분류기의 목표
* 분모 제거 : 최대화 하는 클래스를 찾기 위해서 P\(d\)는 관련이 없기 때문

![](../../.gitbook/assets/image%20%28231%29.png)

* d가 n개의 속성이 있다고 가정하고 x1 부터 xn으로 표현

![](../../.gitbook/assets/image%20%28232%29.png)

* d를 n개의 특징으로 분류하다 보니 파라미터가 아주 많아진다. =&gt; 학습 데이터가 적으면 좋은 성능을 내기 어려움
* Bag of Words 가정 : 단어의 위치는 확률에 영향을 주지 않는다.
* 조건부 독립 가정 : 클래스가 주어지면 속성들은 독립적이다
  * 이를 수식으로 표현하면 다음과 같다

![](../../.gitbook/assets/image%20%28230%29.png)

* 조건부 독립 가정을 통해 분리를 하게 되면 각각의 클래스에 해당하는 특성만 존재하므로 파라미터가 적어지게 된다 =&gt; 주어진 데이터가 적더라도 학습 가능

![](../../.gitbook/assets/image%20%28229%29.png)

* Naive Bayes 분류기는 입력값에 관한 선형모델이다.



### NLP : 문서분류 III

Naive Bayes 분류기 - 학습

* MLE

![](../../.gitbook/assets/image%20%28237%29.png)

* Zero 확률문제
  * 데이터의 부족으로 확률이 0이 될 수 있음

![](../../.gitbook/assets/image%20%28238%29.png)

* * 클래스와 특성의 곱으로 표현되므로 하나라도 0이 되면 전체 확률이 0이 되기 떄문

![](../../.gitbook/assets/image%20%28234%29.png)

* * 이 때 라플라스 스무딩 방법을 통해 해결할 수 있다

![](../../.gitbook/assets/image%20%28235%29.png)

Naive Basyes 분류기 - 요약

* naive 하지는 않음
* 적은 학습데이터로도 좋은 성능과 빠른 속도
* 조건부독립 가정이 실제 데이터에서 성립할 때 최적의 모델
* 문서 분류를 위한 베이스라인 모델로 적합하다





