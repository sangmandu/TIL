# 25 Thu

## \[파이썬 딥러닝 파이토치\] PART 05 NLP

### 03 Models

문자 데이터를 사용한 모델링

* 문장이나 문단과 같은 문자 데이터는 주변 단어에 대한 연속적인 관계를 모델링에 잘 담아야 한다.
* NLP 모델은 언어적 약속을 모델링 하려 했고 Classic Model은 통계적으로 접근하려고 했다.

RNN

* Recurrent Neural Network
* 1개의 데이터에 대해 1개의 결괏값을 예측하도록 학습되어 있다
  * 이를 One To One 문제라고 한다
  * Many to One : 댓글의 악플 가능성 정도를 측정하는 Sentence Classification
  * One to Many : 사진 속 내용을 설명하는 글을 만들어내는 Image Captioning
  * Many to Many\(token by token\) : 문장의 모든 Token에 대한 품사를 예측하는 Pos Tagging
  * Many to Mnay\(encoder-decoder\) : 입력 문장에 대한 번역 문장을 만들어내주는 Translation
* Sequential Data를 다루는 가장 대표적인 모델
* 각 정보는 이전 정보를 참고함으로써 데이터의 순서에 대한 정보를 담을 수 있다는 장점이 있어 음성과 같은 연속성 데이터를 다루는 데 탁월하다
* 성능은 뛰어났지만 반복적인 Back Propagation 때문에 G.V. 문제가 심하다
* 문장의 길이가 길어질수록 성능도 떨어지고 시간도 오래걸린다
  * 이를 해결한 모델이 LSTM

LSTM

* Long Short -Term Memory, 1997
* RNN의 큰 단점인 단기 기억만 가능하다는 부분을 개선
* 핵심은 Cell과 다수의 Gate를 통한 정보 필터링이다.
  * 현재 Token Cell $$ C_{i} $$은 이전 정보를 담고있는 Cell $$ C_{i-1} $$ 에서 얼마나 잊을지에 대한 Gate f를 통과한 값과 현재 토큰에서 얼마나 가져올지에 대한 Gate i를 통과한 값의 합이다.
  * 이전 정보 $$ h_{i-1} $$과 현재 토큰 $$ x_i $$값을 조합하는 과정
  * 이렇게 구한 Cell 값은 최종 Gate를 거치면서 또 한번의 정보 수정을 통해 최종적인 은닉층을 구성한다.

그 외의 모델

* Bi-RNNs
  * 기존 왼쪽에서 오른쪽의 단방향이 아닌 양방향의 정보를 모두 이용하는 것이 핵심
* GRUs
  * LSTM의 Output을 간소화 함
* Attension Mechanism
* Convolution Nerual Network for Text Classification

Transformer

* 보통의 인코더와 디코더는 RNN류의 LSTM이나 GRU 모듈을 사용하고 Attention을 적용하는 방식을 사용했는데 Transformer는 RNN을 전혀 쓰지 않고 여러 Attention 모듈을 이어 만들었다
* 또 순서대로 Token을 입력받는 RNN과 달리 한번에 모든 Token을 입력받아서 학습이 빠르다는 장점이 있다
* 기존 번역 모델에서 RNN류의 모듈을 벗어낫고 Attention 만으로도 좋은 성능을 냈다는 의의가 있지만 Transformer를 이용한 다양한 모델이 연구가 되고 있음이 큰 연구 성과이다

BERT

* 구글에서 발표, 2018
* Pre-training of Deep Bidirectional Transformers for Language Understanding
* NLP에서 Pre-trained 된 모델을 학습하고 이를 Fine-tuning 하는 모델의 가능성과 성능을 동시에 보여줌



