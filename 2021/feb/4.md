# 4 Thu

## PART 01 파이토치 기초

### 01 파이썬 또는 아나콘타 설치하기

파이썬 VS 아나콘다

* 컴퓨터 공학을 전공했거나 소프트웨어 툴을 능숙하게 다룰 수 있는 사용자는 파이썬 설치를 권장
* 소프트웨어 툴을 다루는 데 미숙한 사용자는 아나콘다 설치를 권장. 데이터 분석에 필요한 모듈이 자동으로 설치되기 때문



### 02 CUDA, CuDNN 설치하기

G-FLOPs

* GPU FLoating Operations Per Second의 약자
* 초당 부동소수점 연산을 의미하며 GPU의 성능을 측정하는 요소

CUDA

* GPU를 장착하더라도 파이썬에서 이를 인식할 수 있어야 한다.
* 텐서플로우나 파이토치 등의 딥러닝 프레임워크를 사용하려면 CUDA를 설치해야 한다.
* GPU에서 병렬 처리를 수행하는 알고리즘을 각종 프로그래밍 언어에 사용할 수 있도록 해주는 GPGPU\(General-Purpose computing on Graphics Processing Units\) 기술

CuDNN

* nvidia CUDA Deep Nerual Network Library의 약자
* 딥러닝 모델을 위한 GPU 가속화 라이브러리의 기초 요소와 같은 일반적인 루틴을 빠르게 이행할 수 있도록 해주는 라이브러리



### 03 파이토치 설치하기

nothing



### 04 반드시 알아야 하는 파이토치 스킬

스칼라, 벡터, 매트릭스, 텐서

* `torch.tensor` 를 이용하여 변수를 선언할 수 있다.

![](../../.gitbook/assets/image%20%28206%29.png)

* +, -, \*, / 를 이용해서 직접적인 사칙연산이 가능하며 `torch.(add/sub/mul/div)`를 이용한 사칙연산도 가능하다
* 행렬 요소간 곱이 아닌 행렬 곱은 `torch.matmul`로 가능하다.

Autograd

* Autograd 방식을 이용해 Back Propagation으로 파라미터를 업데이트 하는 방법을 쉽게 구현할 수 있다.

```python
import torch

if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')
```

* `cuda.is_available()`
  * 현재 파이썬이 실행되고 있는 환경에서 torch moudle을 이용할 때 GPU를 이용해 계산할 수 있는지를 파악하는 메서드
  * 참이면 cuda 장비를, 거짓이면 cpu 장비를 이용해 계산한다.

```python
BATCH_SIZE = 64
INPUT_SIZE = 10000
HIDDEN_SIZE = 100
OUTPUT_SIZE = 10
```

* `BATCH_SIZE`
  * 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터의 개수
  * Input으로 이용되는 데이터가 64개이다.
* `INPUT_SIZE`
  * 딥러닝 모델에서의 Input의 크기이자 입력층의 노드 수를 의미.
  * BATCH\_SIZE와 혼동하면 안됨. BATCH는 갯수를, INPUT은 크기를 의미
  * \(64, 1000\)의 텐서가 입력되고 이 때 마다 가중치 갱신이 일어남
* `HIDDEN_SIZE`
  * 딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수를 의미
  * 입력층에서 은닉층으로 전달됐을 때 은닉층의 노드 수를 의미
* `OUTPUT_SIZE`
  * 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기를 의미

```python
x = torch.randn(BATCH_SIZE, 
                INPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = False)
y = torch.randn(BATCH_SIZE, 
                OUTPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = False)
w1 = torch.randn(INPUT_SIZE, 
                HIDDEN_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = True)
w2 = torch.randn(HIDDEN_SIZE, 
                OUTPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = True)
```

* `randn`
  * 평균이 0, 표쥰편차가 1인 정규분포에서 샘플링한 값으로 데이터를 만든다는 것을 의미
* `reguires_grad`
  * 파라미터에 대해서만 Gradient를 계산하면 되므로 가중치에 대해서만 True로 설정

```python
learning_rate = 1e-6
for t in range(1, 501):
    y_pred = x.mm(w1).clamp(min = 0).mm(w2)
    
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 0:
        print("Iteration: ", t, "\t", "Loss: ", loss.item())
    loss.backward()
    
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        
        w1.grad.zero_()
        w2.grad.zero_()
```

* `learning_rate`
  * 학습률을 어떻게 설정하느냐에 따라 Gradient 값에 따른 학습 정도가 결정된다. 딥러닝 모델에서 파라미터 값을 업데이트할 때 가장 중요한 하이퍼 파라미터이기도 하다.
* `y_pred`
  * 딥러닝 모델의 결괏값을 보통 예측값이라고 표현한다.
  * x와 w1과의 행렬 곱 연산에 대해 clamp 함수를 적용하고 다시 w2와의 행렬 곱 연산을 한다.
* `clamp`
  * 여기서는 min만 정의 되었으므로 ReLU와 동일한 역할을 한다.

![](../../.gitbook/assets/image%20%28205%29.png)

* `loss`
  * 예측값과 실제 레이블 값을 비교해 오차를 계산한 값을 loss라고 한다. 여기서는 제곱값의 차를 비교했다.
* `loss.backward()`
  * loss값에 대해 backward\(\) 메서드를 이용하면 각 파리미터 값에 대해 Gradient를 계산하고 이를 통해 Back Propagation을 진행한다는 것을 의미한다.
* `with_torch.no_gard()`
  * 각 파라미터 값에 대해 Gradient를 계산한 결과를 이용해 파라미터 값을 업데이트할 때는 해당 시점의 Gradient 값을 고정한 후 업데이트를 진행해야 한다.
* `w.grad.zero_()`
  * 각 파라미터 값을 업데이트 했다면 Gradient를 초기화 해서 다음 반복문을 진행할 수 있도록 Gradient를 0으로 설정한다.







