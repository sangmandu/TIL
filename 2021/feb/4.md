# 4 Thu

## \[AI 스쿨 1기\] 9주차 DAY 4

### Big Data : Spark MLib 소개

Spark MLib

* 머신러닝 관련 다양한 알고리즘, 유틸리티로 구성된 라이브러리
  * Classfication, Regression, Clustering, Collaborative Filtering, Dimensionality, Reduction
  * 아직 딥러닝 지원은 미약
* RDD 기반과 데이터 프레임 기반의 두 버전이 존재
  * spark.mllib vs spark.ml
    * mllib가 RDD 기반이고 ml은 데이터 프레임 기반
    * mllib는 이전 라이브러리로 더 이상 업데이트가 안됨
    * 따라서 항상 spark.ml을 사용할 것!
* 장점
  * 원스톱 ML 프레임워크
    * 데이터프레임과 SparkSQL등을 이용해 전처리
    * Spark MLlib를 이용해 모델 빌딩
    * ML Pipeline을 통해 모델 빌딩 자동화
    * ML flow로 모델 관리하고 서빙
  * 대용량 데이터도 처리 가능
  * 데이터가 작은 경우는 굳이 사용할 필요는 없음

MLflow

* 모델 개발과 테스트와 관리와 서빙까지 제공해주는 엔드투엔드 프레임 워크
* 파이썬, 자바, R, API를 지원
* 트래킹, 모델, 프로젝트를 지원

Spark MLlib 제공 알고리즘

![](../../.gitbook/assets/image%20%28213%29.png)

Spark MLlib 기반 모델 빌딩의 기본 구조

* 모델 빌딩
  * 트레이닝셋 전처리
  * 모델 빌딩
  * 모델 검증
* Scikit-Learn과 비교했을 때 장점
  * 차이점은 데이터의 크기
  * 트레이닝셋의 크기가 크면 전처리와 모델 빌딩에 있어 Spark가 큰 장점을 가짐
  * Spark는 ML 파이프라인을 통해 모델 개발의 반복을 쉽게 해줌



### Big Data : Spark MLib 피쳐 변환

피쳐 추출과 변환

* 피처 값들을 모델 훈련에 적합한 형태로 바꾸는 것을 지칭
* 크게 Feature Extractor와 Feature Transformer가 존재
* Feature Transformer
  * 피처 값들은 숫자 필드\(여야함\)
  * 숫자 필드 값의 범위 표준화
  * 비어있는 필드들의 값을 어떻게 채울지 결정
* Feature Extractor
  * 기존 피쳐에서 새로운 피쳐를 추출
  * ex\) TF-IDF, Word2Vec

피쳐 변환 String Indexer

![](../../.gitbook/assets/image%20%28210%29.png)

피쳐 변환 Scaler : 숫자 필드값의 범위 표준화

* 숫자 필드 값의 범위를 특정 범위로 변환
* 피쳐 스케일링 혹은 정규화라고 부름

![](../../.gitbook/assets/image%20%28207%29.png)

피쳐 변환 Imputer : 값이 없는 필드 채우기

* 값이 존재하지 않는 레코드들이 존재하지 않는 필드들의 경우 기본값을 정해서 채우는 것

![](../../.gitbook/assets/image%20%28206%29.png)

모델 빌딩과 관련된 흔한 문제들

1. 트레이닝 셋의 관리가 안됨
2. 모델 훈련 방법이 기록이 안 됨
3. 모델 훈련에 많은 시간 소요

ML Pipeline의 등장

* 앞서 언급한 문제들 중 2, 3번을 해결
* 자동화를 통해 에러 소지를 줄이고 반복을 빠르게 가능하게 해줌

Spark MLlib 관련 개념 정리

* ML 파이프라인이란?
  * 데이터 과학자가 머신러닝 개발과 테스트를 쉽게 해주는 기능 \(데이터 프레임 기반\)
  * 머신러닝 알고리즘에 관계없이 일관된 형태의 API를 사용하여 모델리이 가능
  * ML 모델개발과 테스트를 반복가능해줌
* 4개의 요소로 구성
  * DataFrame
  * Transformer
  * Estimator
  * Parameter
* DataFrame
  * ML 파이프라인에서는 데이터프레임이 기본 데이터 포맷
  * 기본적으로 CSV, JSON, Parguet, JDBC를 지원
  * 다음 2가지의 새로운 데이터소스를 지원
    * 이미지 데이터소스
    * LIBSVM 데이터소스
* Transformer
  * 입력 데이터프레임을 다른 데이터프레임으로 변환
  * 2종류의 Transformer가 존재하며 transform이 메인 함수
  * Feature Transformer
    * 입력 데이터프레임의 컬럼으로부터 새로운 컬럼을 만들어내 이를 추가한 새로운 데이터프레임을 출력으로 내줌. 보통 피쳐 엔지니어링을 하는데 사용
  * Learning Model
    * 머신러닝 모델에 해당
* Estimator
  * 머신러닝 알고리즘에 해당. fit이 메인 함수
    * 트레이닝셋 데이터프레임을 입력으로 받아서 머신러닝 모델을 만들어냄
    * 예를 들어 LogisticRegression은 Estimator이고 LogistricRegression.fit\(\)을 호출하면 머신 러닝 모델을 만들어냄
  * ML 파이프라인도 Estimator
  * Estimator는 저장과 읽기 함수 제공
* Parameter
  * Transformer와 Estimator의 공통 API로 다양한 인자를 적용해줌
  * 두 종류의 파라미터가 존재
    * Param : 하나의 이름과 값
    * ParamMap : Param 리스트
  * 파라미터의 예
    * 훈련 반복수 지정을 위해 setMaxIter\(\)를 사용
    * ParamMap\(Ir.maxIter -&gt; 10\)
  * 파라미터는 fit 혹은 transform에 인자로 지정 가능
* ML Pipeline
  * 하나 이상의 트랜스포머와 에스티메이터가 연결된 모델링 웍플로우
    * 입력은 데이터프레임
    * 출력은 머신러닝 모델
  * ML Pipeline 그 자체도 Estimator

## \[파이썬 딥러닝 파이토치\] PART 01 파이토치 기초

### 01 파이썬 또는 아나콘타 설치하기

파이썬 VS 아나콘다

* 컴퓨터 공학을 전공했거나 소프트웨어 툴을 능숙하게 다룰 수 있는 사용자는 파이썬 설치를 권장
* 소프트웨어 툴을 다루는 데 미숙한 사용자는 아나콘다 설치를 권장. 데이터 분석에 필요한 모듈이 자동으로 설치되기 때문



### 02 CUDA, CuDNN 설치하기

G-FLOPs

* GPU FLoating Operations Per Second의 약자
* 초당 부동소수점 연산을 의미하며 GPU의 성능을 측정하는 요소

CUDA

* GPU를 장착하더라도 파이썬에서 이를 인식할 수 있어야 한다.
* 텐서플로우나 파이토치 등의 딥러닝 프레임워크를 사용하려면 CUDA를 설치해야 한다.
* GPU에서 병렬 처리를 수행하는 알고리즘을 각종 프로그래밍 언어에 사용할 수 있도록 해주는 GPGPU\(General-Purpose computing on Graphics Processing Units\) 기술

CuDNN

* nvidia CUDA Deep Nerual Network Library의 약자
* 딥러닝 모델을 위한 GPU 가속화 라이브러리의 기초 요소와 같은 일반적인 루틴을 빠르게 이행할 수 있도록 해주는 라이브러리



### 03 파이토치 설치하기

nothing



### 04 반드시 알아야 하는 파이토치 스킬

스칼라, 벡터, 매트릭스, 텐서

* `torch.tensor` 를 이용하여 변수를 선언할 수 있다.

![](../../.gitbook/assets/image%20%28208%29.png)

* +, -, \*, / 를 이용해서 직접적인 사칙연산이 가능하며 `torch.(add/sub/mul/div)`를 이용한 사칙연산도 가능하다
* 행렬 요소간 곱이 아닌 행렬 곱은 `torch.matmul`로 가능하다.

Autograd

* Autograd 방식을 이용해 Back Propagation으로 파라미터를 업데이트 하는 방법을 쉽게 구현할 수 있다.

```python
import torch

if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')
```

* `cuda.is_available()`
  * 현재 파이썬이 실행되고 있는 환경에서 torch moudle을 이용할 때 GPU를 이용해 계산할 수 있는지를 파악하는 메서드
  * 참이면 cuda 장비를, 거짓이면 cpu 장비를 이용해 계산한다.

```python
BATCH_SIZE = 64
INPUT_SIZE = 10000
HIDDEN_SIZE = 100
OUTPUT_SIZE = 10
```

* `BATCH_SIZE`
  * 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터의 개수
  * Input으로 이용되는 데이터가 64개이다.
* `INPUT_SIZE`
  * 딥러닝 모델에서의 Input의 크기이자 입력층의 노드 수를 의미.
  * BATCH\_SIZE와 혼동하면 안됨. BATCH는 갯수를, INPUT은 크기를 의미
  * \(64, 1000\)의 텐서가 입력되고 이 때 마다 가중치 갱신이 일어남
* `HIDDEN_SIZE`
  * 딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수를 의미
  * 입력층에서 은닉층으로 전달됐을 때 은닉층의 노드 수를 의미
* `OUTPUT_SIZE`
  * 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기를 의미

```python
x = torch.randn(BATCH_SIZE, 
                INPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = False)
y = torch.randn(BATCH_SIZE, 
                OUTPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = False)
w1 = torch.randn(INPUT_SIZE, 
                HIDDEN_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = True)
w2 = torch.randn(HIDDEN_SIZE, 
                OUTPUT_SIZE, 
                device = DEVICE, 
                dtype = torch.float, 
                requires_grad = True)
```

* `randn`
  * 평균이 0, 표쥰편차가 1인 정규분포에서 샘플링한 값으로 데이터를 만든다는 것을 의미
* `reguires_grad`
  * 파라미터에 대해서만 Gradient를 계산하면 되므로 가중치에 대해서만 True로 설정

```python
learning_rate = 1e-6
for t in range(1, 501):
    y_pred = x.mm(w1).clamp(min = 0).mm(w2)
    
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 0:
        print("Iteration: ", t, "\t", "Loss: ", loss.item())
    loss.backward()
    
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        
        w1.grad.zero_()
        w2.grad.zero_()
```

* `learning_rate`
  * 학습률을 어떻게 설정하느냐에 따라 Gradient 값에 따른 학습 정도가 결정된다. 딥러닝 모델에서 파라미터 값을 업데이트할 때 가장 중요한 하이퍼 파라미터이기도 하다.
* `y_pred`
  * 딥러닝 모델의 결괏값을 보통 예측값이라고 표현한다.
  * x와 w1과의 행렬 곱 연산에 대해 clamp 함수를 적용하고 다시 w2와의 행렬 곱 연산을 한다.
* `clamp`
  * 여기서는 min만 정의 되었으므로 ReLU와 동일한 역할을 한다.

![](../../.gitbook/assets/image%20%28205%29.png)

* `loss`
  * 예측값과 실제 레이블 값을 비교해 오차를 계산한 값을 loss라고 한다. 여기서는 제곱값의 차를 비교했다.
* `loss.backward()`
  * loss값에 대해 backward\(\) 메서드를 이용하면 각 파리미터 값에 대해 Gradient를 계산하고 이를 통해 Back Propagation을 진행한다는 것을 의미한다.
* `with_torch.no_gard()`
  * 각 파라미터 값에 대해 Gradient를 계산한 결과를 이용해 파라미터 값을 업데이트할 때는 해당 시점의 Gradient 값을 고정한 후 업데이트를 진행해야 한다.
* `w.grad.zero_()`
  * 각 파라미터 값을 업데이트 했다면 Gradient를 초기화 해서 다음 반복문을 진행할 수 있도록 Gradient를 0으로 설정한다.







