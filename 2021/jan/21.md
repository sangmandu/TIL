---
description: TIL
---

# 21 Thu

## \[AI 스쿨 1기\] 7주차 DAY 4

### Deep Learning: 신경망의 기초 - 다층 퍼셉트론 II

퍼셉트론의 한계

* XOR 문제에서 75% 정확도가 한계
* 선형 분리가 불가능한 상황
* 1969년 퍼셉트론의 한계를 지적하고 다층 구조를 이용한 극복 방안 제시하였지만 당시 기술로 불가능
* 1974년 오류 역전파 알고리즘의 신경망 활용 가능성 제시

다층 퍼셉트론의 핵심 아이디어

* 은닉층을 둠 : 유리한 새로운 특징 공간으로 변환
* 시그모이드 활성함수를 도입 : 연성에서는 출력이 연속값
* 오류 역전파 알고리즘 사용 : 기울기를 계산하고 가중치를 갱신

퍼셉트론 2개를 병렬 결합

* 원래 공간 X = \(x1, x2\)를 새로운 특징 공간 Z = \(z1, z2\)로 변환
* x1과 x2의 두 공간으로 나누고 x2의 공간을 z1과 z2로 나눈다
* 선형 분리가 불가능한 공간을 자세히 보니 선형 분리가 가능한 공간의 결합체
* 용량
  * 3개의 퍼셉트론을 결합한 경우
    * 2차원 공간은 7개 영역으로 나누고 각 영역을 3차원 점으로 변환
  * 일반화 하여 p개 퍼셉트론을 결합하면 p차원 공간으로 변환
  * 다층 퍼셉트론의 용량

![](../../.gitbook/assets/image%20%28191%29.png)

* 딱딱한 공간 분할과 부드러운 공간 분할
  * 계단은 딱딱한 함수로 변환 =&gt; 영역을 점으로 변환
    * ex\) 계단 함수
  * 그 외 활성함수는 부드러운 함수로 변환 =&gt; 영역을 영역으로 변환
    * ex\) 로지스틱 시그모이드, 하이퍼볼릭 탄젠트 시그모이드, softplus, rectifier
  * 대표적으로 비선형 함수인 S자 모양의 sigmoid를 활성함수로 사용
  * 그러나 sigmoid는 오류 역전파를 어렵게 함. =&gt; 깊은 신경망에서는 ReLU 사용
* 퍼셉트론의 은닉층 p
  * p가 너무 크면 과잉적합
  * p가 너무 작으면 과소적합
  * 하이퍼 매개변수 최적화가 필요
* 특칭 벡터 x를 출력 벡터 o로 사상\(mapping\)하는 함수로 간주할 수 있음
* 은닉층은 특징 추출기
  * 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환
    * 얼굴이 있는가 =&gt; 눈이 있는가 =&gt; 검은색인가
  * 현대 기계학습에서는 특징학습이라 부름
* 범용적 근사 이론
  * 하나의 은닉층은 함수의 근사를 표현
  * 다층 퍼셉트론도 공간을 변환하는 근사 함수
* 얕은 은닉층의 구조
  * 지수적으로 더 넓은 폭이 필요할 수 있음
  * 더 과잉적합 되기 쉬움
  * 일반적으로 깊은 은닉층의 구조가 좋은 성능을 가짐
* 성능 향상
  * 순수한 최적화 알고리즘으로는 높은 성능 불가능
  * 아키텍처 : 은닉층과 은닉 노드의 개수
  * 초깃값
  * 학습률
  * 활섬함수



### Deep Learning: 신경망의 기초 - 다층 퍼셉트론 III

은닉층을 통한 특징공간의 변환

* 행렬 곱 : 회전
* 편향 : 이동
* 비선형 함수 : 왜곡

목적 함수

* 평균 제곱 오차로 정의
  * 온라인 정의 : 배치모드를 스트리밍 형태로 진행 
  * 배치 모드 : 데이터셋을 N개의 집합으로 나누어 학습 진행

오류 역전파 알고리즘

* 가중치에 대한 결과값의 미분을 가지고 가중치를 얼마만큼 수정해야 결과값이 수정될 지 예측 가능
* 역전파 주요 예

  * add gate



  ![](../../.gitbook/assets/image%20%28189%29.png)

  * mul gate

  ![](../../.gitbook/assets/image%20%28188%29.png)

  * copy gate

  ![](../../.gitbook/assets/image%20%28190%29.png)

  * max gate

  ![](../../.gitbook/assets/image%20%28194%29.png)

* 미분의 연쇄 법칙을 이용
  * 수인 경우, $$ {dz \over dx} = {dz \over dy}{dy \over dz} $$
  * 벡터인 경우, $$ \nabla {x^z} = ({\partial y \over \partial x})^T \nabla {y^z} $$
    * 야코비안 행렬

미니 배치 방식

* 한번에 t개의 샘플을 처리
* t = 1이면 확률론적 경사 하강법
* t = n이면 배치 경사 하강법
* 














### Deep Learning: 신경망의 기초 - 실습 III PT - TF 자동미분연산

