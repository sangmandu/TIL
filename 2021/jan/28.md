---
description: TIL
---

# 28 Thu

## \[AI 스쿨 1기\] 8주차 DAY 4

### Deep Learning: 신경망의 기초 - 심층학습 최적화 III

활성함수

* 선형 연산 결과인 활성값 z에 비선형 활섬함수 t를 적용하는 과정
* 변천사
  * 선형
  * 계단
  * tanh
  * ReLU
* ReLU
  * 경사도 포화 문제 해소
  * max\(0, z\)
  * 한계 : deactivate 됐을 때는 가중치 갱신이 불가 =&gt; Leaky ReLU
* ELU
  * z, if z &gt;= 0
  * a\(e^z - 1\), elif z &lt; 0

배치 정규화

* 공변량 변화, covariate shift
  * 훈련집합과 테스트집합의 분포가 다름
    * 모의고사와 수능의 형태가 다른 예
  * 내부의 공변량 변화
    * 첫번째 층에서 데이터 정규화를 하더라도 연산을 거친 뒤 입력되는 두번째 층의 입력 데이터는 각 배치마다 분포가 다르다
    * 두번째 층 입장에서는 매번 데이터의 분포가 바뀌는 셈. 세번째, 네번째, ... 층에서는 심각해질 수 있음 =&gt; 학습을 방해하는 요인
    * 매번 출력할 때 마다 정규화를 할 필요성을 느낌 =&gt; 배치 정규화
* 배치 정규화
  * 공변량 시프트 현상을 누그러뜨리기 위해 정규화를 층 단위 적용하는 기법
  * 어디에 적용할 것인가?
    * 선형 연산과 비선형 연산 중 어디?
    * 선형 연산을 하고 나서 비선형 연산을 하기 전!
    * 전체 집합 보다는 미니 배치에 적용하는 것이 좋음
  * 과정
    * 미니 배치 단위로 평균과 분산을 계산
    * 평균과 분산을 가지고 정규화
    * 비례와 이동으로 세부 조정
      * 데이터 정규화와 큰 차이점!
      * r\(감마\) : 비례, b\(베타\) : 이동
      * 이 파라미터는 하이퍼는 아니고 학습에 의해 결정
      * 정규화를 한다고 하더라도 데이터 분포가 고르지 않을 수 있기 때문
  * 장점
    * 신경망의 경사도 흐름 개선
      * 학습이 더 잘되기 때문
    * 높은 학습률 허용
      * 안정적인 학습이 가능
    * 초기화에 대한 의존성 감소
    * 의도하지 않았지만 규제와 유사한 행동을 하며, 드롭아웃의 필요성을 감소시킴
      * 드롭아웃의 효과를 어느정도 내기 때문
  * 최적화를 마친 후 추가적인 후처리 작업 필요
    * 각 노드는 전체 훈련집합을 가지고 독립적으로 평균과 분산을 구함
    * 

규제

* 과잉적합
  * 대부분 가지고 있는 데이터에 비해 훨씬 큰 용량의 모델 사용
  * 현대 기계 학습은 충분히 큰 용량의 모델을 설계한 후 학습 과정에서 여러 규제 기법을 적용한다
* 규제
  * 모델 용량에 비해 데이터가 부족한 경우의 부족조건문제를 푸는 접근법
  * 적절한 가정을 투입해 문제를 품
    * 입력과 출력 사이의 변환은 매끄럽다
    * 유사한 데이터는 가깝게 매핑 된다
  * 티호노프의 규제
    * 대표적인 기법
    * 매끄러운 가정에 기반을 둠
    * 통계에서는 릿지 회귀, 기계학습에서는 가중치 감쇄가 대표적

규제기법

* 명시적 규제 : 가중치 감쇠나 드롭아웃처럼 목적함수나 신경망 구조를 직접 수정하는 방식
* 암시적 규제 : 조기 멈춤, 데이터 증대, 잡음 추가, 앙상블 처럼 간접적으로 영향을 미치는 방식
* 규제항
  * 훈련집합과 무관
  * 데이터 생성 과정에 내재한 사전 지식에 해당
  * 모델의 용량을 제한하는 역할
  * 큰 가중치에 벌칙을 가해 작은 가중치를 유지하기 위해 L2놈이나 L1놈을 사용
* L2 놈
  * 규제를 적용한 목적 함수 = 목적 함수 + 규제 항
  * 규제 항 = L2 놈
* 효과
  * 가중치에 대한 선호도 표현
  * 학습 모델을 단순화시킴으로 일반화 성능 향상 시킴
  * 매끄럽게 하여 최적화 개선
* 조기 멈춤
  * 오버 피팅이 발생하기 전까지 학습하는 기계학습 알고리즘
* 데이터 확대
  * 과잉적합을 방지하는 가장 확실한 방법은 큰 훈련집합 사용
  * 데이터 수집은 비용이 많이 들음
  * 데이터 확대라는 규제 기법은 인위적으로 데이터를 변형함
  * 자연계에서 벌어지는 잠재적인 변형을 프로그램으로 흉내내는 셈
  * 아핀 변환 : 이동, 회전, 반전
    * 수작업 변형과 모든 부류가 같은 변형을 사용한다는 한계
  * 모핑을 이용한 변환
    * 비선형 변환 학습
    * 다양한 형태의 변환
    * 자연영상 확대
    * 잡음을 섞어 확대



### Deep Learning: 신경망의 기초 - 심층학습 최적화 IV

드롭아웃

* 완전연결층의 노드 중 일정 비율\(일반적으로 p = 0.5\)을 임의 선택하여 제거 =&gt; 남은 부분 신경망 학습
* 연결이 많다는 것은 오버피팅 될 가능성이 높다는 의미. 특히 완전연결층은 높은 가능성
* 많은 부분 신경망을 만들고, 앙상을 결합하는 기법으로 볼 수 있음

앙상블

* 서로 다른 여러 개의 모델을 결합하여 일반화 오류를 줄이는 기법
* 현대 기계학습은 앙상블도 규제로 여김
* 두 가지 일
  * 서로 다른 예측기
    * 같은 구조를 사용하지만 서로 다른 초기값과 하이퍼 매개변수를 설정하고 학습
    * 서로 다른 구조의 신경망 여러개를 학습
    * 배깅 : 훈련집합을 여러번 랜덤 샘플링 하여 서로 다른 훈련집합을 구성
    * 부스팅 : i번째 예측기가 풀지 못하는 샘플을 i+1번째 예측기가 잘 인식하도록 연계성을 고려
  * 학습된 예측기를 결합 =&gt; 모델 평균
    * 보통 평균을 구하거나 투표하여 최종 결과 결정

하이퍼 매개변수 최적화

* 학습에 의해 결정되는 변수는 매개변수
* 하이퍼 매개변수는 사람이 설정하는 사전변수
* 선택
  * 모델의 성능을 결정하는 요소
  * 표준 참고 문헌이 제시하는 기본값을 사용할 것
* 탐색
  * 격자 탐색
  * 임의 탐색 : 제일 우월함
  * 로그 탐색
* 차원의 저주 문제 발생
  * 매개변수가 m개고 각각이 q개 구간이면 $$ q^m $$개의 점을 조사해야 함

2차 미분을 이용한 최적화

* 경사 하강법
  * 1차 미분을 사용하는 방법
  * 현재 기계 학습의 주류 알고리즘
  * 한계
    * 목표지점으로의 방향을 바로 알 수 없음
    * 각각의 배치 지점에서의 방향만 알 수 있기 때문
  * 두 가지 개선책이 있다
    * 경사도의 잡음을 줄임 =&gt; 미니 배치 사용
    * 2차 미분 정보를 활용
* 2차 미분 최적화
  * 경사도와 헤시안을 사용하여 2차 근사 사용
  * 근사치의 최소값
* 뉴턴 방법
  * 테일러 급수를 적용
  * 너무 어려움 잘 모르겠음
  * 문제점
    * 해시안 행렬 연산 부담 =&gt; 해시안을 근사화하는 LFGS가 많이 사용됨
* 켤레 경사도 방법
  * 직선 탐색 이동 : 이동 크기를 결정하기 위해 직선으로 탐색하고 미분
  * 이전 가중치 방향과 현재 가중치 방향의 중간 방향으로 이동한다.





