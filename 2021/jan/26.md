# 26 Tue

## \[AI 스쿨 1기\] 8주차 DAY 2

### Deep Learning: 신경망의 기초 - 심층학습기초 III

영상 분류

* 과거에는 매우 어렵고 도전적인 문제
* ILSVRC
  * ImageNet Large Scale Visual Recognition Competition
* CPVR

AlexNet

* 구조
  * 컨볼루션층 5개와 완전 연결층 3개
    * 컨볼루션층은 200만개, FC층은 6500만개 가량의 매개변수
    * 향후 CNN,은 FC층의 매개변수를 줄이는 방향으로 발전
  * GPU의 메모리 크기 제한으로 인해 \#1과 \#2로 분하랗여 학습 수행
* 성공한 요인
  * 외적 요인
    * ImageNet이라는 대규모 사전 데이터
    * GPU를 사용한 병렬 처리
  * 내적 요인
    * 활성함수로 ReLU 사용
    * 지역 반응 정규화 기법 적용 \(지금은 사용하지 않음\)
    * 과잉적합을 방지하는 여러 규제 기법 적용
      * 데이터 확대 : 잘라내기와 반전으로 2048배 확대
      * 드롭아웃 : 완전 연결층에서 사용
  * 테스트 단계에서 앙상블 적용

VGGNet

* 핵심 아이디어
  * 3\*3의 작은 커널을 사용
  * 신경망을 더욱 깊게 만듦
  * 컨볼루션층 8~16개를 두었음
    * AlexNet의 5개에 비해 2~3배 깊어짐
  * 16층짜리 VGG-16 : CONV 13 + FC 3
* 작은 커널의 이점
  * 큰 크기의 커널은 결국 여러 개의 작은 크기 커널로 분해될 수 있음
  * 매개변수는 줄이면서 신경망은 깊어지는 효과
* 1\*1 커널
  * 실제로 적용은 구글넷에서 이루어짐
  * 차원 통합
  * 차원 축소 효과 =&gt; 연산량 감소

GoogLeNet

* 구글에서 만듦
* 핵심은 인셉션 모듈
  * 수용장\(입력\)의 다양한 특징을 추출하기 위해 NIN의 구조를 확장하여 복수의 병렬적인 컨볼루션 층을 가짐
* NIN 구조
  * 기존 컨볼루션 연산을 MLPConv 연산으로 대체
    * 커널 대신 비선형 함수를 황섬함수로 포함하는 MLP를 사용하여 특징 추출이 유리함
  * 신경망의 미소 신경망이 주어진 수용장의 특징을 추상화 시도
  * 전역 평균 풀링 사용
* 구글넷은 NIN 개념을 확장한 신경망
  * 네 종류의 컨볼루션 연산을 사용
  * 다양한 특징들을 추출
* 인셉션 모듈을 9개 결합했음
  * 매개변수가 있는 층 22개
  * 없는 층 \(풀링\) 5개
  * 총 27개 층
  * 완전 연결층은 1개에 불과
  * 보조 분류기
    * 원 분류기의 오류 역전파의 결과와 보조 분류기의 오류 역전파 결과를 결합하여 경사 소멸 문제 완화
    * 학습할 때 도우미 역할, 추론할 때는 제거됨

ResNet

* 잔류\(잔차\) 학습이라는 개념을 이용해서 성능 저하를 피하면서 층 수를 대폭 늘림
* 실제로 가중치를 갱신할 때는 W' = W + d 와 같이 이루어지는데 이 때 W' 전체를 학습하기 보다는 d만을 학습시키려고 하는 것
  * d = W' - W
* 지름길 연결을 두는 이유
  * 깊은 신경망도 최적화가 가능해짐
* VGGNet과 같은 점
  * 3\*3 커널 사용
* VGGNet과 다른 점
  * 잔류 학습 사용
  * 전역 평균 풀링 사용 =&gt; FC 층 제거
  * 배치 정규화 적용

생성모델

* 분별 모델은 X가 있을 때 Y를 추정하는 일
* 생성 모델은
  * Y가 있을 때 X를 추정하는 일
  * X를 추정하는 일
  * X와 Y가 공통적으로 발현되는 것을 추정하는 일
* 현실에 내재한 데이터 발생 분포를 알아낼수가 없음
  * 모델과 가설을 통해 추정
  * 명시적으로 추정하는 것도 어려움 =&gt; 암시적으로 표현

GAN의 핵심

* 생성기 G와 분별기 D의 대립구도
* G는 가짜 샘플 생성 \(위조지폐범\)
* D는 가짜와 진짜를 구별 \(경찰\)
* GAN의 목표는 G의 승리

