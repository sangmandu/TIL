---
description: TIL
---

# 27 Wed

## \[AI 스쿨 1기\] 8주차 DAY 3

### Deep Learning: 신경망의 기초 - 심층학습 최적화 I

평균제곱 오차

* 오차가 클수록 e값이 커지기 때문에 정량적 성능\(벌점\)으로 활용됨
* 하지만 e의 절댓값이 커질 경우 활성함수의 미분값이 0에 수렴하기 때문에 더 많은 오류가 있어도 더 적은 갱신이 발생함

교차 엔트로피

* 정답에 해당하는 y가 확률변수
* 확률 분포 : P는 정답, Q는 신경망
* $$ e = -(ylog_2 o + (1-y) log_2(1-o)) $$

소프트맥스 함수

![](../../.gitbook/assets/image%20%2835%29.png)

음의 로그우도 목적함수

* $$ e = -log_2 o_y $$
* 제대로 분류했을 때 목적함수 값이 작음
* 잘못 분류했을 때 목적함수값이 큼

소프트맥스와 로그우도

* 소프트맥스는 최댓값이 아닌 값을 억제하여 0에 가깝게 만든다는 의도 내포
* 신경망에 의한 샘플의 정답에 해당하는 노드만 보겠다는 경우는 로그우도와 잘 어울림
* 따라서 둘을 결합하여 사용하는 경우가 많음





### Deep Learning: 신경망의 기초 - 심층학습 최적화 II





