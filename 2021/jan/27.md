---
description: TIL
---

# 27 Wed

## \[AI 스쿨 1기\] 8주차 DAY 3

### Deep Learning: 신경망의 기초 - 심층학습 최적화 I

평균제곱 오차

* 오차가 클수록 e값이 커지기 때문에 정량적 성능\(벌점\)으로 활용됨
* 하지만 e의 절댓값이 커질 경우 활성함수의 미분값이 0에 수렴하기 때문에 더 많은 오류가 있어도 더 적은 갱신이 발생함

교차 엔트로피

* 정답에 해당하는 y가 확률변수
* 확률 분포 : P는 정답, Q는 신경망
* $$ e = -(ylog_2 o + (1-y) log_2(1-o)) $$

소프트맥스 함수

![](../../.gitbook/assets/image%20%2847%29.png)

음의 로그우도 목적함수

* $$ e = -log_2 o_y $$
* 제대로 분류했을 때 목적함수 값이 작음
* 잘못 분류했을 때 목적함수값이 큼

소프트맥스와 로그우도

* 소프트맥스는 최댓값이 아닌 값을 억제하여 0에 가깝게 만든다는 의도 내포
* 신경망에 의한 샘플의 정답에 해당하는 노드만 보겠다는 경우는 로그우도와 잘 어울림
* 따라서 둘을 결합하여 사용하는 경우가 많음





### Deep Learning: 신경망의 기초 - 심층학습 최적화 II

규모 문제

* 건강에 관련된 데이터 : 키, 몸무게, 혈압
* 1.885m와 1.525m는 33cm나 차이가 나지만 특징 값 차이는 불과 0. 38
* 65.5kg과 45.0kg은 20.5라는 차이
* 따라서 몸무게에 비해 신장은 100여 배 느리게 학습됨
* 불균형하게 학습되는 원인이 됨 =&gt; 키와 몸무게의 수치를 비슷하게 맞춰줘야함

모든 특징이 양수인 경우의 문제

* 수렴이 느리게 되는 문제 발생

정규화는 규모 문제와 양수 문제를 해결해줌

* 특징별 독립적으로 적용
* 통계학의 정규 분포를 활용한 표준화 변환을 적용
* 최대 최소 변환을 적용한 경우 =&gt; 양의 값을 해결하지 못함

명목 변수를 원핫코드로 변환

* 명목 변수 : 객체간 서로 구분하기 위한 변수
  * EX\) 남 : 1, 여 : 2 또는 태양인 : 1, 태음인 : 2, 소양인 : 3, 소음인 : 4
  * 명목 변수는 거리 개념이 없음
  * 원핫 코드는 값의 개수만큼 비트를 부여

대칭적 가중치 문제

* 초기에 \(또는 학습중에\) 가중치가 모두 동일하게 설정되면 이 후에 가중치가 계속적으로 동일하게 갱신됨
* 신경망을 깊게 쌓는 이유가 없음
* 대칭성을 없애기 =&gt; 난수로 가중치 초기화
  * 가우시안 분포 또는 균열 분포에서 난수 추출

다른 방법들

* Saxe2014 : 가중히 행렬의 행과 열이 수직이 되도록 설정
* Sussilo2014 : 임의 행로를 활용
* Sutskever2013 : 가중치 초기화와 가속도를 동시에 최적화
* Mishkin2016 : 가중치 분포가 아니라 노드의 출력 값 분포가 일정하도록 강제화

경사도의 잡음 현상

* 기계 학습은 훈련집합을 이용하여 매개변수의 경사도를 추정하므로 잡음 가능성이 높다
* stochastic gradient는 batch 방식으로 수렴하기 때문에 full gradient보다 크게 꺾이면서 수렴한다.

모멘텀\(관성\)

* 탄력\(가속도 관성\)은 경사도에 부드러움을 가하여 잡음 효과 줄임
  * 관성 : 과거에 이동했던 방식을 기억하면서 기존 방향으로 일정 이상 추가 이동함 =&gt; 수렴 속도 향상 \(지역 최저점과 안장점에 빠지는 문제를 해소\)
* 관성을 적용한 가중치 갱신 수식

![](../../.gitbook/assets/image%20%2835%29.png)

* a\(alpha\)의 효과
  * a = 0이면 관성이 적용 안됨 =&gt; 이전 경사도 갱신 공식과 동일
  * a가 1에 가까울수록 이전 경사도 정보에 큰 가중치를 줌 =&gt; 그리는 궤적이 매끄러움
  * 보통 0.5, 0.9, 0.99를 사용
  * 또는 세대가 지날 수록 0.5에서 0.99까지 증가
* 관성의 효과
  * 지나침 현상\(overshooting\) 누그러뜨림
* 네스테로프 가속 경사도
  * a자리에서 관성을 먼저 받고 이동한 뒤 b 위치에서 경사도를 구해서 c로 이동한다.
  * a자리에서 관성과 경사도를 구해서 b' 로 이동한다.
  * 멈춤에 용이하다 =&gt; 수렴이 잘된다.

학습률 p의 중요성

* 너무 크면 지나침에 따른 진자 현상
* 너무 작으면 수렴이 느림

적응적 학습률

* adaptive learning rates or per-parameter learning rates
* 기존 경사도 갱신은 모든 매개변수에 같은 크기의 학습률을 사용
* 적응적 학습률은 매개변수마다 상황에 따라 학습률을 조절해 사용
  * ex\) 학습률 담금질
    * stimulated annealing
    * 이전 경사도가 현재 경사도랑 방향이 같으면 매개변수의 값을 키운다
    * 방향이 반대라면 값을 줄이는 전략
* AdaGrad
  * Adaptive Gradient
  * r = r + g \(\*\) g
  * 갱신값 $$ \theta = - {p \over e + \sqrt r } \cdot g$$
  * r이 크면 갱신값이 작아서 조금만 이동
  * r이 작으면 갱신값이 커서 많이 이동
  * e는 분모가 0이 되는 것을 방지하기 위함. 보통 10의 -5승에서 -7승 범위 값으로 설정
* RMSProp
  * Adagrad의 단점
    * 현재값이 더 중요한데 과거값이 발목을 잡을 수 있음
  * r = ar + \(1-a\)g\(\*\)g
  * 과거값도 보지만 현재값도 어느 정도 보겠다 라는 점
* Adam
  * RMSProp에 관성을 추가로 적용한 알고리즘



