---
description: TIL
---

# 22 Fri

## \[AI 스쿨 1기\] 7주차 DAY 5

### Deep Learning: 신경망의 기초 - 심층학습기초 I

심층학습

* 딥러닝
* 다층 퍼셉트론에 은닉층을 여러 개 추가하면서 깊은 신경망이 됨
* 심층학습은 깊은 신경망의 학습
* 심층학습은 새로운 응용을 창출하고 인공지능 제품의 성능을 획기적으로 향상
  * 현대 기계학습을 주도



심층학습의 배경

* 1980년대에 이미 깊은 신경망 아이디어 등장
  * 그러나 실현 불가능
  * 다음과 같은 문제 떄문
    1. 경사 소멸 문제\(vanishing gradient\)
    2. 작은 훈련 집합
    3. 과다한 연산과 시간 소요
  * 일부 연구자들은 지속적인 연구
    * 캐나다에서 주로 연구



경사 소멸 문제

1. forward prop
2. compute loss
3. back prop
   * loss를 줄이기 위함
   * 가중치 수정에 대한 정보가 점점 희미해지는 문제



심층학습의 성공 배경

* 혁신적 알고리즘 등장
  * 합성곱 신경망 구조\(CNN\)
    * 부분 연결과 가중치 공유를 통해서 효율적인 신경망 학습 구조 제공
  * 경사 소멸 문제 해결을 위한 ReLU 활성함수
  * 과잉적합을 방지하는데 효과적인 다양한 규제 기법
  * 층별 예비학습 기법 개발
* 값싼 GPGPU의 등장
* 학습 데이터 양과 질의 향상

![](../../.gitbook/assets/image%20%28197%29.png)



기계학습의 새로운 전환

* 전통적인 다층 퍼셉트론
  * 은닉층은 특징 추출기
  * 얕은 구조\(제한적 특징 추출\)이므로 가공하지 않은 획득한 원래 패턴을 그대로 입력하면 낮은 성능을 낸다.
  * 따라서 사람이 수작업 특징을 선택하거나 추출하여 신경망에 입력함
* 현대 기계학습\(심층학습\)
  * 학습에 의해 자동적으로 데이터로부터 특징 추출 =&gt; 표현 학습
  * 특징 벡터를 신경망의 입력 =&gt; 종단간 학습 =&gt; 사람의 개입이 없음



깊은 신경망의 표현 학습

* 낮은 단계 은닉층은 선이나 모서리와 같은 간단한 특징 추출
* 높은 단계 은닉층은 추상적인 형태의 복잡한 특징 추출
* 표현 학습이 강력해짐에 따라 기존 응용에서 획기적인 성능 향상
  * 영상 인식, 음성 인식, 언어 번역 등
  * 새로운 응용 창출
    * 분류나 회귀 뿐만 아니라 생성 모델이나 화소 수준의 영상 분할



깊은 다층 퍼셉트론의 구조

* DMLP 혹은 deep MLP
* MLP의 동작을 나타내는 식을 보다 많은 단계로 확장한 것
* DMLP 학습은 MLP 학습과 유사
  * 경사도 계산과 가중치 갱신을 더 많은 단계에 걸쳐 수행



역사적 고찰

* 활성함수
  * 퍼셉트론 -&gt; 다층 퍼셉트론 -&gt; 깊은 다층 퍼셉트론
* 목적 함수
  * 평균 제곱 오차 -&gt; 평균 제곱 오차 -&gt; 교차 엔트로피 또는 로그우도



심층 학습은 왜 강력한가?

* 종단간 최적화된 학습이 가능하기 때문
  * 고전적 방법은 분할, 특징 추출, 분류를 따로 구현하여 연결
  * 사람의 직관에 의함
  * 심층 학습은 전체 깊은 신경망을 동시에 최적화 =&gt; 종단간 학습
* 깊이의 중요성
  * 점성은 20개 노드를 가진 은닉층 하나 짜리 신경망
  * 실선은 각각 10개 노드를 가진 은닉층 두 개 짜리 신경망
  * 후자가 더 정교한 분할이 가능하다

![](../../.gitbook/assets/image%20%28192%29.png)

* 계층적 특징
  * 입력 층과 가까울 때는 구체적 특징 EX\) 점, 선, 면
  * 출력 층과 가까울 때는 추상적 특징 EX\) 대략적인 모습, 명도









