---
description: TIL
---

# 4 Mon

## 마크다운 수식입력

 이번주차 강의가 수식을 입력할 일이 많아 아래 사이트를 소개한다. \(물론 나에게\)

  
[https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

## \[프로그래머스 AI 스쿨 1기\] 5주차 DAY 1

### Machine Learning 기초 - 소개 

####  머신러닝이란?

* 기계학습. 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘의 연구.
* 학습데이터 : 입력벡터들과 목표값들
* 머신러닝 알고리즘의 결과는 목표값을 예측하는 함수 
* 숫자 인식에서 입력벡터는 손글씨 이미지, 목표값은 0부터 9까지 숫자중 예측값. 정확히는 10개의 클래스 중 한 클래스\(이 때 클래스 넘버는 1부터 시작함\)

####  핵심개념s

*  학습단계: 함수 y\(x\)를 학습데이터에 기반해 결정하는 단계 
*  시험셋 : 모델을 평가하기 위해 사용하는 새로운 데이터 
*  일반화 : 모델에서 학습에 사용된 데이터가 아닌 이전에 접하지 못한 로운 데이터에 대해 올바른 예측을 수행하는 역량 
*  지도학습 : 분류와 회귀 
*  비지도학습 : 군집 

####  다항식 곡선 근사 

![](../../.gitbook/assets/image%20%2897%29.png)

* Polynomial Curve Fitting
*  회귀 문제에 해당한다.
*  점들을 지나는\(지나지 못하더라도 최대한 가깝게\) 함수 구하기 
* 학습데이터 : 입력벡터와 목표값 
* 목표 : 새로운 입력벡터가 주어질 때 목표값을 예측하는 것 
* 확률이론 : 예측값의 불확실성을 정량화시켜 표현할 수 있는 수학적 프레임워크 제공 
* 결정이론 : 확률적 표현을 바탕으로 최적의 예측을 수행할 수 있는 방법론 제공 

![](../../.gitbook/assets/image%20%2896%29.png)

####  과소적합과 과대적합

 실제 학습 데이터에 크기에 비해 너무 고차원 함수 또는 너무 저차원 함수를 사용하면 실제 성능에서 에러가 많이 발생한다.

 $$ E_{RMS} = \sqrt {2E(w^*)/N} $$ : Root Mean Square

  또한, 고차원 함수더라도 많은 양에 데이터가 존재한다면 과대적합이 발생할 가능성이 적어진다. 그리고 실제로 머신러닝의 데이터 수는 굉장히 많다는 점.

####  규제화\(Regularization\)

 파라미터값이 너무 커지지 않도록 하는 방법.

![](../../.gitbook/assets/image%20%2893%29.png)

  이 때 너무 심하게 하면 과대/과소 적합이 예기치 않게 발생할 수 있음. 



### Machine Learning 기초 - 확률이론1

####  확률 변수 

 확률 변수 X는 표본의 집합 S의 원소 e를 실수값 X\(e\) = x에 대응시키는 함수이다.

*  대문자 X, Y, ... : 확률 변수 
*  소문자 x, y, ... : 확률 변수가 가질 수 있는 값 
*  확률 P는 집합 S의 부분집합을 실수값에 대응시키는 함
* ex\) S = {HH, HT, TH, TT}; throwing coin
* X\(HH\) = 2, X\(HT\) = 1, X\(TH\) = 1, X\(TT\) = 0; head of coin appear
* P\[X = 1\] = P\[{HT, TH}\] = $$\frac {2} {4} = \frac {1} {2}$$

####  연속 확률 변수\(Continuous Random Variables\)

 누적분포함수 F\(x\) = P\[X $$ \in$$\(-$$ \infty$$, x\)\] 일 때,  F\(x\)를 가진 확률 변수 X에 대해서 다음을 만족하는 함수 f\(x\)가 존재한다면 X를 연속 확률 변수라고 부르고 f\(x\)를 X의 확률 밀도 함수\(probability density function\)라고 부른다.

*  확률 변수를 명확히 하기 위해 F\(x\), f\(x\)로 쓰기로 하며 밀도 함수의 경우에는 p\(x\)를 사용하기도 한다.

####   확률 변수의 성질 

*  덧셈 법칙 
*  곱셈 법칙 

![](../../.gitbook/assets/image%20%2894%29.png)

*  베이즈 확률 \(posterior 사후확률, likelihood 가능성, prior 사전확률, marginal normalization 경계확률\)

![](../../.gitbook/assets/image%20%2898%29.png)

####  확률변수의 함수 

 확률변수 X의 함수 Y = f\(X\)도 확률변수이다. 예를 들어 확률 변수 X가 주\(week\)의 수로 표현되었다고 하면 일\(day\)의 수로 표현된 새로운 확률변수를 정의할 수 있다. 

* Y = 7X
* P\[14 &lt;= Y &lt;= 21\] = P\[2 &lt;= X &lt;= 3\]
* $$ p_y(y) = p_x(x)|\frac {dx} {dy} | $$

 k차원의 확률변수 벡터 x = \(x1, ... , xk\)가 주어질 때, k개의 x에 관한 함수들은 새로운 확률변수벡터 y = \(y1, ... yk\)를 정의한다. 간략하게 y = \(x\)로 나타낼 수 있다. 만약 y = g\(x\)가 일대일 변환인 경우\(x = w\(y\)로 유일한 해를 가질 때\), y의 결합확률밀도함수는 다음과 같다.

* $$ p_y (y_1, ... , y_k) = p_x(x_1, ... , x_k)|J|$$
* where J = $$  \begin{matrix} \frac {dx_1} {dy_1} & \frac {dx_1} {dy_2} & ... & \frac {dx_1} {dy_k}  \\ \frac {dx_2} {dy_1} & ... & ... & ... \\ ... \\ \frac {dx_k} {dy_1} & ... & ... & \frac {dx_k} {dy_k} \end{matrix}$$

 예제

 $$ p_{x1,x2} (x1, x2) = e^{-(x_1+x_2)}, x1 > 0, x2 > 0$$일 때, $$ $$$$ y_1 = x_1, y_2 = x_1 + x_2 $$에 의해서 정의되는 y의 pdf는?

 $$ f{y_1, y_2}(y_1, y_2) =  f{x_1, x_2}(x_1, x_2)|J| =  f{x_1, x_2}(y_1, y_2 - y_1) = e^{-\{-y1 + y2 -y1\}} = e^{y2}$$

 $$ 0 < y_1 < y_2 < \infty$$

 $$ f_{y_1}(y_1) = \int^\infty _{y_1} e^{-y_2} = -e^{-y_2} |^\infty _{y_1} = e^{-y_1}$$

####  Inverse CDF Technique

 확률 변수 X가 CDF F\(x\)를 가질 때 연속확률분포함수의 함수로 정의되는 다음 확률변수 Y를 생각해보자.

 







