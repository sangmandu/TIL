---
description: TIL
---

# 4 Mon

## \[인프런\] 단 두 장의 문서로 데이터 분석과 시각화 뽀개기

### df.sort\_values, rename, sort\_index, reset\_index로 데이터 프레임 Reshaping 하기 - Reshaping Data <a id="df.sort_values,-rename,-sort_index,-reset_index&#xB85C;-&#xB370;&#xC774;&#xD130;-&#xD504;&#xB808;&#xC784;-Reshaping-&#xD558;&#xAE30;---Reshaping-Data"></a>

```text
import pandas as pd
import numpy as np
import seaborn as sns # 시각화 툴
```

```text
df = sns.load_dataset("mpg")
df.shape
```

```text
(398, 9)
```

```text
df.head()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu |
| 1 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 |
| 2 | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite |
| 3 | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst |
| 4 | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino |

```text
df.sort_values('mpg')
Order rows by values of a column (low to high).
df.sort_values('mpg',ascending=False)
Order rows by values of a column (high to low).
```

```text
df.sort_values('mpg').head()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 28 | 9.0 | 8 | 304.0 | 193.0 | 4732 | 18.5 | 70 | usa | hi 1200d |
| 25 | 10.0 | 8 | 360.0 | 215.0 | 4615 | 14.0 | 70 | usa | ford f250 |
| 26 | 10.0 | 8 | 307.0 | 200.0 | 4376 | 15.0 | 70 | usa | chevy c20 |
| 103 | 11.0 | 8 | 400.0 | 150.0 | 4997 | 14.0 | 73 | usa | chevrolet impala |
| 124 | 11.0 | 8 | 350.0 | 180.0 | 3664 | 11.0 | 73 | usa | oldsmobile omega |

```text
df.sort_values('mpg', ascending=False).head()
# default는 True이다
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 322 | 46.6 | 4 | 86.0 | 65.0 | 2110 | 17.9 | 80 | japan | mazda glc |
| 329 | 44.6 | 4 | 91.0 | 67.0 | 1850 | 13.8 | 80 | japan | honda civic 1500 gl |
| 325 | 44.3 | 4 | 90.0 | 48.0 | 2085 | 21.7 | 80 | europe | vw rabbit c \(diesel\) |
| 394 | 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 82 | europe | vw pickup |
| 326 | 43.4 | 4 | 90.0 | 48.0 | 2335 | 23.7 | 80 | europe | vw dasher \(diesel\) |

```text
df.sort_values?
```

```text
df.rename(columns = {'y':'year'})
Rename the columns of a DataFrame
```

```text
df = df.rename(columns = {'model_year' : 'year'})
df.head()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu |
| 1 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 |
| 2 | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite |
| 3 | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst |
| 4 | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino |

```text
df.sort_index()
Sort the index of a DataFrame
df.reset_index()
Reset index of DataFrame to row numbers, moving
index to columns
```

```text
df.sort_index().head(10)
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu |
| 1 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 |
| 2 | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite |
| 3 | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst |
| 4 | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino |
| 5 | 15.0 | 8 | 429.0 | 198.0 | 4341 | 10.0 | 70 | usa | ford galaxie 500 |
| 6 | 14.0 | 8 | 454.0 | 220.0 | 4354 | 9.0 | 70 | usa | chevrolet impala |
| 7 | 14.0 | 8 | 440.0 | 215.0 | 4312 | 8.5 | 70 | usa | plymouth fury iii |
| 8 | 14.0 | 8 | 455.0 | 225.0 | 4425 | 10.0 | 70 | usa | pontiac catalina |
| 9 | 15.0 | 8 | 390.0 | 190.0 | 3850 | 8.5 | 70 | usa | amc ambassador dpl |

```text
df.reset_index().head(10)
# index가 없을 때 index를 새로 생성
```

|  | index | mpg | cylinders | displacement | horsepower | weight | acceleration | year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 0 | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu |
| 1 | 1 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 |
| 2 | 2 | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite |
| 3 | 3 | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst |
| 4 | 4 | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino |
| 5 | 5 | 15.0 | 8 | 429.0 | 198.0 | 4341 | 10.0 | 70 | usa | ford galaxie 500 |
| 6 | 6 | 14.0 | 8 | 454.0 | 220.0 | 4354 | 9.0 | 70 | usa | chevrolet impala |
| 7 | 7 | 14.0 | 8 | 440.0 | 215.0 | 4312 | 8.5 | 70 | usa | plymouth fury iii |
| 8 | 8 | 14.0 | 8 | 455.0 | 225.0 | 4425 | 10.0 | 70 | usa | pontiac catalina |
| 9 | 9 | 15.0 | 8 | 390.0 | 190.0 | 3850 | 8.5 | 70 | usa | amc ambassador dpl |

```text
df.drop(columns=['Length','Height'])
Drop columns from DataFrame
```

```text
df.drop(columns=['mpg','year']).head(10)
```

|  | cylinders | displacement | horsepower | weight | acceleration | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | usa | chevrolet chevelle malibu |
| 1 | 8 | 350.0 | 165.0 | 3693 | 11.5 | usa | buick skylark 320 |
| 2 | 8 | 318.0 | 150.0 | 3436 | 11.0 | usa | plymouth satellite |
| 3 | 8 | 304.0 | 150.0 | 3433 | 12.0 | usa | amc rebel sst |
| 4 | 8 | 302.0 | 140.0 | 3449 | 10.5 | usa | ford torino |
| 5 | 8 | 429.0 | 198.0 | 4341 | 10.0 | usa | ford galaxie 500 |
| 6 | 8 | 454.0 | 220.0 | 4354 | 9.0 | usa | chevrolet impala |
| 7 | 8 | 440.0 | 215.0 | 4312 | 8.5 | usa | plymouth fury iii |
| 8 | 8 | 455.0 | 225.0 | 4425 | 10.0 | usa | pontiac catalina |
| 9 | 8 | 390.0 | 190.0 | 3850 | 8.5 | usa | amc ambassador dpl |

```text
df.drop(columns=['mpg','year', 'name']).head(10)
```

|  | cylinders | displacement | horsepower | weight | acceleration | origin |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | usa |
| 1 | 8 | 350.0 | 165.0 | 3693 | 11.5 | usa |
| 2 | 8 | 318.0 | 150.0 | 3436 | 11.0 | usa |
| 3 | 8 | 304.0 | 150.0 | 3433 | 12.0 | usa |
| 4 | 8 | 302.0 | 140.0 | 3449 | 10.5 | usa |
| 5 | 8 | 429.0 | 198.0 | 4341 | 10.0 | usa |
| 6 | 8 | 454.0 | 220.0 | 4354 | 9.0 | usa |
| 7 | 8 | 440.0 | 215.0 | 4312 | 8.5 | usa |
| 8 | 8 | 455.0 | 225.0 | 4425 | 10.0 | usa |
| 9 | 8 | 390.0 | 190.0 | 3850 | 8.5 | usa |

### melt, pivot 으로 Tidy Data 만들기 - Reshaping Data, Method Chaining <a id="melt,-pivot-&#xC73C;&#xB85C;-Tidy-Data-&#xB9CC;&#xB4E4;&#xAE30;---Reshaping-Data,-Method-Chaining"></a>

```text
pd.melt(df)
Gather columns into rows.
```

```text
pd.melt?
```

```text
df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
...                    'B': {0: 1, 1: 3, 2: 5},
...                    'C': {0: 2, 1: 4, 2: 6}})
df
```

|  | A | B | C |
| :--- | :--- | :--- | :--- |
| 0 | a | 1 | 2 |
| 1 | b | 3 | 4 |
| 2 | c | 5 | 6 |

```text
pd.melt(df, id_vars=['A'], value_vars=['B'])
```

|  | A | variable | value |
| :--- | :--- | :--- | :--- |
| 0 | a | B | 1 |
| 1 | b | B | 3 |
| 2 | c | B | 5 |

```text
pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
```

|  | A | variable | value |
| :--- | :--- | :--- | :--- |
| 0 | a | B | 1 |
| 1 | b | B | 3 |
| 2 | c | B | 5 |
| 3 | a | C | 2 |
| 4 | b | C | 4 |
| 5 | c | C | 6 |

```text
pd.melt(df, value_vars=['A','B','C'])
```

|  | variable | value |
| :--- | :--- | :--- |
| 0 | A | a |
| 1 | A | b |
| 2 | A | c |
| 3 | B | 1 |
| 4 | B | 3 |
| 5 | B | 5 |
| 6 | C | 2 |
| 7 | C | 4 |
| 8 | C | 6 |

```text
pd.melt(df, value_vars=['A','B','C']).rename(columns={
'variable' : 'var',
'value' : 'val'})
```

|  | var | val |
| :--- | :--- | :--- |
| 0 | A | a |
| 1 | A | b |
| 2 | A | c |
| 3 | B | 1 |
| 4 | B | 3 |
| 5 | B | 5 |
| 6 | C | 2 |
| 7 | C | 4 |
| 8 | C | 6 |

```text
df.pivot(columns='var', values='val')
Spread rows into columns.
```

```text
df.pivot?
```

```text
df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',
...                            'two'],
...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],
...                    'baz': [1, 2, 3, 4, 5, 6],
...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})
df
```

|  | foo | bar | baz | zoo |
| :--- | :--- | :--- | :--- | :--- |
| 0 | one | A | 1 | x |
| 1 | one | B | 2 | y |
| 2 | one | C | 3 | z |
| 3 | two | A | 4 | q |
| 4 | two | B | 5 | w |
| 5 | two | C | 6 | t |

```text
df2 = df.pivot(index='foo', columns='bar', values='baz')
# row에 있던 값들이 column으로 이동
df2
```

| bar | A | B | C |
| :--- | :--- | :--- | :--- |
| foo |  |  |  |
| one | 1 | 2 | 3 |
| two | 4 | 5 | 6 |

```text
df3 = df.pivot(index='foo', columns='bar', values='baz').reset_index()
df3
```

| bar | foo | A | B | C |
| :--- | :--- | :--- | :--- | :--- |
| 0 | one | 1 | 2 | 3 |
| 1 | two | 4 | 5 | 6 |

```text
df3.melt(id_vars=['foo'], value_vars=['A','B','C'])
```

|  | foo | bar | value |
| :--- | :--- | :--- | :--- |
| 0 | one | A | 1 |
| 1 | two | A | 4 |
| 2 | one | B | 2 |
| 3 | two | B | 5 |
| 4 | one | C | 3 |
| 5 | two | C | 6 |

```text
df3.melt(id_vars=['foo'], value_vars=['A','B','C']).sort_values('bar')
```

|  | foo | bar | value |
| :--- | :--- | :--- | :--- |
| 0 | one | A | 1 |
| 1 | two | A | 4 |
| 2 | one | B | 2 |
| 3 | two | B | 5 |
| 4 | one | C | 3 |
| 5 | two | C | 6 |

```text
df3.melt(id_vars=['foo'], value_vars=['A','B','C']).sort_values(['foo', 'bar'])
```

|  | foo | bar | value |
| :--- | :--- | :--- | :--- |
| 0 | one | A | 1 |
| 2 | one | B | 2 |
| 4 | one | C | 3 |
| 1 | two | A | 4 |
| 3 | two | B | 5 |
| 5 | two | C | 6 |

```text
df3.melt(id_vars=['foo'], value_vars=['A','B','C']).sort_values(['foo', 'bar']).rename(columns = {'value': 'baz'})
```

|  | foo | bar | baz |
| :--- | :--- | :--- | :--- |
| 0 | one | A | 1 |
| 2 | one | B | 2 |
| 4 | one | C | 3 |
| 1 | two | A | 4 |
| 3 | two | B | 5 |
| 5 | two | C | 6 |

### pd.concat\(\[df1,df2\]\) 시리즈, 데이터프레임 합치기 - Reshaping Data <a id="pd.concat([df1,df2])-&#xC2DC;&#xB9AC;&#xC988;,-&#xB370;&#xC774;&#xD130;&#xD504;&#xB808;&#xC784;-&#xD569;&#xCE58;&#xAE30;---Reshaping-Data"></a>

```text
pd.concat([df1,df2])
Append rows of DataFrames
pd.concat([df1,df2], axis=1)
Append columns of DataFrames
```

```text
pd.concat?
```

```text
s1 = pd.Series(['a', 'b'])
s1
```

```text
0    a
1    b
dtype: object
```

```text
s2 = pd.Series(['c', 'd'])
s2
```

```text
0    c
1    d
dtype: object
```

```text
pd.concat([s1, s2])
```

```text
0    a
1    b
0    c
1    d
dtype: object
```

```text
pd.concat([s1, s2], ignore_index=True)
```

```text
0    a
1    b
2    c
3    d
dtype: object
```

```text
pd.concat([s1, s2], keys=['s1', 's2'])
```

```text
s1  0    a
    1    b
s2  0    c
    1    d
dtype: object
```

```text
pd.concat([s1, s2], keys=['s1', 's2'],
...           names=['Series name', 'Row ID'])
```

```text
Series name  Row ID
s1           0         a
             1         b
s2           0         c
             1         d
dtype: object
```

```text
df1 = pd.DataFrame([['a', 1], ['b', 2]],
...                    columns=['letter', 'number'])
df1
```

|  | letter | number |
| :--- | :--- | :--- |
| 0 | a | 1 |
| 1 | b | 2 |

```text
df2 = pd.DataFrame([['c', 3], ['d', 4]],
...                    columns=['letter', 'number'])
df2
```

|  | letter | number |
| :--- | :--- | :--- |
| 0 | c | 3 |
| 1 | d | 4 |

```text
pd.concat([df1, df2])
```

|  | letter | number |
| :--- | :--- | :--- |
| 0 | a | 1 |
| 1 | b | 2 |
| 0 | c | 3 |
| 1 | d | 4 |

```text
df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],
...                    columns=['letter', 'number', 'animal'])
df3
```

|  | letter | number | animal |
| :--- | :--- | :--- | :--- |
| 0 | c | 3 | cat |
| 1 | d | 4 | dog |

```text
# 형태가 다른 두개의 데이터 프레임 합치기
pd.concat([df1, df3])
```

|  | letter | number | animal |
| :--- | :--- | :--- | :--- |
| 0 | a | 1 | NaN |
| 1 | b | 2 | NaN |
| 0 | c | 3 | cat |
| 1 | d | 4 | dog |

```text
pd.concat([df1, df3], join="inner")
```

|  | letter | number |
| :--- | :--- | :--- |
| 0 | a | 1 |
| 1 | b | 2 |
| 0 | c | 3 |
| 1 | d | 4 |

```text
df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],
...                    columns=['animal', 'name'])
df4
```

|  | animal | name |
| :--- | :--- | :--- |
| 0 | bird | polly |
| 1 | monkey | george |

```text
df5 = pd.DataFrame([1], index=['a'])
df5
```

|  | 0 |
| :--- | :--- |
| a | 1 |

```text
df6 = pd.DataFrame([2], index=['a'])
df6
```

|  | 0 |
| :--- | :--- |
| a | 2 |

```text
pd.concat([df5, df6])
```

|  | 0 |
| :--- | :--- |
| a | 1 |
| a | 2 |

```text
pd.concat([df5, df6], verify_integrity=True)
# 오류 발생 : ValueError
```

### merge로 데이터프레임 합치기 left, right, inner, outer 옵션 사용하기 - Combine Data Sets <a id="merge&#xB85C;-&#xB370;&#xC774;&#xD130;&#xD504;&#xB808;&#xC784;-&#xD569;&#xCE58;&#xAE30;-left,-right,-inner,-outer-&#xC635;&#xC158;-&#xC0AC;&#xC6A9;&#xD558;&#xAE30;---Combine-Data-Sets"></a>

```text
import pandas as pd
```

```text
adf = pd.DataFrame({"x1" : ["A", "B", "C"], "x2" : [1, 2, 3]})
adf
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | A | 1 |
| 1 | B | 2 |
| 2 | C | 3 |

```text
bdf = pd.DataFrame({"x1" : ["A", "B", "D"], "x3" : ["T", "F", "T"]})
bdf
```

|  | x1 | x3 |
| :--- | :--- | :--- |
| 0 | A | T |
| 1 | B | F |
| 2 | D | T |

```text
pd.merge(adf, bdf,
how='left', on='x1')
Join matching rows from bdf to adf.
pd.merge(adf, bdf,
how='right', on='x1')
Join matching rows from adf to bdf.
pd.merge(adf, bdf,
how='inner', on='x1')
Join data. Retain only rows in both sets.
pd.merge(adf, bdf,
how='outer', on='x1')
Join data. Retain all values, all rows.
```

```text
pd.merge(adf, bdf, how='left', on='x1')
```

|  | x1 | x2 | x3 |
| :--- | :--- | :--- | :--- |
| 0 | A | 1 | T |
| 1 | B | 2 | F |
| 2 | C | 3 | NaN |

```text
pd.merge(adf, bdf, how='right', on='x1')
```

|  | x1 | x2 | x3 |
| :--- | :--- | :--- | :--- |
| 0 | A | 1.0 | T |
| 1 | B | 2.0 | F |
| 2 | D | NaN | T |

```text
pd.merge(adf, bdf, how='inner', on='x1')
```

|  | x1 | x2 | x3 |
| :--- | :--- | :--- | :--- |
| 0 | A | 1 | T |
| 1 | B | 2 | F |

```text
pd.merge(adf, bdf, how='outer', on='x1')
```

|  | x1 | x2 | x3 |
| :--- | :--- | :--- | :--- |
| 0 | A | 1.0 | T |
| 1 | B | 2.0 | F |
| 2 | C | 3.0 | NaN |
| 3 | D | NaN | T |

```text
adf[adf.x1.isin(bdf.x1)]
All rows in adf that have a match in bdf.

adf[~adf.x1.isin(bdf.x1)]
All rows in adf that do not have a match in bdf.
```

```text
adf.x1.isin(bdf.x1)
```

```text
0     True
1     True
2    False
Name: x1, dtype: bool
```

```text
adf[adf.x1.isin(bdf.x1)]
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | A | 1 |
| 1 | B | 2 |

```text
adf[~adf.x1.isin(bdf.x1)]
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 2 | C | 3 |

```text
pd.merge(ydf, zdf)
Rows that appear in both ydf and zdf
(Intersection).

pd.merge(ydf, zdf, how='outer')
Rows that appear in either or both ydf and zdf
(Union).

pd.merge(ydf, zdf, how='outer',
indicator=True)
.query('_merge == "left_only"')
.drop(columns=['_merge'])
Rows that appear in ydf but not zdf (Setdiff).
```

```text
ydf = pd.DataFrame({"x1" : ["A", "B", "C"], "x2" : [1, 2, 3]})
ydf
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | A | 1 |
| 1 | B | 2 |
| 2 | C | 3 |

```text
zdf = pd.DataFrame({"x1" : ["B", "C", "D"], "x2" : [2, 3, 4]})
zdf
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | B | 2 |
| 1 | C | 3 |
| 2 | D | 4 |

```text
pd.merge(ydf, zdf)
# default : inner join
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | B | 2 |
| 1 | C | 3 |

```text
pd.merge(ydf, zdf, how='outer')
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | A | 1 |
| 1 | B | 2 |
| 2 | C | 3 |
| 3 | D | 4 |

```text
pd.merge(ydf, zdf, how='outer', indicator=True)
```

|  | x1 | x2 | \_merge |
| :--- | :--- | :--- | :--- |
| 0 | A | 1 | left\_only |
| 1 | B | 2 | both |
| 2 | C | 3 | both |
| 3 | D | 4 | right\_only |

```text
pd.merge(ydf, zdf, how='outer', indicator=True).query('_merge == "left_only"')
```

|  | x1 | x2 | \_merge |
| :--- | :--- | :--- | :--- |
| 0 | A | 1 | left\_only |

```text
pd.merge(ydf, zdf, how='outer',indicator=True).query('_merge == "left_only"').drop(columns=['_merge'])
```

|  | x1 | x2 |
| :--- | :--- | :--- |
| 0 | A | 1 |

concat은 위아래로 합칠 때, mergs는 좌우로 합칠 때 사용하면 좋다

### 파이썬 판다스로 groupby 활용하여 다양한 데이터 집계를 활용하기 - Group Data <a id="&#xD30C;&#xC774;&#xC36C;-&#xD310;&#xB2E4;&#xC2A4;&#xB85C;-groupby-&#xD65C;&#xC6A9;&#xD558;&#xC5EC;-&#xB2E4;&#xC591;&#xD55C;-&#xB370;&#xC774;&#xD130;-&#xC9D1;&#xACC4;&#xB97C;-&#xD65C;&#xC6A9;&#xD558;&#xAE30;---Group-Data"></a>

```text
import pandas as pd
import seaborn as sns
```

```text
df.groupby(by="col")
Return a GroupBy object,
grouped by values in column
named "col".

df.groupby(level="ind")
Return a GroupBy object,
grouped by values in index
level named "ind".

size()
Size of each group.

agg(function)
Aggregate group using function.
```

```text
df = sns.load_dataset("mpg")
df.head()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | origin | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu |
| 1 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 |
| 2 | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite |
| 3 | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst |
| 4 | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino |

```text
df.groupby(by="origin")
```

```text
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000017AD720BE88>
```

```text
df.groupby(by="origin").size()
```

```text
origin
europe     70
japan      79
usa       249
dtype: int64
```

```text
df['origin'].value_counts()
```

```text
usa       249
japan      79
europe     70
Name: origin, dtype: int64
```

```text
df.groupby(by="origin").max()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| origin |  |  |  |  |  |  |  |  |
| europe | 44.3 | 6 | 183.0 | 133.0 | 3820 | 24.8 | 82 | vw rabbit custom |
| japan | 46.6 | 6 | 168.0 | 132.0 | 2930 | 21.0 | 82 | toyouta corona mark ii \(sw\) |
| usa | 39.0 | 8 | 455.0 | 230.0 | 5140 | 22.2 | 82 | pontiac ventura sj |

```text
df.groupby(by="origin").min()
```

|  | mpg | cylinders | displacement | horsepower | weight | acceleration | model\_year | name |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| origin |  |  |  |  |  |  |  |  |
| europe | 16.2 | 4 | 68.0 | 46.0 | 1825 | 12.2 | 70 | audi 100 ls |
| japan | 18.0 | 3 | 70.0 | 52.0 | 1613 | 11.4 | 70 | datsun 1200 |
| usa | 9.0 | 4 | 85.0 | 52.0 | 1800 | 8.0 | 70 | amc ambassador brougham |

```text
df.groupby(by="origin")['weight'].mean()
```

```text
origin
europe    2423.300000
japan     2221.227848
usa       3361.931727
Name: weight, dtype: float64
```

```text
df.groupby(by="origin")['weight'].median()
```

```text
origin
europe    2240
japan     2155
usa       3365
Name: weight, dtype: int64
```

```text
df.groupby?
```

```text
df.groupby(['origin', 'model_year'])['cylinders'].mean()
```

```text
origin  model_year
europe  70            4.000000
        71            4.000000
        72            4.000000
        73            4.000000
        74            4.000000
        75            4.000000
        76            4.250000
        77            4.000000
        78            4.833333
        79            4.250000
        80            4.111111
        81            4.500000
        82            4.000000
japan   70            4.000000
        71            4.000000
        72            3.800000
        73            4.250000
        74            4.000000
        75            4.000000
        76            4.500000
        77            4.166667
        78            4.000000
        79            4.000000
        80            4.076923
        81            4.333333
        82            4.000000
usa     70            7.636364
        71            6.200000
        72            6.888889
        73            7.241379
        74            6.266667
        75            6.400000
        76            6.363636
        77            6.222222
        78            6.000000
        79            6.260870
        80            4.285714
        81            4.923077
        82            4.300000
Name: cylinders, dtype: float64
```

```text
pd.DataFrame(df.groupby(['origin', 'model_year'])['cylinders'].mean())
```

|  |  | cylinders |
| :--- | :--- | :--- |
| origin | model\_year |  |
| europe | 70 | 4.000000 |
| 71 | 4.000000 |  |
| 72 | 4.000000 |  |
| 73 | 4.000000 |  |
| 74 | 4.000000 |  |
| 75 | 4.000000 |  |
| 76 | 4.250000 |  |
| 77 | 4.000000 |  |
| 78 | 4.833333 |  |
| 79 | 4.250000 |  |
| 80 | 4.111111 |  |
| 81 | 4.500000 |  |
| 82 | 4.000000 |  |
| japan | 70 | 4.000000 |
| 71 | 4.000000 |  |
| 72 | 3.800000 |  |
| 73 | 4.250000 |  |
| 74 | 4.000000 |  |
| 75 | 4.000000 |  |
| 76 | 4.500000 |  |
| 77 | 4.166667 |  |
| 78 | 4.000000 |  |
| 79 | 4.000000 |  |
| 80 | 4.076923 |  |
| 81 | 4.333333 |  |
| 82 | 4.000000 |  |
| usa | 70 | 7.636364 |
| 71 | 6.200000 |  |
| 72 | 6.888889 |  |
| 73 | 7.241379 |  |
| 74 | 6.266667 |  |
| 75 | 6.400000 |  |
| 76 | 6.363636 |  |
| 77 | 6.222222 |  |
| 78 | 6.000000 |  |
| 79 | 6.260870 |  |
| 80 | 4.285714 |  |
| 81 | 4.923077 |  |
| 82 | 4.300000 |  |

```text
df.pivot_table?
```

```text
Signature:
df.pivot_table(
    values=None,
    index=None,
    columns=None,
    aggfunc='mean',
    fill_value=None,
    margins=False,
    dropna=True,
    margins_name='All',
    observed=False,
) -> 'DataFrame'
```

```text
df2 = pd.DataFrame(
[[4, 7, 10],
[5, 8, 11],
[6, 9, 12]],
index=[1, 2, 3],
columns=['a', 'b', 'c'])
df2
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 7 | 10 |
| 2 | 5 | 8 | 11 |
| 3 | 6 | 9 | 12 |

```text
df2.shift(1)
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | NaN | NaN | NaN |
| 2 | 4.0 | 7.0 | 10.0 |
| 3 | 5.0 | 8.0 | 11.0 |

```text
df2.shift(-1)
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 5.0 | 8.0 | 11.0 |
| 2 | 6.0 | 9.0 | 12.0 |
| 3 | NaN | NaN | NaN |

```text
df2['a'].shift(2)
```

```text
1    NaN
2    NaN
3    4.0
Name: a, dtype: float64
```

```text
df2['b'].shift(-1)
```

```text
1    8.0
2    9.0
3    NaN
Name: b, dtype: float64
```

```text
df2['b'] = df2['b'].shift(-1)
df2
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 5 | NaN | 11 |
| 3 | 6 | NaN | 12 |

```text
df['model_year']
```

```text
0      70
1      70
2      70
3      70
4      70
       ..
393    82
394    82
395    82
396    82
397    82
Name: model_year, Length: 398, dtype: int64
```

```text
df['model_year'].rank(method='max')
# 해당 값이 큰 순으로 몇번째 순서인지
```

```text
0       29.0
1       29.0
2       29.0
3       29.0
4       29.0
       ...  
393    398.0
394    398.0
395    398.0
396    398.0
397    398.0
Name: model_year, Length: 398, dtype: float64
```

```text
df['model_year'].rank(method='min')
# 해당 값이 작은 순으로 몇번째 순서인지
```

```text
0        1.0
1        1.0
2        1.0
3        1.0
4        1.0
       ...  
393    368.0
394    368.0
395    368.0
396    368.0
397    368.0
Name: model_year, Length: 398, dtype: float64
```

```text
df['model_year'].rank(method='min').value_counts()
```

```text
86.0     40
245.0    36
183.0    34
368.0    31
153.0    30
339.0    29
281.0    29
310.0    29
1.0      29
217.0    28
58.0     28
30.0     28
126.0    27
Name: model_year, dtype: int64
```

```text
df['model_year'].rank(pct=True).head()
# pct는 percentage를 의미
```

```text
0    0.037688
1    0.037688
2    0.037688
3    0.037688
4    0.037688
Name: model_year, dtype: float64
```

```text
df['model_year'].rank(method='first').head()
# 먼저 등장하는 순위
```

```text
0    1.0
1    2.0
2    3.0
3    4.0
4    5.0
Name: model_year, dtype: float64
```

```text
df.rank?
```

```text
df2
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 5 | NaN | 11 |
| 3 | 6 | NaN | 12 |

```text
df2.cumsum()
# 누적 값 구하기
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 9 | NaN | 21 |
| 3 | 15 | NaN | 33 |

```text
df2['b'] = [9.0, 13.0, 11.5]
df2.cummax()
# df[b,3]의 값이 13보다 작은 11.5이므로 그대로 유지되는 모습
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 5 | 13.0 | 11 |
| 3 | 6 | 13.0 | 12 |

```text
df2.cummin()
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 4 | 9.0 | 10 |
| 3 | 4 | 9.0 | 10 |

```text
df2.cumprod()
# 누적곱
```

|  | a | b | c |
| :--- | :--- | :--- | :--- |
| 1 | 4 | 9.0 | 10 |
| 2 | 20 | 117.0 | 110 |
| 3 | 120 | 1345.5 | 1320 |

## 마크다운 수식입력

 이번주차 강의가 수식을 입력할 일이 많아 아래 사이트를 소개한다. \(물론 나에게\)

  
[https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

## \[프로그래머스 AI 스쿨 1기\] 5주차 DAY 1

### Machine Learning 기초 - 소개 

####  머신러닝이란?

* 기계학습. 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘의 연구.
* 학습데이터 : 입력벡터들과 목표값들
* 머신러닝 알고리즘의 결과는 목표값을 예측하는 함수 
* 숫자 인식에서 입력벡터는 손글씨 이미지, 목표값은 0부터 9까지 숫자중 예측값. 정확히는 10개의 클래스 중 한 클래스\(이 때 클래스 넘버는 1부터 시작함\)



####  핵심개념s

*  학습단계: 함수 y\(x\)를 학습데이터에 기반해 결정하는 단계 
*  시험셋 : 모델을 평가하기 위해 사용하는 새로운 데이터 
*  일반화 : 모델에서 학습에 사용된 데이터가 아닌 이전에 접하지 못한 로운 데이터에 대해 올바른 예측을 수행하는 역량 
*  지도학습 : 분류와 회귀 
*  비지도학습 : 군집 



####  다항식 곡선 근사 

![](../../.gitbook/assets/image%20%2897%29.png)

* Polynomial Curve Fitting
*  회귀 문제에 해당한다.
*  점들을 지나는\(지나지 못하더라도 최대한 가깝게\) 함수 구하기 
* 학습데이터 : 입력벡터와 목표값 
* 목표 : 새로운 입력벡터가 주어질 때 목표값을 예측하는 것 
* 확률이론 : 예측값의 불확실성을 정량화시켜 표현할 수 있는 수학적 프레임워크 제공 
* 결정이론 : 확률적 표현을 바탕으로 최적의 예측을 수행할 수 있는 방법론 제공 

![](../../.gitbook/assets/image%20%2896%29.png)

####  

#### 과소적합과 과대적합

 실제 학습 데이터에 크기에 비해 너무 고차원 함수 또는 너무 저차원 함수를 사용하면 실제 성능에서 에러가 많이 발생한다.

 $$ E_{RMS} = \sqrt {2E(w^*)/N} $$ : Root Mean Square

  또한, 고차원 함수더라도 많은 양에 데이터가 존재한다면 과대적합이 발생할 가능성이 적어진다. 그리고 실제로 머신러닝의 데이터 수는 굉장히 많다는 점.



####  규제화\(Regularization\)

 파라미터값이 너무 커지지 않도록 하는 방법.

![](../../.gitbook/assets/image%20%2893%29.png)

  이 때 너무 심하게 하면 과대/과소 적합이 예기치 않게 발생할 수 있음. 



### Machine Learning 기초 - 확률이론1

####  확률 변수 

 확률 변수 X는 표본의 집합 S의 원소 e를 실수값 X\(e\) = x에 대응시키는 함수이다.

*  대문자 X, Y, ... : 확률 변수 
*  소문자 x, y, ... : 확률 변수가 가질 수 있는 값 
*  확률 P는 집합 S의 부분집합을 실수값에 대응시키는 함
* ex\) S = {HH, HT, TH, TT}; throwing coin
* X\(HH\) = 2, X\(HT\) = 1, X\(TH\) = 1, X\(TT\) = 0; head of coin appear
* P\[X = 1\] = P\[{HT, TH}\] = $$\frac {2} {4} = \frac {1} {2}$$



####  연속 확률 변수\(Continuous Random Variables\)

 누적분포함수 F\(x\) = P\[X $$ \in$$\(-$$ \infty$$, x\)\] 일 때,  F\(x\)를 가진 확률 변수 X에 대해서 다음을 만족하는 함수 f\(x\)가 존재한다면 X를 연속 확률 변수라고 부르고 f\(x\)를 X의 확률 밀도 함수\(probability density function\)라고 부른다.

*  확률 변수를 명확히 하기 위해 F\(x\), f\(x\)로 쓰기로 하며 밀도 함수의 경우에는 p\(x\)를 사용하기도 한다.

####   확률 변수의 성질 

*  덧셈 법칙 
*  곱셈 법칙 

![](../../.gitbook/assets/image%20%2894%29.png)

*  베이즈 확률 \(posterior 사후확률, likelihood 가능성, prior 사전확률, marginal normalization 경계확률\)

![](../../.gitbook/assets/image%20%2898%29.png)

####  확률변수의 함수 

 확률변수 X의 함수 Y = f\(X\)도 확률변수이다. 예를 들어 확률 변수 X가 주\(week\)의 수로 표현되었다고 하면 일\(day\)의 수로 표현된 새로운 확률변수를 정의할 수 있다. 

* Y = 7X
* P\[14 &lt;= Y &lt;= 21\] = P\[2 &lt;= X &lt;= 3\]
* $$ p_y(y) = p_x(x)|\frac {dx} {dy} | $$

 k차원의 확률변수 벡터 x = \(x1, ... , xk\)가 주어질 때, k개의 x에 관한 함수들은 새로운 확률변수벡터 y = \(y1, ... yk\)를 정의한다. 간략하게 y = \(x\)로 나타낼 수 있다. 만약 y = g\(x\)가 일대일 변환인 경우\(x = w\(y\)로 유일한 해를 가질 때\), y의 결합확률밀도함수는 다음과 같다.

* $$ p_y (y_1, ... , y_k) = p_x(x_1, ... , x_k)|J|$$
* where J = $$  \begin{matrix} \frac {dx_1} {dy_1} & \frac {dx_1} {dy_2} & ... & \frac {dx_1} {dy_k}  \\ \frac {dx_2} {dy_1} & ... & ... & ... \\ ... \\ \frac {dx_k} {dy_1} & ... & ... & \frac {dx_k} {dy_k} \end{matrix}$$

####  예제

 $$ p_{x1,x2} (x1, x2) = e^{-(x_1+x_2)}, x1 > 0, x2 > 0$$일 때, $$ $$$$ y_1 = x_1, y_2 = x_1 + x_2 $$에 의해서 정의되는 y의 pdf는?

 $$ f{y_1, y_2}(y_1, y_2) =  f{x_1, x_2}(x_1, x_2)|J| =  f{x_1, x_2}(y_1, y_2 - y_1) = e^{-\{-y1 + y2 -y1\}} = e^{y2}$$

 $$ 0 < y_1 < y_2 < \infty$$

 $$ f_{y_1}(y_1) = \int^\infty _{y_1} e^{-y_2} = -e^{-y_2} |^\infty _{y_1} = e^{-y_1}$$

 Inverse CDF Technique를 사용하면 반경이 r인 원 안에 랜덤하게 점을 찍을 수 있다.



####   기댓값

 확률분포 p\(x\)하에서 함수 f\(x\)의 평균값



####  분산과 공분산

 f\(x\)의 분산 : f\(x\)의 값들이 기댓값으로부터 흩어져 있는 정



#### 확률을 해석하는 두 가지 다른관점 : 빈도주의 대 베이지안

빈도주의 : 반복가능한 사건들의 빈도수에 기반

베이지안 : 불확실성을 정량적으로 표현

북극 얼음이 이번 세기말까지 녹아 없어질 확률은 반복가능하지 않은 사건임

빈도주의는 추정사를 사용해서 확률을 구하며 구해진 파라미터의 불확실성은 부트스트랩 방법을 이용해 구한다.

베이지안은 사전확률을 모델에 포함시킬 수 있는 장점이 있다.



#### 정규분포

단일변수 x를 위한 가우시안 분포













