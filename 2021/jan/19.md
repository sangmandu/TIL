---
description: TIL
---

# 19 Tue

## \[AI 스쿨 1기\] 7주차 DAY 2

### Deep Learning: 신경망의 기초 - 기계학습과 수학 I

#### 기계학습에서 수학의 역할

* 수학은 목적함수를 정의하고 목적함수의 최저점을 찾아주는 최적화 이론 제공
* 최적화 이론에 학습률, 멈춤조건과 같은 제어를 추가하여 알고리즘 구축
* 사람은 알고리즘을 설계하고 데이터를 수집

#### 기본 선형대수

* 데이터를 벡터, 행렬, 텐서 등으로 표현하고 이를 공간적으로 이해해야 하기 때문에 선형대수를 공부해야함.

#### 벡터

* 샘플을 특징 벡터로 표현
  * EX\) 꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비
  * 4개의 특징이 각각 5.1, 3.5, 1.4, 0.2 인 샘플
* 데이터 집합의 여러 특징 벡터를 첨자로 구분
  * 데이터의 색인을 위첨자로 또는 아래첨자로 표현할 수 있다
  * 사람마다 표현 방식이 다르다

#### 행렬

* 여러 개의 벡터를 담음
* 요소 $$ x_{ij} $$: i번째 행, j번째 열
* 훈련집합을 담은 행렬을 설계행렬이라 부름
* 기본적으로 행렬은 대문자, 벡터는 소문자를 씀

#### 전치행렬

* A의 전치행렬은 $$A^T$$
* 행과 열의 순서가 바뀐 행렬
* 주어진 데이터를 표현하기 쉽게 하기 위해서 사용
* $$ (AB)^T = B^TA^T $$

#### 행렬을 이용하면 방정식을 간결하게 표현 가능

![](../../.gitbook/assets/image%20%28157%29.png)

#### 특수 행렬

![](../../.gitbook/assets/image%20%28163%29.png)

#### 행렬 연산

* 곱셈
  * $$C = AB$$
  * 교환 법칙은 성립하지 않음
  * 분배법칙과 결합법칙 성립
* 내적
  * $$C = A \cdot B = A^TB $$
  * 두 벡터\(또는 행렬\)간의 유사성을 측정하는 연산

#### 텐서

* 3차원 이상의 구조를 가진 숫자 배열
  * 0차 : 스칼라
  * 1차 : 벡터
  * 2차 : 행렬

#### 유사도와 거리

![](../../.gitbook/assets/image%20%28161%29.png)

* 벡터를 기하학적으로 해석
* 각도를 기준으로 벡터의 유사도를 판단
* 코사인 유사도

![](../../.gitbook/assets/image%20%28158%29.png)

#### 벡터와 행렬의 거리를 놈으로 측정

* 벡터의 p차 놈$$ ||x||_p = ( \sum _{i=1,d} |x_i|^p)^{1 \over p} $$
* 최대 놈 : $$ ||x||_\infty = max(|x_1|, |x_2|, \cdots ,|x_d|) $$

![](../../.gitbook/assets/image%20%28162%29.png)

* 프로베니우스 놈
  * 행렬의 크기를 측정

![](../../.gitbook/assets/image%20%28160%29.png)

#### 놈의 정의가 애매하다면? =&gt;[ \(링크\)](https://bskyvision.com/825)

* 놈에 대해 확실한 이해가 되었다



#### 퍼셉트론

* 1958년 고안된 분류기 모델
* 퍼셉트론의 동작
  * 내적을 통해 유사성을 판단
  * 

![](../../.gitbook/assets/image%20%28159%29.png)

* 활성 함수로는 계단함수 사용

#### 퍼셉트론의 물리적 의미

* w에 수직이고 원점으로부터 $$ {T \over ||w||_2} $$ 만큼 떨어져 있음
* 2차원 공간은 두 개의 부분공간을 나누는 결정직선이 존재한다
* 3차원은 결정평면, 4차원 이상은 결정 초평면

여러 퍼셉트론 출력 표현 \(멀티 퍼셉트론\)

* 입력벡터가 d개면 퍼셉트론 하나마다 d개의 가중치와 입력이 존재한다.
* 가중치 벡터를 각 부류의 기준 벡터로 간주하면, c개 부류의 유사도 계산하는 것과 비슷하다.

### 

### 

### Deep Learning: 신경망의 기초 - 기계학습과 수학 II



### Deep Learning: 신경망의 기초 - 기계학습과 수학 III

 

### Deep Learning: 신경망의 기초 - 실습 I



## \[Statistics 110\] 9강- 기댓값, 지시확률변수와 선형성 \(Expectation, Indicator Random Variables, Linearity\)



## \[Statistics 110\] 10강- 기댓값 \(Expectation Continued\)



