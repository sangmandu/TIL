---
description: TIL
---

# 19 Tue

## \[AI 스쿨 1기\] 7주차 DAY 2

### Deep Learning: 신경망의 기초 - 기계학습과 수학 I

#### 기계학습에서 수학의 역할

* 수학은 목적함수를 정의하고 목적함수의 최저점을 찾아주는 최적화 이론 제공
* 최적화 이론에 학습률, 멈춤조건과 같은 제어를 추가하여 알고리즘 구축
* 사람은 알고리즘을 설계하고 데이터를 수집

#### 기본 선형대수

* 데이터를 벡터, 행렬, 텐서 등으로 표현하고 이를 공간적으로 이해해야 하기 때문에 선형대수를 공부해야함.

#### 벡터

* 샘플을 특징 벡터로 표현
  * EX\) 꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비
  * 4개의 특징이 각각 5.1, 3.5, 1.4, 0.2 인 샘플
* 데이터 집합의 여러 특징 벡터를 첨자로 구분
  * 데이터의 색인을 위첨자로 또는 아래첨자로 표현할 수 있다
  * 사람마다 표현 방식이 다르다

#### 행렬

* 여러 개의 벡터를 담음
* 요소 $$ x_{ij} $$: i번째 행, j번째 열
* 훈련집합을 담은 행렬을 설계행렬이라 부름
* 기본적으로 행렬은 대문자, 벡터는 소문자를 씀

#### 전치행렬

* A의 전치행렬은 $$A^T$$
* 행과 열의 순서가 바뀐 행렬
* 주어진 데이터를 표현하기 쉽게 하기 위해서 사용
* $$ (AB)^T = B^TA^T $$

#### 행렬을 이용하면 방정식을 간결하게 표현 가능

![](../../.gitbook/assets/image%20%28157%29.png)

#### 특수 행렬

![](../../.gitbook/assets/image%20%28169%29.png)

#### 행렬 연산

* 곱셈
  * $$C = AB$$
  * 교환 법칙은 성립하지 않음
  * 분배법칙과 결합법칙 성립
* 내적
  * $$C = A \cdot B = A^TB $$
  * 두 벡터\(또는 행렬\)간의 유사성을 측정하는 연산

#### 텐서

* 3차원 이상의 구조를 가진 숫자 배열
  * 0차 : 스칼라
  * 1차 : 벡터
  * 2차 : 행렬

#### 유사도와 거리

![](../../.gitbook/assets/image%20%28167%29.png)

* 벡터를 기하학적으로 해석
* 각도를 기준으로 벡터의 유사도를 판단
* 코사인 유사도

![](../../.gitbook/assets/image%20%28160%29.png)

#### 벡터와 행렬의 거리를 놈으로 측정

* 벡터의 p차 놈$$ ||x||_p = ( \sum _{i=1,d} |x_i|^p)^{1 \over p} $$
* 최대 놈 : $$ ||x||_\infty = max(|x_1|, |x_2|, \cdots ,|x_d|) $$

![](../../.gitbook/assets/image%20%28168%29.png)

* 프로베니우스 놈
  * 행렬의 크기를 측정

![](../../.gitbook/assets/image%20%28165%29.png)

#### 놈의 정의가 애매하다면? =&gt;[ \(링크\)](https://bskyvision.com/825)

* 놈에 대해 확실한 이해가 되었다



#### 퍼셉트론

* 1958년 고안된 분류기 모델
* 퍼셉트론의 동작
  * 내적을 통해 유사성을 판단
  * 

![](../../.gitbook/assets/image%20%28162%29.png)

* 활성 함수로는 계단함수 사용

#### 퍼셉트론의 물리적 의미

* w에 수직이고 원점으로부터 $$ {T \over ||w||_2} $$ 만큼 떨어져 있음
* 2차원 공간은 두 개의 부분공간을 나누는 결정직선이 존재한다
* 3차원은 결정평면, 4차원 이상은 결정 초평면

#### 여러 퍼셉트론 출력 표현 \(멀티 퍼셉트론\)

* 입력벡터가 d개면 퍼셉트론 하나마다 d개의 가중치와 입력이 존재한다.
* 가중치 벡터를 각 부류의 기준 벡터로 간주하면, c개 부류의 유사도 계산하는 것과 비슷하다.

#### 학습의 정의

* 추론
  * O = f\(Wx\)
  * W와 x를 알고 O를 추론
* 훈련
  * O = f\(Wx\)
  * O와 x를 알고 W를 추론

#### 선형결합과 벡터공간

* 벡터
  * 공간사으이 한 점으로 화살표 끝이 벡터의 좌표에 해당
* 선형결합이 만드는 벡터공간
  * $$ c = \alpha _1a +\alpha _2b $$
  * 선형결합으로 만들어진 공간을 벡터공간이라고 부름

![](../../.gitbook/assets/image%20%28163%29.png)

#### 선형방정식의 해

* 불능 : 해 없음
* 부정 : 다수의 해 존재
* 유일해 존재 : 역행렬을 이용하여 해를 구함

#### 행렬식

![](../../.gitbook/assets/image%20%28166%29.png)

* 역행렬의 존재 유무
  * det = 0 : 역행렬 없음
  * det != 0 : 역행렬 존재
* 기하학적 의미
  * det = 0 : 하나의 차원을 따라서 축소되어 부피를 잃게됨
  * det = 1 : 부피 유지, 방향 보존
  * det = -1 : 부피 유지, 방향 보존 안됨
  * det = 5 : 부피 5배 확장, 방향 보존

#### 정부호 행렬

* 양의 정부호 행렬 : 0이 아닌 모든 벡터 x에 대해 $$ x^TAx > 0 $$
* 고유값은 모두 양수이다
* 역행렬도 정부호 행렬
* 양의 정부호, 음의 정부호로 나뉘며 0을 포함하냐에 따라 준정부호라고 한다

#### 분해

* 3717 = 3 \* 3 \* 7 \* 59
* 행렬도 분해하면 유용하다

#### 고윳값과 고유벡터

* $$ Av = \lambda v $$
* A라는 행렬에 의해서 아래와 같이 바뀔 때 자세히 보면 고유벡터의 크기가 바뀌었을 뿐이지 방향이 바뀌지 않는다. 따라서 위와 같은 식으로 표현할 수 있다는 뜻

![](../../.gitbook/assets/image%20%28161%29.png)

#### 고유 분해

* $$ A = Q A'Q^{-1}$$
* Q는 A의 고유 벡터를 열에 배치한 행렬이고 A' 은 고윳값을 대각선에 배치한 대각행렬이다.
* 고유 분해는 고유값과 해당 고유 벡터가 존재하는 정사각행렬에만 적용 가능하다
* 기계학습에서는 정사각행렬이 아닌 경우의 분해도 필요하므로 한계점이 있다

![](../../.gitbook/assets/image%20%28164%29.png)

#### 특잇값 분해

* 정사각행렬이 아닌 행렬을 분해할 때 사용

![](../../.gitbook/assets/image%20%28158%29.png)

#### 특잇값 분해의 기하학적 해석

![](../../.gitbook/assets/image%20%28159%29.png)



### Deep Learning: 신경망의 기초 - 기계학습과 수학 II

확률과 통계

* 기계 학습은 불확실성을 다루는 확률과 통계를 이용해 데이터를 처리해야한다.
* 함수가 아닌 분배를 알고리즘으로 사용하기 때문

확률 분포

* 확률 질량 함수 : 이산 확률 변수
  * ex\) 윷놀이에서 나올 수 있는 도개걸윷모 확률
* 확률 밀도 함수 : 연속 확률 변수

확률 벡터

* 확률변수를 요소로 가짐
* Iris에서 x는 4차원 확률 벡터이다.
  * x = $$ (x_1, x_2, x_3, x_4)^T$$

베이즈 정리

![](../../.gitbook/assets/image%20%28170%29.png)

베이즈 정리의 해석

* 사후 확률 = 우도 확률 \* 사전 확률
* 기계학습에서는 사후 확률에 관심이 높다.

기계학습에 적용

* 아이리스 데이터 분류 문제
  * 특징 벡터 x를 통해 분류한다.
  * 꽃이 versicolor일 때 이 꽃의 특징은 x 이다 =&gt; 알기 쉬움
  * 이 꽃의 특징이 x일때 이 꽃은 versicolor 이다 =&gt; 알기 어려움
  * 이것을 베이즈 정리를 이용하여 구한다

최대 우도

* Maximum Likelihood
* 어떤 확률변수의 관찰된 값들을 토대로 그 확률변수의 매개변수를 구하는 방법
* 이 때 우도값이 최대가 되는 매개변수를 아는것이 목표
  * 따라서, 최대우도값을 알고싶은 것이 아니다
  * 최대우도값은 확률의 곱으로 도출되는데 이를 간단하게 하기 위해 로그를 대입
  * 로그를 대입해도 최대값의 위치는 변하지 않으므로 이 때의 매개변수를 도출

데이터의 요약 정보

* 평균 =&gt; 평균 벡터
* 분산 =&gt; 공분산 행렬

가우시안 분포

![](../../.gitbook/assets/image%20%28171%29.png)

다차원 가우시안 분포

![](../../.gitbook/assets/image%20%28174%29.png)

베르누이 분포

![](../../.gitbook/assets/image%20%28175%29.png)

이항 분포

* 베르누이 시행을 n번 한 것

![](../../.gitbook/assets/image%20%28172%29.png)

로지스틱 시그모이드

* 일반적으로 베르누이 분포의 매개변수를 조정하여 얻어진다
* 0부터 1의 값으로 변환해주는 비선형 함수

소프트플러스 함수

* 0부터의 값으로 변환해주는 비선형 함수

그 외의 분포

* 지수 분포
* 라플라스 분포
* 디랙 분포
* 혼합 분포

![](../../.gitbook/assets/image%20%28173%29.png)

변수 변환

* 기존 확률변수를 새로운 확률 변수로 바꾸는 것 

![](../../.gitbook/assets/image%20%28176%29.png)



### Deep Learning: 신경망의 기초 - 기계학습과 수학 III

 정보이론

* 사건이 지닌 정보를 정량화 할 수 있나?
  * "아침에 해가 뜬다" 와 "오늘 아침에 일식이 있었다" 라는 두 사건 중 후자의 경우가 드물게 발생하므로 정보가 더 많을 가능성이 있다. 전자는 흔한 일로 이루어지기 때문
* 기본원리 : 확률이 작을수록 많은 정보를 가지고 있다
  * 잘 일어나지 않은 사건의 정보량이 많다

자기 정보

* 사건\(메시지\) $$ e_i$$의 정보량
  * 단위
    * 로그의 밑이 2인 경우 : 비트
    * 로그의 밑이 자연상수인 경우 : 나츠
  * 주로 비트를 사용한다
* $$ h(e_i) = -log_2P(e_i)~or~ h(e_i) = -log_eP(e_i)$$
  * 동전에서 앞면이 나오는 사건의 정보량은 $$ -log_2(1/2) = 1$$이다.
  * 주사위에서 1이 나오는 사건의 정보량은 $$ -log2(1/6) \approx 2.58 $$이다.
  * 주사위의 사건이 동전의 사건보다 높은 정보량을 가짐

엔트로피

* 확률변수 x의 불확실성을 나타내는 엔트로피
* 모든 사건 정보량의 기대값으로 표현

![](../../.gitbook/assets/image%20%28177%29.png)



### Deep Learning: 신경망의 기초 - 실습 I



## 



