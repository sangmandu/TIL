---
description: TIL
---

# 19 Tue

## \[AI 스쿨 1기\] 7주차 DAY 2

### Deep Learning: 신경망의 기초 - 기계학습과 수학 I

#### 기계학습에서 수학의 역할

* 수학은 목적함수를 정의하고 목적함수의 최저점을 찾아주는 최적화 이론 제공
* 최적화 이론에 학습률, 멈춤조건과 같은 제어를 추가하여 알고리즘 구축
* 사람은 알고리즘을 설계하고 데이터를 수집

#### 기본 선형대수

* 데이터를 벡터, 행렬, 텐서 등으로 표현하고 이를 공간적으로 이해해야 하기 때문에 선형대수를 공부해야함.

#### 벡터

* 샘플을 특징 벡터로 표현
  * EX\) 꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비
  * 4개의 특징이 각각 5.1, 3.5, 1.4, 0.2 인 샘플
* 데이터 집합의 여러 특징 벡터를 첨자로 구분
  * 데이터의 색인을 위첨자로 또는 아래첨자로 표현할 수 있다
  * 사람마다 표현 방식이 다르다

#### 행렬

* 여러 개의 벡터를 담음
* 요소 $$ x_{ij} $$: i번째 행, j번째 열
* 훈련집합을 담은 행렬을 설계행렬이라 부름
* 기본적으로 행렬은 대문자, 벡터는 소문자를 씀

#### 전치행렬

* A의 전치행렬은 $$A^T$$
* 행과 열의 순서가 바뀐 행렬
* 주어진 데이터를 표현하기 쉽게 하기 위해서 사용
* $$ (AB)^T = B^TA^T $$

#### 행렬을 이용하면 방정식을 간결하게 표현 가능

![](../../.gitbook/assets/image%20%28157%29.png)

#### 특수 행렬

![](../../.gitbook/assets/image%20%28169%29.png)

#### 행렬 연산

* 곱셈
  * $$C = AB$$
  * 교환 법칙은 성립하지 않음
  * 분배법칙과 결합법칙 성립
* 내적
  * $$C = A \cdot B = A^TB $$
  * 두 벡터\(또는 행렬\)간의 유사성을 측정하는 연산

#### 텐서

* 3차원 이상의 구조를 가진 숫자 배열
  * 0차 : 스칼라
  * 1차 : 벡터
  * 2차 : 행렬

#### 유사도와 거리

![](../../.gitbook/assets/image%20%28167%29.png)

* 벡터를 기하학적으로 해석
* 각도를 기준으로 벡터의 유사도를 판단
* 코사인 유사도

![](../../.gitbook/assets/image%20%28160%29.png)

#### 벡터와 행렬의 거리를 놈으로 측정

* 벡터의 p차 놈$$ ||x||_p = ( \sum _{i=1,d} |x_i|^p)^{1 \over p} $$
* 최대 놈 : $$ ||x||_\infty = max(|x_1|, |x_2|, \cdots ,|x_d|) $$

![](../../.gitbook/assets/image%20%28168%29.png)

* 프로베니우스 놈
  * 행렬의 크기를 측정

![](../../.gitbook/assets/image%20%28165%29.png)

#### 놈의 정의가 애매하다면? =&gt;[ \(링크\)](https://bskyvision.com/825)

* 놈에 대해 확실한 이해가 되었다



#### 퍼셉트론

* 1958년 고안된 분류기 모델
* 퍼셉트론의 동작
  * 내적을 통해 유사성을 판단
  * 

![](../../.gitbook/assets/image%20%28162%29.png)

* 활성 함수로는 계단함수 사용

#### 퍼셉트론의 물리적 의미

* w에 수직이고 원점으로부터 $$ {T \over ||w||_2} $$ 만큼 떨어져 있음
* 2차원 공간은 두 개의 부분공간을 나누는 결정직선이 존재한다
* 3차원은 결정평면, 4차원 이상은 결정 초평면

#### 여러 퍼셉트론 출력 표현 \(멀티 퍼셉트론\)

* 입력벡터가 d개면 퍼셉트론 하나마다 d개의 가중치와 입력이 존재한다.
* 가중치 벡터를 각 부류의 기준 벡터로 간주하면, c개 부류의 유사도 계산하는 것과 비슷하다.

#### 학습의 정의

* 추론
  * O = f\(Wx\)
  * W와 x를 알고 O를 추론
* 훈련
  * O = f\(Wx\)
  * O와 x를 알고 W를 추론

#### 선형결합과 벡터공간

* 벡터
  * 공간사으이 한 점으로 화살표 끝이 벡터의 좌표에 해당
* 선형결합이 만드는 벡터공간
  * $$ c = \alpha _1a +\alpha _2b $$
  * 선형결합으로 만들어진 공간을 벡터공간이라고 부름

![](../../.gitbook/assets/image%20%28163%29.png)

#### 선형방정식의 해

* 불능 : 해 없음
* 부정 : 다수의 해 존재
* 유일해 존재 : 역행렬을 이용하여 해를 구함

#### 행렬식

![](../../.gitbook/assets/image%20%28166%29.png)

* 역행렬의 존재 유무
  * det = 0 : 역행렬 없음
  * det != 0 : 역행렬 존재
* 기하학적 의미
  * det = 0 : 하나의 차원을 따라서 축소되어 부피를 잃게됨
  * det = 1 : 부피 유지, 방향 보존
  * det = -1 : 부피 유지, 방향 보존 안됨
  * det = 5 : 부피 5배 확장, 방향 보존

#### 정부호 행렬

* 양의 정부호 행렬 : 0이 아닌 모든 벡터 x에 대해 $$ x^TAx > 0 $$
* 고유값은 모두 양수이다
* 역행렬도 정부호 행렬
* 양의 정부호, 음의 정부호로 나뉘며 0을 포함하냐에 따라 준정부호라고 한다

#### 분해

* 3717 = 3 \* 3 \* 7 \* 59
* 행렬도 분해하면 유용하다

#### 고윳값과 고유벡터

* $$ Av = \lambda v $$
* A라는 행렬에 의해서 아래와 같이 바뀔 때 자세히 보면 고유벡터의 크기가 바뀌었을 뿐이지 방향이 바뀌지 않는다. 따라서 위와 같은 식으로 표현할 수 있다는 뜻

![](../../.gitbook/assets/image%20%28161%29.png)

#### 고유 분해

* $$ A = Q A'Q^{-1}$$
* Q는 A의 고유 벡터를 열에 배치한 행렬이고 A' 은 고윳값을 대각선에 배치한 대각행렬이다.
* 고유 분해는 고유값과 해당 고유 벡터가 존재하는 정사각행렬에만 적용 가능하다
* 기계학습에서는 정사각행렬이 아닌 경우의 분해도 필요하므로 한계점이 있다

![](../../.gitbook/assets/image%20%28164%29.png)

#### 특잇값 분해

* 정사각행렬이 아닌 행렬을 분해할 때 사용

![](../../.gitbook/assets/image%20%28158%29.png)

#### 특잇값 분해의 기하학적 해석

![](../../.gitbook/assets/image%20%28159%29.png)



### Deep Learning: 신경망의 기초 - 기계학습과 수학 II

확률과 통계

* 기계 학습은 불확실성을 다루는 확률과 통계를 이용해 데이터를 처리해야한다.
* 함수가 아닌 분배를 알고리즘으로 사용하기 때문

확률 분포

* 확률 질량 함수 : 이산 확률 변수
  * ex\) 윷놀이에서 나올 수 있는 도개걸윷모 확률
* 확률 밀도 함수 : 연속 확률 변수

확률 벡터

* 확률변수를 요소로 가짐
* Iris에서 x는 4차원 확률 벡터이다.
  * x = $$ (x_1, x_2, x_3, x_4)^T$$

베이즈 정리

![](../../.gitbook/assets/image%20%28170%29.png)

베이즈 정리의 해석

* 사후 확률 = 우도 확률 \* 사전 확률
* 기계학습에서는 사후 확률에 관심이 높다.

기계학습에 적용

* 아이리스 데이터 분류 문제
  * 특징 벡터 x를 통해 분류한다.
  * 꽃이 versicolor일 때 이 꽃의 특징은 x 이다 =&gt; 알기 쉬움
  * 이 꽃의 특징이 x일때 이 꽃은 versicolor 이다 =&gt; 알기 어려움
  * 이것을 베이즈 정리를 이용하여 구한다

최대 우도

* Maximum Likelihood
* 어떤 확률변수의 관찰된 값들을 토대로 그 확률변수의 매개변수를 구하는 방법
* 이 때 우도값이 최대가 되는 매개변수를 아는것이 목표
  * 따라서, 최대우도값을 알고싶은 것이 아니다
  * 최대우도값은 확률의 곱으로 도출되는데 이를 간단하게 하기 위해 로그를 대입
  * 로그를 대입해도 최대값의 위치는 변하지 않으므로 이 때의 매개변수를 도출

데이터의 요약 정보

* 평균 =&gt; 평균 벡터
* 분산 =&gt; 공분산 행렬

가우시안 분포

![](../../.gitbook/assets/image%20%28171%29.png)

다차원 가우시안 분포

![](../../.gitbook/assets/image%20%28174%29.png)

베르누이 분포

![](../../.gitbook/assets/image%20%28175%29.png)

이항 분포

* 베르누이 시행을 n번 한 것

![](../../.gitbook/assets/image%20%28172%29.png)

로지스틱 시그모이드

* 일반적으로 베르누이 분포의 매개변수를 조정하여 얻어진다
* 0부터 1의 값으로 변환해주는 비선형 함수

소프트플러스 함수

* 0부터의 값으로 변환해주는 비선형 함수

그 외의 분포

* 지수 분포
* 라플라스 분포
* 디랙 분포
* 혼합 분포

![](../../.gitbook/assets/image%20%28173%29.png)

변수 변환

* 기존 확률변수를 새로운 확률 변수로 바꾸는 것 

![](../../.gitbook/assets/image%20%28176%29.png)



### Deep Learning: 신경망의 기초 - 기계학습과 수학 III

####  정보이론

* 사건이 지닌 정보를 정량화 할 수 있나?
  * "아침에 해가 뜬다" 와 "오늘 아침에 일식이 있었다" 라는 두 사건 중 후자의 경우가 드물게 발생하므로 정보가 더 많을 가능성이 있다. 전자는 흔한 일로 이루어지기 때문
* 기본원리 : 확률이 작을수록 많은 정보를 가지고 있다
  * 잘 일어나지 않은 사건의 정보량이 많다



자기 정보

* 사건\(메시지\) $$ e_i$$의 정보량
  * 단위
    * 로그의 밑이 2인 경우 : 비트
    * 로그의 밑이 자연상수인 경우 : 나츠
  * 주로 비트를 사용한다
* $$ h(e_i) = -log_2P(e_i)~or~ h(e_i) = -log_eP(e_i)$$
  * 동전에서 앞면이 나오는 사건의 정보량은 $$ -log_2(1/2) = 1$$이다.
  * 주사위에서 1이 나오는 사건의 정보량은 $$ -log2(1/6) \approx 2.58 $$이다.
  * 주사위의 사건이 동전의 사건보다 높은 정보량을 가짐



엔트로피

* 확률변수 x의 불확실성을 나타내는 엔트로피
* 모든 사건 정보량의 기대값으로 표현

![](../../.gitbook/assets/image%20%28182%29.png)

* 두 확률간의 유사도를 측정할 때 사용한다
* 불확실성이 가장 클 때\(즉 이 말은 확률이 공정하다는 뜻\) 엔트로피가 가장 크다
* 엔트로피 예
  * 윷을 나타내는 윷놀이의 확률변수는 2.0306비트
  * 6을 나타내는 주사위의 확률변수는 2.585
  * 주사위가 윷보다 엔트로피가 높은 이유?
    * 주사위는 모든 사건이 동일한 확률
    * 윷보다 예측하기 어려움
    * 주사위가 윷보다 더 무질서하고 불확실성이 큼
    * 엔트로피가 높음



교차 엔트로피

* 엔트로피 : 확률을 정량화 한 것
* 교차 엔트로피 : 두 개의 확률의 유사도 =&gt; 동일한 정보를 얼마나 공유하는 지

![](../../.gitbook/assets/image%20%28185%29.png)

* 원래 엔트로피 식에서 바깥 P\(x\)가 바뀐 것.
* 심층학습이 손실함수로 많이 사용된다.
* 이 식을 전개하면,

![](../../.gitbook/assets/image%20%28184%29.png)

* Q : 예측값, P : 데이터 분포
* P값은 고정이고 Q값을 변화시키는 작업
* 교차 엔트로피를 손실함수로 사용하는 경우는 P값은 변화하지 않으므로 KL 발산을 최소하 하는 것이 목표



KL 다이버전스

![](../../.gitbook/assets/image%20%28186%29.png)

* P와 Q는 둘 다 데이터 분포이며 서로의 분포를 조작해서 유사하게 만드는 것이 목표
* KL Area는 두 값의 차이나는 면적을 그리고 있음 =&gt; 이 면적을 최소화 하는 것이 목표
* 두 확률분포 사이의 거리를 계산할 때 주로 사용한다

![](../../.gitbook/assets/image%20%28180%29.png)

* 이는 P의 엔트로피 + P와 Q간의 KL 다이버전스를 더한것이다.
* 가지고 있는 데이터 분포 P\(x\)와 추정한 데이터 분포 Q\(x\)간의 차이를 최소화하는데 교차 엔트로피를 사용한다.



교차 엔트로피 적용 예

![](../../.gitbook/assets/image%20%28183%29.png)

* 모델을 통해 5가지의 입력값을 입력
* 이 때 softmax value로 얻은 확률분포와 실제 라벨\(극단적인 확률분포\)을 비교할 때 사용한다.



최적화

* 순수 수학 최적화
  * 주어지는 목적함수의 최저점을 찾아야 한다
* 기계 학습 최적화
  * 훈련집합이 주어지고 이에 따라 정해지는 목적함수를 최소로 만들어내는 모델의 매개변수를 찾아야 한다
  * 주로 SGD\(확률론적 경사 하강법\)를 사용한다
  * 손실함수는 미분하는 과정이 필요 하다 =&gt; 오류 역전파 알고리즘 사용



매개변수 공간 탐색

* 훈련집합의 크기가 작기 때문에 데이터의 특징 공간을 모두 만족시키는 확률분포를 구할수는 없다.
* 특징 공간에 대해 설정한 가설과 목적함수를 선택하고 이 목적함수가 최저가 되도록 하는 매개변수를 탐색하는 전략을 사용한다



학습 모델의 매개변수 공간

* 특징 공간보다 수만배 많은 차원을 가짐
  * MNIST 인식하는 심층학습 모델은 784차원 특징 공간
  * 매개변수 공간은 수십만~수백만 차원의 매개변수 공간
  * 실제로 매개변수 공간, 손실 함수는 구하기가 어렵다.
  * 이 때의 미분값을 구해서 최적해를 찾으려고 함



최적화 문제 해결

* 낱낱탐색 알고리즘
  * 차원이 조금만 높아져도 적용 불가능
* 무작위 탐색 알고리즘
  * 아무런 전략이 없음
* 초기해 theta를 설정하고 작아지는 방향 d\(theta\)를 구한다.
  * 미분으로 찾아낸다.



미분에 의한 최적화

* 도함수는 함수의 기울기를 의미 =&gt; 함수의 값이 커지고 작아지는 방향을 알 수 있음
* 이계도함수를 사용해 극소, 극대값을 찾을 수도 있다



편미분

* 변수가 복수인 함수의 미분
* 미분 값이 이루는 벡터를 경사도라고 부름 = gradient
* 기계학습에서 매개변수 집합은 복수이므로 편미분을 사용한다



독립변수와 종속변수의 구분

y = wx + b

* x는 독립변수, y는 종속변수
* 기계학습에서 예측 단계를 위한 해석은 무의미함
* 최적화는 예측 단계가 아니라 학습 단계에 필요



야코비언 행렬

* 1차 편도 미분한 행렬

![](../../.gitbook/assets/image%20%28177%29.png)



해세 행렬

* 2차 편도 미분한 행렬

![](../../.gitbook/assets/image%20%28187%29.png)



경사하강법

* 낮은 곳을 찾아가는 원리
* 이동 방향은 기울기이고, 이동 거리는 학습률이다.



집단\(무리\) 경사 하강 알고리즘

* BGD : Batch Gradient Descent
* 샘플의 경사도를 구하고 평균한 후 한꺼번에 갱신
* 훈련집합 전체를 다 봐야 갱신이 일어나므로 학습 과정이 오래 걸리는 단점
* 정확한 방향으로 수렴



확률론적 경사 하강 알고리즘

* SGD : Sthchastic gradient descent
* 한 샘플 혹은 작은 집단의 경사도를 계산한 후 즉시 갱신
* 작은 무리 단위를 한 세대 Epoch라고 부름
* 수렴이 다소 헤맬 수 있음.



추가 경사 하강 알고리즘들

* Momentum
* NAG
* Adagrad
* Adadelta
* Rmsprop



### Deep Learning: 신경망의 기초 - 실습 I

CPU vs GPU

* CPU 
  * 코어수가 적고 각각의 코어가 더 빠르다 연산력이 우월
  * 연속적인 작업에 우월
* GPU
  * 코어수가 많고 각각의 코어가 좀 둔하다
  * 병렬적인 작업에 우월



GPU

* 행렬의 연산에 있어 GPU가 유리
* GigaFLOPs per Dollor
  * GigaFLOPs는 연산력의 단위

![](../../.gitbook/assets/image%20%28178%29.png)

* Deep Learning Explosion : 이 때 딥러닝이 가장 발전했다
* 딥러닝 모델을 사용할 때 CPU 보다 GPU 성능이 64배에서 76배까지 난다. 
  * 공정한 비교는 아닐 수 있지만 그것을 감안해도 엄청난 차이
  * 완전 최적화 되지 않았을 때는 3배 정도 차이가 난다



TPU

* 구글에서 사용하는 용어
* 딥러닝 연산에 효율적으로 사용될 수 있도록 설계되어 있는 아키텍처



Programming GPUs

* CUDA
  * NVIDIA에서만 사용할 수 있음
  * GPU 연산에 사용할 수 있음
* OPENCL
  * 어느 장치에서도 사용할 수 있음
  * NVIDIA에서는 조금 느림
* HIP



CPU / GPU Communication

* 연산은 GPU, 데이터는 DISK
* 둘 사이의 통신이 느리면 병목현상 발생
* 다음과 같은 개선점이 있다
  * RAM을 더 사용
  * HDD대신 SSD
  * CPU 쓰레드를 더 많이 사용해서 미리 데이터 fetch

Deeplearning Framework

* 빠르게 개발할 수 있음
* 기울기를 자동으로 개산
* 모든 연산들이 GPU에서 계산될 수 있다



PyTorch : Fundamental Concepts

* Tensor
  * Numpy와 비슷
  * GPU 에서 연산 가능
* Autograd
  * 직접 기울기를 구하는 것이 아니라 백워드로 구해진다
  * 포워드로 계산할 때 중간 연산을 저장할 필요 없고 수동으로 할 필요도 없다
* Module













## 



