---
description: TIL
---

# 18 Mon

## \[AI 스쿨 1기\] 7주차 DAY 1

### Deep Learning: 신경망의 기초 - 인공지능과 기계학습 소개

#### 인공지능

* 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해능력 등을 컴퓨터 프로그램으로 실현한 기술
* 우리가 집중할 것은 인간의 "학습 능력"

#### 일상 속 인공지능

* 음성인식 - Siri
* 추천 시스템 - eBay, Netflix
* 자율주행 - Waymo
* 실시간 객체 인식 - Face ID
* 로봇 - HUBO
* 번역 - papago



### Deep Learning: 신경망의 기초 - 기계학습 I

#### 사람\(동물\)과 기계의 학습

* 파블로프의 개 : 반복적인 신호를 제공
* 기계도 사람과 동일하지는 않지만 비슷하게 가능

#### 학습

* 경험의 결과로 나타나는, 지속적인 행동의 변화나 그 잠재력의 변화 또는 지식을 습득하는 과정

#### 기계 학습

* 어떤 프로그램이 T라는 작업을 수행한다. 이 프로그램의 성능을 P라는 척도로 평가했을 때 경험  E를 통해 성능이 개선된다면 이 프로그램은 학습을 한다고 말할 수 있다.
* 경험 E를 통해 주어진 작업 T에 대한 성능 P의 향상
* Experience, Task, Performance
* 초창기 지식기반
  * 하나의 개념에 대한 여러 특징들을 하나도 빠짐없이 나열할 수는 없다
  * 매번 새로운 모양과 다양한 특징들이 존재
    * EX\) 개는 털이 있고 혀가 길고 몸집이 가로로 길고 ...
* 데이터 중심 접근방식의 전환

#### 관찰된 데이터를 어떻게 설명할 것인가?

* 데이터의 패턴에 대한 가설
  * X = {1, 2, 3, 4}, Y = {3, 6, 9, 12}
  * suppose Y = 3X
  * Y = WX + b
* 가설의 모델은 1차원, 2차원 일수도 고차원이나 딥러닝 모델일 수도 있다

#### 기계학습의 훈련

* 임의의 매개변수 값에서 시작해서 개선하여 정량적인 최적 성능에 도달
* 주어진 문제에 대해 예측을 가장 정확하게 할 수 있는 최적의 매개변수를 찾는 작업
* 훈련을 마치면 추론을 수행 =&gt; 새로운 특징에 대응되는 목표치를 예측
* 테스트 집합에 대한 높은 성능을 일반화 능력이라고 한다.

#### 기계학습의 필수요소

* 데이터
* 데이터 규칙 존재
* 수학적으로 설명 불가능

#### 사람의 학습과 기계 학습 비교

![](../../.gitbook/assets/image%20%28141%29.png)

#### 차원의 저주

* 데이터의 차원이 높을 수록
  * 더 많은 데이터가 필요하다
  * 데이터간의 규칙을 찾기 힘들다



### Deep Learning: 신경망의 기초 - 기계학습 II

#### 과학 기술의 정립 과정

1. 데이터 수집
2. 모델 정리\(가설\)
3. 예측
4. 반복

#### 기계학습

* 데이터를 수집
* 데이터를 정리
* 모델 생성

#### 데이터의 양과 질

* 다양한 데이터를 충분한 양만큼 수집하면 과업의 성능이 향상된다.

#### 적은 양의 데이터베이스로 높은 성능을 달성하는 방법

* 데이터 희소 특성 가정 - 이상한 데이터는 존재하지 않음
* 매니폴드\(많이 끼다\) 가정 =&gt; 고차원의 데이터는 낮은 차원의 데이터들이 유사성을 지님

#### 데이터 가시화

* 4차원 이상의 초공간은 한번에 가시화 불가능

#### 선형 회귀 문제

* 직선 모델을 사용하므로 두 개의 매개변수 w와 b 필요
* 현실적으로는 선형 모델을 하기 어려움. =&gt; 잡음이 섞이기 때문 =&gt; 비선형 모델 필요
* 제곱 오차 방법으로 손실함수를 통해 개선 가능



### Deep Learning: 신경망의 기초 - 기계학습 III

#### Target distribution \(Target function\)

* 데이터를 구분짓는 구분선
* 실제로 보이지는 않으며, 데이터를 통해서 유추 가능
* input distribution과 target distribution을 통해 training exapmles를 생성
* 목적함수 error measure를 통해 learning algorithm을 수정한다.
* 이 때 가설 h를 learning algorithm을 통하여 최종 가설 g를 도출한다.

#### 과소적합과 과잉적합

![](../../.gitbook/assets/image%20%28144%29.png)

* 과소적합
  * underfitting
  * 모델의 용량\(자유도 - 파라미터 수\)이 작아 오차가 클 수 밖에 없다
  * 고차원의 모델을 사용한다.
* 과잉적합
  * overfitting
  * 훈련집합에 대해서만 완벽하게 근사화하고 새로운 데이터를 예측할 때 큰 문제 발생
  * 모델의 용량이 크기 때문에 학습 과정에서 잡음까지 수용했기 때문
  * 주로 고차원의 모델을 사용하기 때문에 오버피팅 문제가 기계학습에서 많이 발생
  * 이러한 과잉적합을 방지하기 위해 차원을 낮추는 정규화를 사용

#### 훈련집합을 여러번 수집하여 1차~12차에 반복 적용하는 실험

![](../../.gitbook/assets/image%20%28143%29.png)

* 2차
  * 매번 큰 오차 발생 =&gt; 편향이 큼
  * 모델마다 모양이 비슷함 =&gt; 낮은 변동 =&gt; 분산이 작음
* 12차
  * 매번 작은 오차 발생 =&gt; 편향이 작음
  * 모델마다 모양이 상이함 =&gt; 높은 변동 =&gt; 분산이 큼
* 기계학습의 목표
  * 낮은 편향과 낮은 분산을 가진 예측 모델을 가지는 것이 목표
  * 그러나 모델의 편향과 분산은 상충 관계이기 때문에 편향을 최소로 유지하며 분산도 최대로 낮추는 전략이 필요하다

![](../../.gitbook/assets/image%20%28142%29.png)

편향과 분산의 관계

* 용량 증가 =&gt; 편향 감소, 분산 증가 경향
* 일반화 오차 성능\(= 편향 + 분산\)은 U형의 곡선을 가짐

검증집합을 이용한 모델 선택

* 검증집합\(Validation set\)은 데이터의 양이 많을 때 사용한다.
* Original set = Training set + Testing set 의 기본 비율을 다음과 같이 바꾼다.
  * Original set = Training set + Validation set + Testing set
* Testing set을 적용하기 전에 검증집합을 이용해 미리 성능을 측정할 수 있다.

교차점증

* 교차검증\(Cross validation\)은 데이터의 양이 적을 때 사용한다.
* 데이터를 여러 부분으로 나눈 뒤 각 부분을 돌아가면서 검증집합으로 사용

부트스트랩

* 임의의 복원추출 샘플링을 반복한다
* 데이터 분포가 불균형일 때 적용한다
  * class1 : 10000개, class2 : 100개
  * class1에 대해서 치우친 학습을 할 가능성이 크다
  * class1 : 100개, class2 : 50개를 뽑아 학습하거나
  * class1 : 10000개, class2 : 10000\(복사 또는 변형으로 생성\)을 뽑아 학습한다.
  * 데이터의 양이 적으면 좋지 않기 때문에 주로 수를 늘리는 방법으로 적용한다

모델 선택의 한계와 현실적인 해결책

* 데이터의 용량보다 큰 모델을 선택한 뒤, 점점 작게 만든다. =&gt; 여러 규제 기법을 적용

규제

* 데이터 확대
  * 데이터를 더 많이 수집하면 일반화 능력이 향상됨
    * 데이터 수집은 많은 비용이 듦 =&gt; 실측자료를 사람이 일일이 표식해야 하기 때문\(labeling\)
  * 인위적으로 데이터 확대 =&gt; 재활용
    * 훈련집합에 있는 샘플을 변형한다
    * 변형 : 약간의 회전 또는 왜곡\(고유 특성은 변하지 않는 정도로\)
* 가중치 감쇠
  * 가중치를 작게 조절한다
  * 기존 함수는 훈련 집합이 변화하면 분산이 커진다.
  * 가중치를 작게 조절한 개선된 목적함수는 분산이 작다.
  * 원래 가진 모델의 용량이 다 발현되지 못하게 하는 것이 목표





## \[Statistics 110\] 6강- Monty Hall 문제와 심슨의 역설 \(Monty Hall, Simpson's Paradox\)

#### Present Part \[6 / 34\]

