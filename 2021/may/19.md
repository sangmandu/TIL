# 19 Wed

## \[동빈나 논문 분석\] [DETR : End-to-End Object Detection with Transformers](https://youtu.be/hCWUTvVrG7E)

### 연구 배경

기존 방법의 문제점

* 방법들이 너무 복잡하다
* 다양한 라이브러리를 활용해야 한다
* 사전 지식을 요구 한다
  * bounding box의 형태
  * bounding box가 겹칠 때의 처리 방법
* 사전 지식을 통해 겹치지 않는 bounding box를 찾는 방법을 `NMS`라고 하는데 이것을 간단하게 구현하려고 했다
  * 논문에서는 생략한다고 표현했다



### 핵심 아이디어

이분 매칭을 통해 Set prediction problem을 직접적으로 해결한다

* 고양이가 존재하고 개가 존재한다고 하든 그 반대든 상관이 없다
  * 바운딩박스의 순서는 상관이 없다
* prediction과 실제값은 바운딩 박스의 loss함수가 작은 것 끼리 매칭된다
  * 잘못된 예측이 있더라도 no object 로 반환하게 한다
* 출력 개수는 실제 인스턴스의 개수보다 더 크게 설정해야 한다.

Transformer

* Transformer
  * `Attention` 을 통해 전체 이미지의 문맥 정보를 이해
  * 이미지 내 각 인스턴스의 상호작용 파악이 용이하다
  * 큰 bounding box에서의 거리가 먼 픽셀 간의 연관성 파악이 용이하다
* Encoder
  * 이미지의 특징 정보를 포함하고 있는 각 픽셀 위치 데이터를 입력받아 인코딩을 수행한다
  * 인코더는 d \* HW 크기의 연속성을 띠는 feaure map응 입력받는다
    * d는 image feature, HW는 각각의 픽셀 위치 정보
    * 
* Decoder
  * N개의 object query를 초기 입력으로 받으며 인코딩된 정보를 활용
  * 각 object query는 이미지 내 서로 다른 고유한 인스턴스를 구별
  * N개의 object query\(학습된 위치 임베딩\)를 초기 입력으로 이용
  * 인코더가 global attention을 통해 인스턴스를 분리하고 디코더는 각 인스턴스의 클래스와 경계선을 추출한다



### 2장 Related work



2.1 

* 중복을 제거하기 위해 사용했다
* 사람이 preprocessing을 해야하는 과정을 많이 간소화 하려고 했다
* 작은 object에 대해 비교적 적은 성능을 낸다
* 학습이 굉장히 오래 걸린다는 단점이 있다
* 확장성이 크다
* 첫 모델부터 높은 성능을 가지고 있어 발전가능성이 있다
* RNN 대신 Transformer를 사용한다



2.2 Transformers and Parallel Decoding

* 여기에서도 Parallel





