# 9 Sun

## 딥러닝 CNN 완벽 가이드 - Fundamental 편

### Keras를 이용한 Conv2D와 Pooling 적용 실습 01

Conv2D를 쓸때에는 무조건 입력을 3차원으로 해줘야 한다.

* \(H, W, C\)
  * 실습 코드에서는 Fashion MNIST 데이터셋을 쓰고 이 데이터셋이 Gray Scale이기 때문에 Channel = 1 이지만 실제로는 3이 많이 사용된다.
* 2차원으로 입력하면 오류가 난다.

이 때 반환되는 결과는 4차원의 Feature Map이 반환되는데 이를 4차원이라고 하지 않고 배치를 제외하고 3차원의 Feature Map 이라고 한다.

1개의 필터는 3차원이다.

* 커널의 집합을 필터라고 한다.
* 따라서 커널은 2차원

### Keras를 이용한 Conv2D와 Pooling 적용 실습 02

![](../../.gitbook/assets/image%20%28620%29.png)

kernelSize \* kernerSize \* filters \* channel 이 필요한 파라미터 개수가 된다. 

* 여기서 필요한 파라미터 개수는 3 \* 3 \* 32 \* 1 = 288 이다.
* 이 때 bias 32를 더해줌으로써 총 320이 된다

Conv2d\_3 레이어 같은 경우는 다음과 같다.

* 3 \* 3 \* 64 \* 32 = 18,432
  * 이전 레이어에서 전해준 채널 수가 32개이기 때문에
* bias 64를 더해줌으로써 총 18496이 된다.

![](../../.gitbook/assets/image%20%28621%29.png)

이 후 flatten\(1차원으로 변환\)을 거치면 13 \* 13 \* 64 = 10,816 이 된다.

dense는 Fully Connected Network 이므로 Input이 10,816이고 Output이 100이므로 총 1,081,600 + 100 = 1,081,700 의 인자가 필요하다.

### CNN을 이용하여 Fashion MNIST 예측 모델 구현하기

Feature Extract 과정을 거친뒤 flatten을 거쳐 dense layer를 거칠 때 오버피팅이 발생할 확률이 늘어난다. 따라서 이 때 Dropout 기법을 적용한다.

* FC Layer는 너무 촘촘하게 연결되어 있어서, 굉장히 고차원의 방정식이 등장하게 되고 따라서 이 때문에 오버피팅이 생긴다.

보통 Dense 후 한다.

* MaxPooling이나 Flatten 후에도 할 수도 있다. 잘 하지는 않는다.

Conv2D\(\)는 입력으로 배치를 제외한 3차원 입력을 해야한다.

* 하지만 2차원으로 입력해도 3차원으로 변경해준다
  * \(28, 28\) =&gt; \(28, 28, 1\)
* 명확하게는 3차원 입력으로 해주는 것이 좋다. \(비록 Grayscale 이더라도\)

### 다채널 입력 데이터의 Convolution 적용 이해 - 01

다시 강조! Conv 연산은 입력 이미지도 3차원 출력되는 피처맵도 3차원이다.

* 단일 필터도 3차원
  * 여러 개의 커널을 가지는 것을 필터라고 한다.
  * CNN은 여러 개의 3차원 필터를 개별 feature map에 적용하는 방식이다.
* 필터의 채널 수는 항상 입력 텐서의 채널 수와 동일하다.
  * 같지 않으면 연산이 이루어지지 않는다.
  * 예를 들어, 입력 이미지가 3차원 RGB 이미지라면 첫번째 Conv 연산이 될 때의 필터의 채널도 3개.
* 필터 하나 당 피처맵이 하나 생긴다.
  * RGB 이미지라서 채널이 3개니까 피처맵이 3개 생긴다고 생각하면 안된다. 각 필터를 거친 결과물이 3개가 생기는 것은 맞다. 이 때 이 결과물을 모두 element wise 하게 더하는 과정을 거친 결과물이 피처맵이다.
* 결론적으로, 출력 피처맵의 채널 수는 Conv를 적용한 필터의 개수로 결정된다. \(단일 필터의 채널 수가 아니다\)



### 다채널 입력 데이터의 Convolution 적용 이해 - 02

출력 크기 = \(입력 크기 - 커널 사이즈 \) / 스트라이드 + 1

* 여기서 패딩은 없다고 가정
  * 패딩까지 고려한 식은 다음 강의



### 컨볼루션\(Convolution\) 적용 시 출력 피처맵의 크기 계산 공식 이해

출력 크기 = \(입력 크기 - 커널 사이즈 + 2 \* 패딩\) 스트라이드 + 1

* 일반적으로 패딩 = 0, 스트라이드 = 1일 경우 I - F + 1이 O 크기
* 실제로 입출력 크기를 맞춰줄 때는 이를 계산할 필요가 없다.
  * `x = Conv2D(filters=1, kernel_size=3, strides=1, padding='same')(input_tensor)`
  * 다음과 같이 `padding = 'same'` 으로 설정하면 입출력 크기가 동일하다.
* 근데 여기에는 stride가 항상 1이어야 된다는 가정이 있다. 실제로 stride가 2일때에는 다음과 같이 된다.
  * 입력 크기가 6 x 6 인데도 불과하고 출력 크기는 3 x 3 이 된다.

```python
input_tensor = Input(shape=(6, 6, 1))
x = Conv2D(filters=1, kernel_size=3, strides=2, padding='same')(input_tensor)
print('x.shape:', x.shape)
>>> x.shape(None, 3, 3, 1)
```

* 만약 6x6 입력에 Stride 2, Padding 0을 적용시에 2.5가 된다.
  * 이 값은 2로 간주된다.
  * 2가 된다는 뜻은, 0.5 만큼의 정보는 버려졌다는 뜻이며 이는 곧, 컨볼루션이 입력의 양끝쪽은 이루어지지 않았다는 이야기다.

![](../../.gitbook/assets/image%20%28622%29.png)

* 또한 패딩은 위, 아래, 왼쪽, 오른쪽을 선택적으로 추가할 수 있다.
  * 이 때 튜플형태로 입력하게 된다.
  * 먼저 오는 튜플은 UpDown, 두번째 튜플은 LeftRight
  * `padding=((1, 0), (0, 1))`
    * \(1, 0\) =&gt; 위쪽만 패딩
    * \(0, 1\) =&gt; 오른쪽만 패딩

```python
input_tensor = Input(shape=(6, 6, 1))
padded_input = ZeroPadding2D(padding=((1, 0),(1,0)))(input_tensor)
x = Conv2D(filters=1, kernel_size=3, strides=2)(padded_input)
print('x.shape:', x.shape)
```

대칭성을 위해서 보통 커널의 크기는 홀수이다

* 3x3, 5x5, 7x7
* 최근의 CNN은 대부분 3x3 커널을 사용한다.



