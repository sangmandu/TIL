# 8 Sat

## 딥러닝 CNN 완벽 가이드 - Fundamental 편

### Dense Layer기반 Image 분류의 문제점

문제점 1

* MNIST나 Fashion\_MNIST는 이미지 중앙에 분류 대상이 존재하고 분류 대상 이외에는 모두 배경색이 검은색이다.
* 실제 이미지는 분류 대상이 이미지의 어디에 있을지 모르고 분류 대상 이외에 다른 이미지의 색상이 다양하다

문제점 2

* 이미지의 크기가 커질 수록 너무 많은 Weight가 필요하다
* 500 x 500의 이미지가 은닉층 하나를 거칠 때 Weight 250,000개가 필요하다
  * 은닉층 10개면 250M 개가 필요하다

문제점 3

* Dense는 이미지의 지역성을 고려하지 않는다



### Feature Extractor와 CNN 개요

Feature Extraction

* 이미지의 특징이 될 만한것을 뽑아내는 것
* 전통적인 머신러닝은 이것을 사람이 직접했는데 딥러닝에서는 이것을 자체적으로 하게 된다.

Layer 별 Feature

* Layer가 깊어질수록 추상적인 특징을 뽑아내게 된다
  * 깊어질수록 추상화의 추상화
* 초기 Layer일수록 구체적이고 단순한 특징을 뽑아낸다
  * 점, 선, 면 등

CNN 구조

* CNN은 Feature Extractor와 Classifier로 구성이 된다
* Conv, Pool, Activation은 모두 F.E 이다.



### 컨볼루션\(Convolution\) 연산 이해

CNN은

* Classification에 맞는 최적의 Feature를 추출하고
* 최적의 Feature 추출을 위한 최적의 Weight 값을 계산하고
* 최적의 Feature 추출을 위한 필터값\(필터 W\)을 계산한다

이미지 필터

* 보통 정방 행렬을 원본 이미지에 순차적으로 슬라이딩 해가면서 새로운 픽셀값을 만들면서 적용한다
  * 블러 피터 같은 경우는 평균을 내서 더한다 =&gt; 이미지가 흐려지는 효과

Convolution 연산

* 강의에 설명이 너무 잘되어있음

![](../../.gitbook/assets/image%20%28568%29.png)



### 커널\(Kernel\)과 피처맵\(Feature Map\)

필터와 커널의 구분

* CNN에서 필터와 커널은 거의 혼용되어서 사용된다
* 명확히는 필터는 여러개의 커널로 구성되어 있다
* 커널은 필터내에서 서로 다른 값을 가질 수 있다
* `conv_out_01=Conv2D(filter=32, kernel_size=3)(input_tensor)`
  * fkernel의 크기는 3 \* 3
  * kernel의 개수는 32개

커널 사이즈 특징

* 보통 커널은 정방행렬이다.
* Kernel size라고 하면 면적\(가로x세로\)을 의미한다
  * 커널 사이즈의 크기가 크면 클 수록 입력 Feature Map\(또는 원본 이미지\)에서 더 많은\(또는 더 큰\) Feature 정보를 가져올 수 있다
  * 하지만 큰 사이즈의 커널은 훨씬 더 많은 연산량과 파라미터가 필요하다
* Receptive Field, 수용장
  * 입력에서 Feature를 만드는 영역의 기본 크기

CNN의 필터 값

* 일반적으로 Vision영역에서는 특정 필터를 스스로 만들거나 기존에 설계된 다양한 필터를 선택하여 이미지에 적용한다
* 딥러닝의 CNN은 최적의 필터값을 학습을 통해 스스로 최적화함



### 스트라이드\(Stride\)와 패딩\(Padding\)

스트라이드

* 영어 단어 뜻은 `거닐다`
* Filter를 적용할 때 Sliding Window가 이동하는 간격을 의미
* 기본은 1이지만 2로 설정하게 되면 Feature map의 크기를 절반정도로 줄인다.
  * stride를 키우면 공간적인 feature 특성을 손실할 가능성이 높지만 불필요한 특성을 제거하는 효과를 가져올 수도 있고 Convolution 연산 속도를 향상 시킬 수 있다

패딩

* 이미지의 크기\(또는 Feature map\)가 작아지는 것을 막기 위한 기법
* Filter 적용 전 보존하려는 Feature map 크기에 맞게 입력의 좌우 끝과 상하 끝에 열과 행을 추가한다
* `Conv2D(padding='same')` 을 하면 feature map을 유지할 수 있고 `Conv2D(padding='valid')` 를 하면 별도의 패딩을 적용하지 않는다
* 수를 무엇으로 채울지에 대해 여러가지 방법이 있다
  * 0므로 모두 채우는 Zero Padding
    * 모서리 주변값이 0이 되어 노이즈가 약간 증가되는 우려도 있을 수 있지만 큰 영향은 없다



### 풀링\(Pooling\)

* Conv가 적용된 Feature map의 일정 영역 별로 하나의 값을 추출하여 Feature map의 사이즈를 줄이는 기법
  * 왜냐하면 filter가 적용할 때 중복되서 적용되는 픽셀값들이 있다.
  * 보통 Pooling크기와 Stride를 동일하게 부여하여 모든 값이 한번만 처리 될 수 있도록 한다

특징

* feature들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화시켜준다
* feature map의 크기를 줄이므로 computation 연산 성능을 향상시킨다
* max pooling의 경우 보다 sharp한 feature를 추출하고 average의 경우 smooth한 feature를 추출한다
* 일반적으로는 sharp한 feature가 classification에 유리해서 max pooling을 많이 사용한다

stride/padding/pooling

* stride를 증가시키는것과 pooling 모두 출력 feature map의 크기를 줄이는데 사용한다
* pooling의 경우 특정 위치의 feature값이 손실 되는 이슈로 최근에는 사용하지 않음
* 최근 발표되는 논문에서 stride로 feature map 크기를 줄이는 것이 pooling 보다 더 나은 성능을 보인다는 연구 결과를 발표하기 시작했다
* ResNet부터는 최대한 Pooling을 자제하고 Stride를 이용하여 Network를 구성하는 경향이 강해진다



