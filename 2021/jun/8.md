# 8 Tue

## 현업 실무자에게 배우는 Kaggle 머신러닝 입문

### 랜덤 포레스트\(Random Forest\) 소개

랜덤 포레스트

* 분류, 회구 분석등에 사용되는 앙상블 학습 방법의 일종
  * 앙상블은 여러개의 악기가 합주하는 것을 의미\(아래 설명 참조\)
* 훈련 과정에서 구성한 다수의 결정 트리로부터 부류\(분류\) 또는 평균 예측치\(회귀 분석\)를 출력함으로써 동작한다
* 전체 데이터의 일부를 샘플링한 서브 데이터를 이용해서 학습시킨 여러개의 결정트리의 예측값들간에 보팅을 통해서 최종 출력값을 만들어내는 기법

앙상블 러닝

* 앙상블 러닝은 여러 개의 분류기의 예측 결과값 간의 투표를 통해서 최종 결과값을 만들어내는 기법
* 앙상블 러닝을 이용하면 더욱 좋은 예측 성능을 기대할 수 있다
* 분류기는 같은 모델로, 다른 모델로 해도 상관이 없다
* 투표 방식은 다수결 투표로 할 수도 있고 평균값을 취할 수도 있다

랜덤 포레스트의 장단점

* 장점
  * 많은 하이퍼 파라미터 튜닝을 거치지 않아도 일반적으로 안정적인 좋은 성능을 발휘한다
  * 병렬 처리를 이용해서 여러개의 트리를 한번에 학습시킬 수 있다
* 단점
  * 학습 시간이 상대적으로 오래 걸린다

Estimaotr

* `sklearn.ensemble.RandomForestClassifer`
* `sklearn.ensemble.RandomForestRegressor`



### DataFrame의 .replace\(\) 함수

`df.replace()`

* 컬럼의 값을 원하는 형태로 변경하는 함수
* ex
  * `df['RainToday'].replace({'No':0, 'Yes':1}, inplace=True)`



### 범주형\(Categorical\) 데이터 처리하기 - One-hot Encoding

Integer Encoding

* 범주형 값을 졍수형 값으로 매칭해 변환하는 것
* ex
  * dog : 1
  * cat : 2

One-hot Encoding

* 범주형 값을 특정 인덱스가 1이고 나머지 부분은 0의 값을 가진 Binary value로 표현하는 것
  * dog : \[1, 0\]
  * cat : \[0, 1\]
* `pd.get_dummies(df, prefix=)` 를 통해 원핫 인코딩 구현 가능

정수 인코딩은 의미 없는 정수 값에 대해 의미를 부여할 수 있는 오해의 여지가 있음





### Rain in Australia 데이터셋 소개

* 호주 기상청에서 2010년에 발표한 데이터셋
* Target Value : Yes\(tommorow rain\) or No\(tommorow don't rain\)
* Binary Classification
* Number of data : 145,460



### Random Forest Classifier로 내일 비가 올지 안올지 예측해보자 - Rain in Australia 데이터셋

* Input data : 22 Dimension \(Rain Tommorow 컬럼 제외\)
* Target data : RainTommorow 컬럼
  * YES : 내일 비옴
  * NO : 내일 비 안옴
* Estimator
  * DecisionTreeClassifier
  * RandomforestClassifier
* 추가적인 적용기법
  * Data Cleansing : 결측치 처리



#### 결측치의 존재 여부

* `df.info()` 를 통해 전체 데이터의 개수와 컬럼 별 데이터의 개수를 알 수 있다.
* `전체 데이터 개수 != 컬럼 별 데이터 개수` 일 경우 결측치가 존재하는 것을 알 수 있다.
  * 왜냐하면 결측치는 개수로 세지지 않기 때문이다
* 또한, `df.isnull().mean().sort_values()` 를 통해 확인할 수 있다.

![](../../.gitbook/assets/image%20%28542%29.png)

#### 

#### 결측치 처리

* `df.dtypes == 'object'` 이면 categorical data이다.
  * 결측치를 주로 최빈값으로 채워준다.
* `df.dtypes != 'object'` 이면 numerical data이다.
  * 결측치를 주로 평균이나 중앙값으로 채워준다.
* 또한 `label` 값, 즉 `RainTommorrow` 가 결측치가 있는 경우는 임의로 채우기가 어려우므로 drop 한다
  * `df.dropna(how='any', inplace=True)`
  * 이 때 `how='any'` 는 결측치가 하나라도 존재하면 해당 로우를 제거하라는 의미이다.



### 이상치\(Outlier\) 제거 기법

* 이상치란 다른 데이터와 크게 다른 값을 갖는 데이터를 의미한다.
* ex\) 키가 크면 주로 몸무게도 크다. 이 때의 이상치는 키는 작은데 몸무게가 크다.
* 이상치는 모델의 학습에 방해가 될 수 있기 때문에 이상치 제거는 머신러닝 모델의 성능향상에 도움이 된다. 



#### 이상치 제거 기법 - IQR

* Inter Quantile Range
* `IQR = Q3 - Q1`
* `data < Q1 - 1.5*IQR`  or `data > Q3 + 1.5*IQR` 일 경우에 이상치로 판단하고 제거한다.
* Seaborn의 boxpot\(\)이 IQR를 그려준다.



#### 이상치 제거 컬럼 선택

* 이상치를 제거해줘도 결과값이랑 크게 관련이 없으면 모델의 성능 향상에 큰 변화가 없다.
* 따라서, 결과값과 상관관계가 있는 컬럼의 이상치를 제거하는 것이 좋다



#### 구현

```python
def outler(data, columns):
    for column in columns:
        Q1 = data[column].quantile(0.25)
        Q3 = data[column].quantile(0.75)
        IQR = Q3 - Q1
        
        ...
        
    return outlier_index
```



### Outlier 제거를 통해 Random Forest Classifier 성능 향상 시켜보기 - Rain in Australia 데이터셋

이전과 동일하며, 위의 구현 처럼 아웃라이어 데이터의 인덱스만 받아서 drop해준다.

성능이 좋아지는 결과.



