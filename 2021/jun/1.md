# 1 Tue

## How to Win a Data Science Competition: Learn from Top Kagglers

### 1Weeks : Introduction & Recap

### About University

생략



### Introduction

생략



### Rules

* 유료 강의이기 때문에 최소한의 내용만을 구성하려고 한다. 
  * 근데 원래 최소한의 내용만 쓴다 나는. 다른 꼼꼼하고 자세한 블로거들과는 다르다. 그래서 TIL을 써도 알맹이가 없을 때가 많은 단점. 그만큼 시간은 안걸린다는 장점. 이 얘기를 왜 여기서 했지
* 제출물에 대해서 20% 이상 다른 사람의 제출물의 내용을 참고하지 말라고 하기 때문에 자력으로 하라는 듯 보인다.



### Meet your lecturers

* 선생님들의 엄청난 스펙
* 
### Course overview

수업 목표와 숙제, 일정에 관한 전반적인 설명



### Comptetition Mechanics

대회는 다음과 같이 구성된다

* Data 
  * 주최측에서 트레이닝 자료로 우리에게 제공된다. 우리는 답을 찾기 위해 이 데이터를 사용한다.
  * 데이터는 다양한 형태로 표현된다. ex\) csv, ssv, etc
  * 또한 주어지지 않더라도 모델의 성능을 높이기 위해 다른 데이터 셋\(ex 공공 이미지 데이터 셋\)을 사용할 수 있다. 이 부분은 각 대회의 규칙을 꼭 참고해야 한다.
* Model
  * 데이터를 답으로 바꿔주는 매개체
  * 높은 예측성과 재현성을 가지고 있어야 한다.
* Submission
  * 예측값을 서버로 보내는 과정. 제출하는 과정.
  * 대부분은 예측값만 보내고 소스나 모델등은 필요하지 않지만 도전자들이 코드를 공개하기도 한다.
* Evaluation
  * 모델이 얼마나 좋은지를 평가하는 기능으로 정의된다.
  * 예측과 올바른 답에 대해 얼마나 잘 작동하는지 평가한다. 이것이 정확성
  * 경연에서 쓰이는 척도는 정확성 뿐만 아니라, Logistic loss, AUC, RMSE, MAE 등 다양하므로 최적화하고자 하는 수치를 잘 파악해야 한다.
* Leaderboard
  * 참가한 모든 팀의 성과를 보여주는 것이다.
  * 보통 가장 높은 점수의 결과를 보여주며 참가자가 무수히 많은 제출로 테스트 데이터에 과적합 되는것을 막기위해서 공개된 테스트 데이터와 숨겨진 테스트 데이터로 나뉘며 후자만이 최종 심의로 논의된다.



### Kaggle Overview \[screencast\]

캐글의 구성에 대한 전반적인 설명



### Real World Application vs Competitions

캐글의 대회는 단지 모델링에 초점이 맞쳐져 있다. 따라서 실제로 더 복잡한 현실 문제를 다루는 것보다 간단하다. 형식화, 배치, 테스트라는 질문을 다루지 않는다.

* 알고리즘은 단지 도구일 뿐이지 이것이 전부가 아니다. 
* 휴리스틱이나 개인적 분석을 통한 간편 추론도 좋다. 잘못된 것이 아니다. 또한, 복잡한 해결책이나 더 발전된 특징 분석을 무서워하지 말라. 모델의 성능을 위해서 다 필요한 것
* 창의적으로 생각하라. 소스코드를 읽고 수정하는 것을 두려워하지 말라
* 최선을 다하라
* 대회를 즐겨라
* 경험과 흥미를 얻어라

### Practice Quiz

문제 분석과 풀이를 하려고 했지만 문제 자체에서 제공하는 풀이가 너무 명쾌하니 패스.

앞으로도 패스.



### Recap of main ML algorithms

#### Linear Model

* 간단한 차원을 분리하기는 쉽고 명확하지만 복잡한 차원에서 분리하기 어렵다
* Logistic Regression과 SVM을 통해 복잡한 차원에서 분리가 가능하다.
* 선형 모델은 많은 라이브러리에서 구현 가능하다
  * ex scikit learn, vowpal wabbit

#### Tree-based : Decision Tree, Random Forest, GBDT

* 많은 분할이 이루어지는 기법이다.
* 두 공간을 분리하고 분리된 공간에서 또 분리하는 기법이다
* 우승자들이 주로 이 방법을 많이 사용했다.
* 그러나 분할을 너무 많이하면서 선형 종속성을 확보하기 어렵다
  * 이말이 무슨뜻이지;;
    * 아마 과적합이 된다는 뜻 같다
* 결정 트리에서 가지가 너무 많으면 과적합 되기 쉽다. 이 때 랜덤포레스틀 이용하면 과적합이 해소 된다. 예를 들어 특징이 30개인 모델이 있다면 5개씩 특징을 가져서 결정 트리를 그은 그래프를 생성하고 이를 평균내서 새로운 그래프를 만들면 굉장히 매끄러운 그래프가 생성된다.
* GBDT는 Gradient Boosting Decision Tree의 약어이며 학습을 진행할수록 파라미터를 갱신하는 GD와 다르게 새로운 모델을 추가하게 된다.
  * 이것도 자세히는 모르겠다

#### kNN-based methods

* 특정 점과 가까운 점들을 분류. 



### Software/Hardware Requirements

하드웨어적 요구사항

* RAM
  * 보통 32기가를 쓰고 64기가나 128기가를 선호하기도 한다고 한다
    * 내 램은 8기가인데..
* Cores
  * 32개의 코어로도 부족하다고 한다
* Storage
  * 공간이 매우 커야하고 이미지 데이터 같은 경우 SSD의 크기가 중요하다.

소프트웨어적 요구사항

* Python과 R을 주로 쓴다. 파이썬은 엄청난 양의 고품질 오픈소스 라이브러리를 가지고 있다.
* Python 라이브러리
  * Numpy
    * 선형 대수학 라이브러리이다.
    * 유용한 선형 대수학 루틴과 난수 기능이 포함되어 있다
  * Pandas
    * SQL과 유사한 방식으로 데이터를 처리한다
  * Scikit learn
    * 고전적인 기계 학습 알고리즘의 자료
    * 가상 시스팀 지원, 다양한 분류 회귀 및 클러스터링 지원
  * matplotlib
    * 막대 차트, 히스토그램, 분산형 차트와 같은 다양한 시각화를 가능하게 한다
* 그 이외의 패키지
  * Microsoft / LightGBM
  * dmlc XGBoost
  * Keras
  * danielfrg / tsne
* 추가적인 도구
  * vowpal wabbit
    * 놀라운 속도와 큰 데이터 용량을 처리한다

















