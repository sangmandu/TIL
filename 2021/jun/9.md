# 9 Wed

## 현업 실무자에게 배우는 Kaggle 머신러닝 입문

### XGBoost 소개

* eTreme Gradient Boosting
* Gradient Boosting 알고리즘에 추가적인 테크닉들을 결합한 알고리즘
  * 기본 원리는 Gradient Boosting 기반
* 캐글에 상위권 사람들이 많이 사용함
* 병렬적이고 효율적이고 최적화되어있음

#### 앙상블 러닝

* 앙상블 러닝은 크게 Bagging 방식과 Boosting 방식으로 나눌 수 있다.
* Bagging
  * 매번 랜덤하게 샘플을 뽑아서 독립적으로 학습시킨 분류기들의 결과를 종합하는 것
  * 대표적인 방식으로는 랜덤 포레스트가 있다
* Boosting
  * 매번 샘플을 뽑아서 학습시키되, 독립적이지 않고 순차적으로 학습 시킨다
  * 이전 단계에서 오차가 큰 샘플들이 다시 뽑히도록 한다
    * 오차가 큰 샘플들에 가중치를 부여해서 뽑힉 확률이 높도록 한다
  * 대표적인 방식으로는 AdaBoost, XGBoost, GradientBoost 등이 있다.

#### GBM

* Gradient Boosting Machine
* 학습과정에서 파라미터를 최적화하는데 GD 알고리즘을 사용한다.

#### XGBoost의 장점과 단점

장점

* 대부분의 상황에서 안정적이고 좋은 성능
* Feature Enginerring을 많이 적용하지 않아도 안정적인 성능

단점

* 하이퍼 파라미터가 방대해서 튜닝하는 것이 상대적으로 어렵다



### Stroke Preidction 데이터셋 소개

* 나이, 성벼르 고혈압 유무 등을 토대로 뇌졸중을 가진 사람인지 아닌지 예측해보는 데이터셋
* Feature : 12 Dimentsion
* Target Value  : Binary Classification
  * stroke : 뇌졸증
  * not stroke
* 데이터 개수 : 5,110개



ㅈㅂㅈ 



### XGBoost를 이용해서 뇌졸중\(Stroke\) 발생유무 예측해보기 - Stroke Prediction 데이터셋





