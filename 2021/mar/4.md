# 4 Thu

## \[AI 스쿨 1기\] 12주차 DAY 4

### Recommendation system : ML 기반 추천 엔진 - SVD & 딥러닝 추천 엔진

사용자/아이템 기반 협업 필터링의 문제점

* 확장성 : 큰 행렬 계산은 쉽지 않음
  * 아이템 기반은 계산량이 줄어들긴 함
  * Spark를 사용하면 큰 행렬 계산도 얼마든지 가능
* 부족한 데이터
  * 많은 사용자들이 충분한 리뷰를 남기지 않음
* 해결책
  * 모델 기반 협업 필터링 : 머신 러닝 기술을 사용해 평점을 예측. 입력은 사용자 - 아이템 평점 행렬
    * 행렬 분해 방식
    * 딥러닝 방식
    * 계산량을 줄이고 의미있는 정보만 남기는 것이 포인트

행렬 분해 방식

* 협업 필터링 문제를 사용자-아이템 평점 행렬을 채우는 문제로 재정의
  * 다음 그림에서 ?를 채우는 문제
  * 사용자 또는 아이템을 적은 수의 차원으로 사용해서 문제를 간단하게 함

![](../../.gitbook/assets/image%20%28283%29.png)

* 가장 많이 사용되는 행렬 분해 방식
  * PCA, Principal Component Analysis
  * SVD, Singular Vector Decomposition \(또는 SVD ++\)

PCA

* 차원을 축소하되 원래 의미는 최대한 보존
* 정확히 어떤 이유로 축소되었는지 알기는 어려움

SVD

* 2개 혹은 3개의 작은 행렬의 곱으로 단순화 \(소인수 분해와 비슷\)

SVD++

* 넷플릭스 컨테스트 때 고안된 방식
* SVD나 PCA는 완전하게 채워져있는 행렬의 차원을 줄이는 방식인데 SVD++는 희소 행렬이 주어졌을 때 비어있는 셀들을 채우는 방식
  * 채워진 셀들의 값을 최대한 비슷하게 채우는 방식
  * 보통 RMSE의 값을 최소화 하는 방식으로 SGD를 사용



### Recommendation system : 오토인코더 소개

오토 인코더

* 대표적인 비지도학습을 위한 딥러닝 모델
* 입력이 곧 출력이 되는 구조
  * 곧 입력 레이어의 차원과 출력 레이어의 차원이 동일해야함
  * 단, 은닉층의 차원은 입출력 차원의 비해 굉장히 작아야 함
* 데이터의 숨겨진 구조를 발견하면서 노드의 수를 줄이는 것이 목표
  * 입력 데이터에서 불필요한 특징들을 제거한 압축된 특징을 학습하려는 것

케라스

* 파이썬으로 작성된 오픈소스 딥러닝 라이브러리
* 구글에서 시작
* 다양한 프레임워크 위에서 동작하는 상위레벨 딥러닝 프레임 워크
* TensorFlow 2.0에서 케라스가 TF의 상위레벨 라이브러리로 공식 확정됨
* API를 사용하는 세 가지 방법
  * Sequential 모델 API
    * 가장 간단하며 가장 많이 사용
    * 하나의 입력 데이터, 출력 데이터 그리고 순차 레이어 스택을 지원
  * Functional API
    * 레고블록 모델
    * 다중 입력 데이터, 출력 데이터 그리고 임의의 그래프 구조 지원
    * Sequential 모델에 비해 복잡
  * Model Subclassing

