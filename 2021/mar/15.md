# 15 Mon

## 딥러닝 1단계: 신경망과 딥러닝 - 딥러닝 소개

### 소개

* 전기가 우리 사회를 비약적으로 변화시키고 발전시켰듯이 인공지능이 지금 그 자리에 있다

우리가 배울 것

1. 신경망과 딥러닝
   * 고양이를 만드는 전통이 있음 =&gt; 고양이 인식기를 만들 것
2. 심층 신경망
   * 하이퍼 파라미터 튜닝, 정규화, 최적화 등
3. 머신러닝 프로젝트를 어떻게 설계할 것인가
   * 홀드아웃 교차검증 세트
   * end to end deep learning
4. CNN
5. 자연어 처리

### 신경망은 무엇인가?

* 집의 크기에 따른 가격을 예측할 때 집의 크기를 X, 가격을 Y라고 한다면 X를 가지고 Y를 예측하는 함수를 뉴런, 신경망이라고 한다.
  * 이 같은 경우는 굉장히 작은 신경망
  * 이러한 신경망을 굉장히 많이 쌓을 수 있음
* 만약 집의 크기 말고도 침실의 개수도 가격에 영향을 줄 수 있다.
  * 우편번호나, 주변 건물의 땅값도 영향을 줄 수 있는 가능성이 있음

### 지도학습

<table>
  <thead>
    <tr>
      <th style="text-align:center">&#xC785;&#xB825; X</th>
      <th style="text-align:center">&#xCD9C;&#xB825; Y</th>
      <th style="text-align:center">&#xC801;&#xC6A9;</th>
      <th style="text-align:center">&#xC2E0;&#xACBD;&#xB9DD;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:center">&#xC8FC;&#xD0DD; &#xD2B9;&#xC9D5;</td>
      <td style="text-align:center">&#xAC00;&#xACA9;</td>
      <td style="text-align:center">&#xBD80;&#xB3D9;&#xC0B0;</td>
      <td style="text-align:center">&#xD45C;&#xC900;</td>
    </tr>
    <tr>
      <td style="text-align:center">&#xAD11;&#xACE0;, &#xC720;&#xC800; &#xC815;&#xBCF4;</td>
      <td style="text-align:center">&#xAD11;&#xACE0;&#xB97C; &#xD074;&#xB9AD;&#xD560;&#xC9C0;</td>
      <td style="text-align:center">&#xC628;&#xB77C;&#xC778; &#xAD11;&#xACE0;</td>
      <td style="text-align:center">&#xD45C;&#xC900;</td>
    </tr>
    <tr>
      <td style="text-align:center">&#xC774;&#xBBF8;&#xC9C0;</td>
      <td style="text-align:center">&#xD2B9;&#xC815; &#xC815;&#xBCF4;</td>
      <td style="text-align:center">&#xC0AC;&#xC9C4; &#xD0DC;&#xADF8;</td>
      <td style="text-align:center">&#xD569;&#xC131;&#xACF1;</td>
    </tr>
    <tr>
      <td style="text-align:center">&#xC624;&#xB514;&#xC624;</td>
      <td style="text-align:center">&#xD14D;&#xC2A4;&#xD2B8;</td>
      <td style="text-align:center">&#xC74C;&#xC131; &#xC778;&#xC2DD;</td>
      <td style="text-align:center">&#xC21C;&#xD658;</td>
    </tr>
    <tr>
      <td style="text-align:center">&#xC601;&#xC5B4;</td>
      <td style="text-align:center">&#xC911;&#xAD6D;&#xC5B4;</td>
      <td style="text-align:center">&#xAE30;&#xACC4; &#xBC88;&#xC5ED;</td>
      <td style="text-align:center">&#xC21C;&#xD658;</td>
    </tr>
    <tr>
      <td style="text-align:center">
        <p>&#xCC28;&#xB7C9;&#xC774; &#xCC0D;&#xC740; &#xC774;&#xBBF8;&#xC9C0;,</p>
        <p>&#xB808;&#xC774;&#xB354; &#xC815;&#xBCF4;</p>
      </td>
      <td style="text-align:center">&#xB2E4;&#xB978; &#xC790;&#xB3D9;&#xCC28;&#xC758; &#xC704;&#xCE58;</td>
      <td
      style="text-align:center">&#xC790;&#xC728; &#xC8FC;&#xD589;</td>
        <td style="text-align:center">
          <p>&#xD569;&#xC131;&#xACF1;,</p>
          <p>&#xD558;&#xC774;&#xBE0C;&#xB9AC;&#xB4DC;</p>
        </td>
    </tr>
  </tbody>
</table>

구조적 데이터

* 잘 정의되어 있는 데이터
  * 주택 가격을 예측할 때 집의 크기나 방의 개수 등이 해당

비구조적 데이터

* 잘 정의되어 있지 않은 모호한 데이터
  * 고양이 사진이나 음성, 텍스트 등이 해당
* 사람은 비구조적 데이터를 잘 이해하고 컴퓨터는 구조적 데이터를 잘 이해한다
* 신경망의 등장으로 컴퓨터도 비구조적 데이터를 잘 이해할 수 있게되었다.
  * 컴퓨터가 비구조적 데이터를 다룰 때 더 복잡하다.

### 딥러닝의 흥행 이유

* 기존 알고리즘은 데이터의 양이 방대해지더라도 어느 정도 선에서 성능이 수렴한다.
  * 디지털 시대에서 사람들의 동작은 디지털 데이터로 기록되기 때문에 많은 데이터가 쌓이게 된다
* 신경망은 그 크기가 크면 클수록\(일반적으로\) 훈련을 하면 할 수록 아주 높은 성능을 발휘할 수 있게 된다
  * 두 가지 전제조건 : 많은 양의 데이터를 가지고 있어야 하고 규모가 큰 신경망이 존재해야 한다.
  * 언젠가 데이터가 모자랄 수도 있고, 신경망이 너무 커서 학습 시간이 굉장히 오래 걸릴 수도 있다.
* 훈련 세트가 작을 때에는 성능이 알고리즘에 좌우되지만 훈련 세트가 클 때는 많은 데이터와 신경망의 크기에 영향을 받게된다.
* 
레이블 데이터

* \(X, y\)로 표현되는 데이터
* 훈련 세트의 크기는 m으로 나타낸다.

딥러닝의 성능 향상

* 3가지 평가 요소
  * 데이터의 양
  * 컴퓨팅 자원
  * 알고리즘
* 아이디어 =&gt; 개발 =&gt; 실험 과정을 거치며 딥러닝이 형성되고 성능을 측정할 수 있게된다.



## 딥러닝 1단계: 신경망과 딥러닝 - 신경망과 로지스틱회귀

### 이진 분류

* 보통은 데이터를 for문으로 다루지만 딥러닝에서는 그렇게 하지 않음

로지스틱 회귀

* 이진 분류를 위한 알고리즘
  * 고양이 사진이 입력됐을 때 True or False의 Label을 얻기를 원함
  * 입력된 이미지의 특징을 가지고 있는 특징\(특성\) 벡터를 만들어서 입력값 X로 정의
    * 만약 64 by 64 크기의 이미지라면 64 \* 64 \* 3 = 12288의 특징벡터를 가지고 있음

\(x, y\)

* x는 n차원 안에 있는 특징벡터 =&gt; n \* m 의 벡터 \(m \* n 벡터보다 다루기 더 좋음\)
* y는 0 또는 1 \(이진 분류에서는\)
* m개의 training example : $$ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... ,(x^{(m)}, y^{(m)}) $$

### 로지스틱 회귀

* 출력물이 0 또는 1인 이진 분류에 사용되는 알고리즘
* 가장 쉽게 사용하는 함수 식은 y = wx + b
  * 그러나 좋은 예측 함수는 아니다
  * 여기서 y는 0과 1 사이의 확률값인데 비해 wx+ b는 범위제한이 없는 값이다. 이 때 시그모이드 함수를 적용해서 이 wx+b 를 0과 1 사이에 두게 된다.
    * 아주 큰 음수일 경우는 0에 수렴하고 아주 큰 양수일 경우는 1에 수렴한다

### 로지스틱 회귀의 비용함수

* 우리의 목표는 실제값과 매우 근사한 예측값을 구하는 것
  * 물론 실제값과 동일한 예측값을 구해도 좋지!

$$ x^{(i)} $$

* i번째 데이터라는 뜻이다
* 위첨자로 쓰면 이런 의미가 되고 아래첨자로 쓰면 신경망의 층을 의미한다.
* 근데 혼용하는 사람이 많아서, 그 때 그 때 교육자나 저자의 방식을 따라가는 것이 좋음

손실 함수

* 보통 거리최솟값 식\(제곱오차법\)으로 사용하는데 로지스틱 회귀는 지역 최솟값에 빠질 수 있어서 사용하지 않음
* 손실 함수값이 크면 예측값이랑 실제값이랑 차이가 매우 크다는 거임
  * 이 말은 잘못된 예측을 했다는 뜻
  * 따라서 손실 함수값을 작게 만드는 방향으로 설계하는 것
* 우리의 목표는 손실함수 값이 작아지게 하는 인자를 찾는 것
  * 우리가 y = wx + b 로 놓았으니까 w랑 b를 찾는것이 우리의 목표

### 경사하강법

* 우리의 목표는 손실 함수J\(w, b\)를 작게 만드는 거고 따라서 작게 만드는 w랑 b를 찾는것이 우리 목표
* 이걸 찾는 방법이 경사하강법, gradient descent 이다.
* w와 b와 J\(w, b\)에 대한 3차원 그림을 그리면 활처럼 볼록한 3차원 함수가 그려지는데 여기서 가장 아래에 있는 점이 손실 함수가 최소인 위치이고 이 때의 w와 b를 찾으면 된다.
  * 어디서 시작하든 가장 맨 밑으로 내려오게 되있음
  * 근데 3차원은 좀 직관적이지 못할 수 있으니 b를 제외한 w만을 가지고 J에 대한 이차원 함수를 그려서 설명하기도 함
* J가 작아지는 방향으로 이동하기 위해서는 w의 변화가 있어야 하는데 이 때의 식은 다음과 같다.

![](../../.gitbook/assets/image%20%28296%29.png)

* alph는 학습률인데 이는 나중에 더 알아보자
* cost\(W\) = J 라고 생각하면 된다. J에 대한 W의 미분값 =&gt; 즉 J를 고려했을 때 W가 움직이는 변화량이다. 이만큼을 빼줘서 W값이 J가 작아지는 방향으로 바뀌게 한다

### 미분

* 단순 미분 설명이라 패스

### 더 많은 미분 예제

* 이하 동문

### 계산 그래프

* 정방향 전파는 신경망의 출력값을 계산하고
* 역방향은 경사나 도함수를 계산한다.
* J = 3\(a + bc\) 라고 하자
  * u = bc 라고 정의하고
  * v = a+ u 라고 정의하면
  * J = 3v 라고 할 수 있다.

![](../../.gitbook/assets/image%20%28297%29.png)

* 이렇게 나누는 이유는 정방향으로 전파할 때 값들의 흐름을 쉽게 볼 수 있는 이유도 있지만 역방향으로 전파되면서 경사나 도함수를 계산할 때 위와 같이 생각하고 이해하면 쉽기 때문이다.

### 계산 그래프로 미분하기

* J = 3v 이므로 v가 0.001 증가하면 J는 0.003 증가하게 된다.
  * 이는 j에 대한 v의 미분값이 3이기 때문에 변화량에 대해 3배 차이가 있다고 생각하면 된다.
  * 이를 정방향 전파로 볼 수 있다. =&gt; v를 통해 J의 값을 계산하기 때문
* 반대로 J =3v 이므로 J가 0.003 감소하면 v는 0.001 감소하게 된다. 
  * v가 0.001 감소하게 되면 a와 u도 0.001 감소하게 된다
  * 이는 v에 대한 a와 u의 미분값이 1이기 때문에 동등하게 변화한다.
* 우리는 J에 대한 c의 변화량이 궁금한건데, 이를 알수가 없다. 그런데 연쇄법칙을 이용하면 알 수 있게된다.
  * J에 대한 v의 변화량을 알고
  * v에 대한 u의 변화량을 알고
  * u에 대한 c의 변화량을 알기 떄문에 이를 결합하면 J에 대한 c의 변화량이 도출된다.

### 로지스틱 회귀의 경사하강법

* 각각의 가중치에 대한 독립적인 갱신이 가능하다

### m개 샘플의 경사하강법

![](../../.gitbook/assets/image%20%28295%29.png)

