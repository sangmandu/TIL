# 21 Wed

## 딥러닝 CNN 완벽 가이드 - Fundamental 편

### 경사하강법의 이해

* 특징이 많을 수록 가중치의 개수가 늘어나게 된다.
* 가중치의 개수가 늘어나면 고차원 방정식으로 비용 함수가 최소가 되는 가중치 W를 찾기가 어려워진다.
* 고차원 방정식에 대한 문제를 해결하면서 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식

손실함수의 편미분

* W에 대해서 모든 항을 미분할 수 없기 때문에 각 W에 대한 미분을 하게 된다.

가중치와 절편

* 가중치와 절편은 손실 함수의 편미분 값을 갱신하면서 갱신된다.
* 이 때 일정한 계수를 곱해서 갱신되는데 이 계수를 학습률이라고 한다.



### 경사하강법을 이용하여 선형회귀 구현하기 - 01

```text
# 데이터 건수
N = len(target)
# 예측 값. 
predicted = w1 * rm + w2*lstat + bias
# 실제값과 예측값의 차이 
diff = target - predicted
# bias 를 array 기반으로 구하기 위해서 설정. 
bias_factors = np.ones((N,))

# weight와 bias를 얼마나 update할 것인지를 계산.  
w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))
w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))
bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))

# Mean Squared Error값을 계산. 
mse_loss = np.mean(np.square(diff))
```



### 경사하강법을 이용하여 선형회귀 구현하기 - 02

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])
```

* 사이킷런의 minmax스칼라를 이용하면 데이터의 값을 0에서 1 사이로 정규화할 수 있다.

```python
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

model = Sequential([
    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. 
    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. 
    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')
])
# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. 
model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])
model.fit(scaled_features, bostonDF['PRICE'].values, epochs=1000)

```

* 케라스 모듈을 이용하면 모델 구성에 필요한 환경들을 한번에 구성할 수 있다.



### 확률적 경사하강법과 미니배치 경사하강법의 이해

GD, Gradient Descent

* 전체 학습 데이터를 기반

SGD, Stochastic GD

* 전체 학습 데이터 중 한 건만 임의로 선택
  * 매우 큰 데이터 중 하나만 택하기 때문에 의아할 수 있지만 의외로 조금은 성능이 나온다.

Mini - Batch GD

* 전체 학습 데이터 중 특정 크기 만큼 임의로 선택



### 확률적 경사하강법 구현하기

```python
batch_indexes = np.random.choice(target.shape[0], batch_size)
```

* `np.random.choice` 는 무작위로 일정크기만큼 선택하는 함수
* 이 때 `batch_size` 가 1이면 SGD, 2 이상이면 Mini - Batch 이다.



### 미니 배치 경사하강법 구현하기

* 추가적으로 BATCH\_SIZE 만큼을 지정해주는 것 이외에는 확률적 경사하강법과 차이가 없다
* Keras는 반드시 미니 배치 경사하강법을 적용하며 기본 배치 사이즈는 32이다.



### 경사하강법의 주요 문제

Learning Rate

* 너무 작으면 수렴하는데 오랜 시간이 걸림
* 너무 크면 찾지 못하거나 발산됨

전역 최소점, 국소 최소점

* 모든 비용 함수가 Convex와 같은 이상적인 함수가 되지 않음



