# 22 Thu

## 딥러닝 CNN 완벽 가이드 - Fundamental 편

### 심층신경망의 이해와 오차 역전파 개요

입력층 - 은닉층 - 출력층의 3단 구조

* Feed Forward, 순전파 수행
* Backpropagation, 역전파를 수행하면서 가중치 갱신
* 이 두 과정을 반복 수행

결론적으로, 역전파라는 것은 출력층으로부터 미분을 계속 수행하면서 입력층까지의 가중치를 연속적으로 갱신하는 것

### 오차 역전파\(Backpropagation\)의 이해 - 미분의 연쇄 법칙

역전파

* 출력층부터 역순으로 기울기를 전달하여 전체 레이어의 가중치를 갱신

미분의 연쇄 법칙, Chain Rule을 이용



### 합성 함수의 연쇄 결합이 적용된 심층 신경망

연쇄 법칙의 의의

* 아무리 개별 변수가 복잡하게 구성된 함수의 미분이라도 해당 함수가 \(미분 가능한\) 내포 함수의 연속적인 결합으로 되어 있다면 연쇄 법칙으로 쉽게 미분 가능하다



### 오차 역전파\(Backpropagation\)의 Gradient 적용 메커니즘 - 01

Upstream Gradient

* 다른 레이어에서 전해져 온 미분 값

Local Gradient

* 같은 레이어에서 전해져 온 미분 값



### 오차 역전파\(Backpropagation\)의 Gradient 적용 메커니즘 - 02

* 층이 여러개이고, 한 층의 유닛이 여러개라면 여러개의 역전파 미분 값을 합산하게 된다.





### 활성화 함수\(Activation Function\)의 이해

활성화 함수

* Sigmoid Function
  * 이진 분류시 마지막 분류 출력층에 사용
  * 은닉층의 Vanishing Gradient 문제로 더 이상 사용되지 않음
* Softmax
  * 멀티 분류시 마지막 분류 츨력층에 사용
  * Score값을 확률값 0~1로 변환하는데 이 때 모든 개별 출력값의 합이 1이 되도록 매핑한다.
* Hyperbolic Tangent
* ReLU
  * 은닉층에 사용됨
  * 0보다 작으면 출력은 0
  * 0보다 크면 입력값을 출력
  * 다양한 유형의 변형이 존재
    * Leaky ReLU, ELU



### Tensorflow Playground에서 딥러닝 모델의 학습 메커니즘 정리해보기

{% embed url="https://playground.tensorflow.org/" %}

여기서 분류와 회귀를 하는 모델의 은닉층을 조작할 수 있다.



