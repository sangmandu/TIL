# 23 Fri

## 딥러닝 CNN 완벽 가이드 - Fundamental 편

### 손실\(Loss\) 함수의 이해와 크로스 엔트로피\(Cross Entropy\) 상세 - 01

손실 함수

* 손실 함수 = 비용 함수 = 목적 함수
* loss = cost = objective
* 네트워크 모델이 얼마나 학습에 데이터에 잘 맞고있는지에 대한 척도

크로스 엔트로피 로스

* 소프트맥스에 기반한 손실 함수이다.
  * Categorical CE Loss와 Binary CE Loss가 있다.



### 손실\(Loss\) 함수의 이해와 크로스 엔트로피\(Cross Entropy\) 상세 - 02

* 정확도를 비교하는 것 만으로는 예측의 신뢰도를 파악하기 어렵다. 그리고 계단함수 모양으로 정확도가 개선된다. 비용함수는 신뢰도를 파악하기 좋고 연속적인 모양으로 정확도가 개선된다.

크로스 엔트로피와 스퀘어 에러

* 스퀘어 에러는 잘못된 예측에 대해서 상대적으로 높은 비율의 페널티가 부여되어 Loss값의 변화가 심하다.
  * 이 점 때문에 크로스 엔트로피에 비해 수렴이 어렵다
* 또, 아주 잘못된 예측에 대해 낮은 비율의 페널티가 부여된다.



### 옵티마이저\(Optimizer\)의 이해 - Momentum, AdaGrad

최적화, Optimizer

* GD를 최적으로 적용하는 방법
* Loss 함수가 매우 많은 변수와 함수로 이루어져 있기 때문에 수렴하지 않을 가능성도 존재한다. =&gt; 최적화가 필요한 이유

주요 Optimizer

![](../../.gitbook/assets/image%20%28485%29.png)

* Momentum
  * 과거의 있던 가중치를 고려하는 식
  * 감마는 모멘텀 계수라고 하며, 보통 0.9의 값을 가진다
  * 지역 최소점에 빠져서 못나오는 현상을 해결한다
  * 좀 더 빨리 전역 최소점에 도착할 수 있도록 한다.

![](../../.gitbook/assets/image%20%28491%29.png)

* AdaGrad, Adaptive Gradient
  * 그동안 적게 변화된 가중치는 보다 큰 학습률을 적용하고, 많이 변화된 가중치는 보다 작은 학습률을 적용하는 원리
  * 엡실론은 분모가 0이 되는 것을 막기 위함
  * 처음에는 큰 학습률이 적용되지만 최저점에 가까울 수록 학습률이 작아진다
  * Iteration이 증가하면서 분모값이 커져서\(St 값이 커짐\) 학습률이 아주 작게 변환되는 문제점이 있다.
    * 이 문제점을 RMSProp이 해결한다



### 옵티마이저\(Optimizer\)의 이해 - RMSProp, Adam

![](../../.gitbook/assets/image%20%28488%29.png)

* RMSProp
  * 지나치게 학습률이 작아지는 것을 막기 위해 Gradient 제곱값을 단순히 더하는 것이 아니라 지수 가중 평균법으로 구한다.
  * 과거의 영향을 줄일 수 있도록 설정하며 보통 0.9를 적용

![](../../.gitbook/assets/image%20%28492%29.png)

* ADAM, Adaptive Momnet Estimation
  * RMSProp의 방법을 유지하되 모멘텀만 지수 가중 평균법으로 변경

케라스에서의 사용

* SGD
  * `keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)`
  *  lr : 학습률
  * momentum : 0보다 크거나 같은 관성 매개변수
  * decay : 업데이트마다 적용되는 학습률의 감소율
* RMSProp
  * `keras.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None nesterov=False)`
  * rho : gamma 값
* Adam
  * `keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=False)`
  * amsgrad : adam의 변형인 amsgrad의 적용여부를 설정

학습률 최적화 유형

* optimizer 방식
  * weight update시에 학습률을 동적으로 변경
  * 한번 세팅하면 그대로 사용 가능
* Learning Rate Scheduler 방식
  * 에포크 마다 성능 평가 지표 등에 따라 동적으로 학습률을 변경

