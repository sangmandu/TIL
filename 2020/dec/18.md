---
description: TIL
---

# 18 Fri

## 밑바닥부터 시작하는 딥러닝

###  4장 신경망 학습

###  4.4 기울기

 모든 변수의 편미분을 벡터로 정리한 것을 **기울기** 라고 한다. 기울기의 방향은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다. 그러나 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이지, 기울기가 가리키는 곳에 함수의 최솟값이 있는지를 보장할 수는 없다. 

 **경사법**은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 반복적으로 함수의 값을 점차 줄이는 것이 기계학습을 최적화하는 데 흔히 쓰는 방법이며 신경만 학습에 많이 사용한다. 경사하강법을 코드로 나타내면 다음과 같다.

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
```

 인수 f는 최적화하려는 함수, init\_x는 초기값, lr은 learning rate를 의미하는 학습률, step\_num은 경사법에 따른 반복 횟수를 뜻한다.

 학습률 같은 매개변수를 **하이퍼 파라미터**라고 한다. 이는 가중치와 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼 파라미터는 사람이 직접 설정해야 하는 매개변수이다. 일반적으로 이들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.





 



