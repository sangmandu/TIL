---
description: '210907'
---

# \[ì„ íƒ ê³¼ì œ\] BERT Fine-tuning with transformers

### ë°ì´í„°ì…‹ ë‹¤ìš´ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

```text
# !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
# !tar -xf aclImdb_v1.tar.gz
```

```python
import torch
from transformers import DistilBertTokenizerFast
from transformers import DistilBertConfig
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
from pathlib import Path
from sklearn.model_selection import train_test_split
```

* ìš°ë¦¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ëŠ” Imdb ë¼ëŠ” ë°ì´í„°ì…‹ì´ë‹¤. IMDbëŠ” Internet Movdi Databaseì˜ ì¤€ë§ë¡œ ì˜í™”, ë°°ìš°, ë“œë¼ë§ˆ, ë¹„ë””ì˜¤ ê²Œì„ ë“±ì— ê´€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì˜¨ë¼ì¸ ë°ì´í„°ë² ì´ìŠ¤ì´ë‹¤. 2014ë…„ 8ì›” 1ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜í™” ì•½ 3ë°±ë§Œê±´, ì¸ë¬¼ ì •ë³´ ì•½ 6ë°±ë§Œê±´ì„ ì†Œìœ í•˜ê³  ìˆë‹¤. 
* ìŠ¤íƒ í¬ë“œ ëŒ€í•™êµì—ì„œ 2011ë…„ì— ë‚¸ [ë…¼ë¬¸](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)ì—ì„œ ì´ ë°ì´í„°ë¥¼ ì†Œê°œí•˜ì˜€ê³  ë…¼ë¬¸ì—ì„œëŠ” í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 50ëŒ€50ë¹„ìœ¨ë¡œ ë¶„í• í•˜ì—¬ 88.89%ì˜ ì •í™•ë„ë¥¼ ì–»ì—ˆë‹¤ê³  ì†Œê°œí–ˆë‹¤.
* í”íˆ ì˜í™”ì— ëŒ€í•œ ë¦¬ë·° ë°ì´í„°ë¥¼ í†µí•´ ê°ì„±ì„ ë¶„ë¥˜í•˜ëŠ” ëª©ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì´ë‹¤.
* ì´ì— ëŒ€í•œ [Text Classification ë²¤ì¹˜ë§ˆí‚¹](https://paperswithcode.com/sota/text-classification-on-imdb)ë„ ì´ë£¨ì–´ì§„ë‹¤.

### Split

```python
def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in ["pos", "neg"]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is "neg" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')
```

* `read_imdb_split` 
  * ê²½ë¡œë¥¼ ì¸ìë¡œ ì…ë ¥ë°›ëŠ”ë‹¤. `pathlib` ëŠ” íŒŒì´ì¬ 3.4ë¶€í„° ì¶”ê°€ëœ ë‚´ì¥í•¨ìˆ˜ë¡œ ì´ì „ì—ëŠ” `os` ëª¨ë“ˆì„ ì‚¬ìš©í–ˆê³  ì´í›„ì—ëŠ” íŒŒì¼ìœ„ì¹˜ ì°¾ê¸°, íŒŒì¼ ì…ì¶œë ¥ê³¼ ê°™ì€ ë™ì‘ì„ í•œë‹¤. `os` ëª¨ë“ˆê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ê°€ ìˆë‹¤.
    * `os` ì™€ ë‹¬ë¦¬ `pathlib` ëŠ” íŒŒì¼ì‹œìŠ¤í…œ ê²½ë¡œë¥¼ ë‹¨ìˆœí•œ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ ê°ì²´ë¡œ ë‹¤ë£¬ë‹¤.
    * ì´ë ‡ê²Œ ë˜ë©´ì„œ `/` ë¼ëŠ” ê³„ì¸µ êµ¬ë¶„ ë¬¸ìë¥¼ ê²½ë¡œ êµ¬ë¶„ ë¬¸ìë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤. ì¦‰, ì—°ì‚°ìë¥¼ ìƒˆë¡­ê²Œ ì •ì˜í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤ëŠ” ì´ì ì´ ìƒê²¼ë‹¤.
    * ë¬´ìŠ¨ ë§ì´ëƒë©´, ì´ì „ì—ëŠ” dir1ê³¼ dir2ì™€ dir3ë¥¼ ì—°ê²°í•˜ë ¤ë©´ `os.path.join(dir1, dir2, dir3)` ì™€ ê°™ì´ ì‘ì„±í–ˆì–´ì•¼ í•˜ëŠ”ë° `path` ë¥¼ ì‚¬ìš©í•˜ë©´ `dir1 / dir2 / dir3` ì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ì—°ì‚°ìë¡œ í‘œí˜„í•  ìˆ˜ ìˆê²Œëœë‹¤.
    * `pathlib` ì˜ `Path` ëŠ” ì£¼ì–´ì§„ ê²½ë¡œë¥¼ ê°ì²´í™”í•œë‹¤.
    * `os` ëª¨ë“ˆì„ ì‚¬ìš©í•  ë•ŒëŠ” `os.listdir` ë˜ëŠ” `glob` ë¥¼ ì´ìš©í•´ì„œ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” íŒŒì¼ë“¤ì„ ë¦¬ìŠ¤íŒ…í–ˆëŠ”ë°, `pathlib` ì—ì„œëŠ” `iterdir` ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì´ ë•Œ ë¦¬ìŠ¤íŒ…ëœ ì›ì†Œë“¤ë„ ëª¨ë‘ `pathlib` ê°ì²´ì´ë‹¤.
    * ë˜, ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œëƒë©´ ê¸°ì¡´ì˜ ì…ì¶œë ¥ê³¼ ë‹¬ë¦¬ `pathlib` ì˜ ì…ì¶œë ¥ì€ ë²ˆê±°ë¡­ê²Œ íŒŒì¼ì„ ì—´ê³  ë‹«ì„ í•„ìš”ê°€ ì—†ë‹¤. íŒŒì¼ì„ ì—´ì„ ë•ŒëŠ” `read_text()` ë¥¼, ì“¸ ë•ŒëŠ” `write_text()` ë¥¼ ì‚¬ìš©í•œë‹¤.
    * [ìì„¸íˆ ì•Œì•„ë³´ê¸°](https://brownbears.tistory.com/415) / [ìì„¸íˆ ì•Œì•„ë³´ê¸°2](https://ryanking13.github.io/2018/05/22/pathlib.html)
  * ì£¼ì–´ì§„ ê²½ë¡œì— ìˆëŠ” íŒŒì¼ë“¤ì„ ì½ì–´ ë¼ë²¨ë§ í•˜ëŠ” ì‘ì—…ì„ ê±°ì³ textsì™€ labelsë¡œ ë°˜í™˜í•œë‹¤.
    * ì‚¬ì‹¤ 8ë²ˆ ë¼ì¸ì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì¸ë°, ëª¨ë“  ë°˜ë³µë¬¸ë§ˆë‹¤ ifë¬¸ì„ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì²«ë²ˆì§¸ `label_dir` ì„ ê²°ì •í•˜ëŠ” ë°˜ë³µë¬¸ì„ `enumerate` ë¡œ ì‘ì„±í•´ì„œ ì¸ë±ìŠ¤ë¥¼ ë¼ë²¨ê°’ìœ¼ë¡œ ì£¼ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤.

```python
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)
```

* train datasetì„ 8:2 ë¹„ìœ¨ë¡œ train/valid ë¡œ ë‚˜ëˆ„ì–´ì¤€ë‹¤.

### Tokenizer

```python
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
```

* `DistilBertToeknizerFast`
  * DistilBertì˜ ê³µì‹ ë¬¸ì„œëŠ” [ì—¬ê¸°](https://huggingface.co/transformers/model_doc/distilbert.html)
  * distilì€ ì¦ë¥˜ë¼ëŠ” ëœ»ìœ¼ë¡œ ì•¡ì²´ ìƒíƒœì˜ í˜¼í•©ë¬¼ì„ ë¶„ë¦¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê¸°ì¡´ì˜ BERTë³´ë‹¤ ê°€ë³ê³ , ì €ë ´í•˜ë©´ì„œ ë¹ ë¥¸ ë²„ì „ìœ¼ë¡œ ê³ ì•ˆë˜ì—ˆê¸° ë–„ë¬¸ì— BERTì—ì„œ ë¬´ê±°ìš´ í˜¼í•©ë¬¼ë“¤ì„ ì œê±°í–ˆë‹¤ëŠ” ëœ»ì—ì„œ DistilBert ë¡œ ì´ë¦„ì„ ë¶™ì¸ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤.
    * BERTëŠ” Bidirectional Encoder Representations from Trasnformersì˜ ì•½ì–´ë¡œ ë‘ ê°œì˜ ë¬¸ì¥ì„ ì…ë ¥ë°›ì€ í›„ì— ì´ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë˜ëŠ” ëª¨ë¸ì´ë‹¤. ê·¸ë˜ì„œ 50:50 ë¹„ìœ¨ë¡œ ì‹¤ì œ ì´ì–´ì§€ëŠ” ë‘ ê°œì˜ ë¬¸ì¥ê³¼ ëœë¤ìœ¼ë¡œ ì´ì–´ë¶™ì¸ ë‘ ê°œì˜ ë¬¸ì¥ì´ í›ˆë ¨ ë°ì´í„°ë¡œ ì œê³µëœë‹¤.
  * Tokenizerì˜ ê³µì‹ ë¬¸ì„œëŠ” [ì—¬ê¸°](https://huggingface.co/transformers/main_classes/tokenizer.html)
  * ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ì„œ ê°€ëŠ¥í•œ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì†ë„ê°€ ë¹ ë¦„ì„ ê°•ì ìœ¼ë¡œ ë‚´ì„¸ìš°ê³  ìˆëŠ” í† í¬ë‚˜ì´ì €ì´ë‹¤.
  * ê¸°ë³¸ ë²„ì „ê³¼ -Fast ë²„ì „ì´ ìˆìœ¼ë©° ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë™ ë°©ì‹ì€ ë™ì¼í•˜ë‚˜ í›„ìëŠ” í† í°ê³¼ ì›ë˜ ë¬¸ìê°„ì— ë§¤í•‘ì„ ì¢€ ë” ë°œì „ëœ ë°©ë²•ìœ¼ë¡œ í•œë‹¤ê³  í•œë‹¤.\(?\)
    * ì‘ë™ ë°©ì‹ì´ ì—„ì²­ ë™ì¼í•œ ê²ƒì€ ë˜ ì•„ë‹ˆë¼ê³  í•œë‹¤.
    * [ì—¬ê¸°](https://discuss.huggingface.co/t/difference-betweeen-distilberttokenizerfast-and-distilberttokenizer/5961/2)



```python
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)
```

* ê°ê°ì˜ ì¸ì½”ë”©ì„ í† í¬ë‚˜ì´ì§• í•œë‹¤.
* `truncation=True` : ìŒì ˆì´ë‚˜ ì–´ì ˆ ë‹¨ìœ„ë¡œ ìë¥´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•™ìŠµëœ tokenë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ìë¥¸ë‹¤. ì›ë˜ì˜ ì…ë ¥ ë¬¸ì¥ê³¼ ê¸¸ì´ì™€ ê´€ë ¨ì´ ì—†ë‹¤.
* `padding=True` : max\_lengthë³» ì‘ì€ ê¸¸ì´ì˜ sequenceë“¤ì€ 0ìœ¼ë¡œ ë¶€ì¡±í•œ ê¸¸ì´ê¸°ã… ì±„ì›Œì§„ë‹¤. 



### Dataset

```python
class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
val_dataset = IMDbDataset(val_encodings, val_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)
```

* ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ ì–¸í•œë‹¤.

### Train

```python
config = DistilBertConfig.from_pretrained('distilbert-base-uncased',vocab_size=30522, \
                                          max_position_embeddings=512, sinusoidal_pos_embds=False, \
                                          n_layers=6, n_heads=12, dim=768, hidden_dim=3072, \
                                          dropout=0.1, attention_dropout=0.1, activation='gelu')
```

* í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•œë‹¤.



```python
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=100,
)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
```

* `training_args` : trainì„ ìœ„í•œ argumnetë¥¼ ì„¤ì •í•œë‹¤. 
  * `warmup_steps` : í•™ìŠµ ì´ˆê¸°ì— `convergence problem` ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì‹œë„ë“¤ì´ ìˆì—ˆê³  ì§€ê¸ˆê¹Œì§€ ê°ê´‘ë°›ëŠ” ë°©ë²•ì€ `warmup heuristic` ì´ë‹¤. ë§ ê·¸ëŒ€ë¡œ í•™ìŠµ ì´ˆê¸°ì— warm upì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë©° ìì„¸íˆëŠ”, ì´ˆê¸° stepì—ëŠ”ì•„ì£¼ ì¡°ê¸ˆì”©ë§Œ ì¦ê°€í•˜ëŠ” learning rateë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì¸ë° ì´ stepì„ ì •ì˜í•´ì¤€ë‹¤.
  * `weight_decay` : íŠ¹ì • ê°€ì¤‘ì¹˜ê°’ì´ ì˜¤ë²„í”¼íŒ… ë˜ëŠ” ê²ƒì„ ë§‰ê¸°ìœ„í•´ ì ìš©í•˜ëŠ” regularization ê¸°ë²•ì´ë‹¤. 
* `model` : `pretrained` ëœ `DistilBertForSequenceClassiffication` ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.

### Inference

```python
from datasets import load_metric
from torch.utils.data import DataLoader
from tqdm import tqdm

metric= load_metric("accuracy")
test_dataloader = DataLoader(test_dataset, batch_size=128)
model.eval()
for batch in tqdm(test_dataloader):
    batch = {k: v.to("cuda") for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

* metricì€ ì •í™•ë„ë¡œ ì„¤ì •í•˜ë©° test ë°ì´í„°ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•œë‹¤.
* ì´ˆê¸°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ 0.927ì´ ë‚˜ì˜¨ë‹¤.

![](../../../.gitbook/assets/image%20%281069%29.png)



ì œì‹œëœ ê³¼ì œëŠ” 0.92 ì´ìƒì˜ ì •í™•ë„ë¥¼ ê°€ì§€ëŠ” ê²ƒì´ì—ˆëŠ”ë°, ì• ì´ˆë¶€í„° ì˜ ê°€ì§€ê³  ìˆë„¤ ^^? ì¢€ ë” ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•„ë³´ì.



## Try to make performance better

í˜„ì¬ IMDb ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![](../../../.gitbook/assets/image%20%281089%29.png)

* í˜„ì¬ ìš°ë¦¬ëŠ” 2014ë…„ì— ë‚˜ì˜¨ ëª¨ë¸ì˜ ì •í™•ë„ 92.58 ë³´ë‹¤ ë†’ì€ 92.7ì´ë‹¤. í˜„ì¬ ìµœê³  ì„±ëŠ¥ì´ 96.8ì´ê³  ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ 95ì—ì„œ 97 ì‚¬ì´ì— ìˆìœ¼ë‹ˆ 95 ì´ìƒìœ¼ë¡œ ì ìˆ˜ë¥¼ ë†’ì´ë©´ ì˜ ë†’ì¸ ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.

ì´ ì¤‘ 4ìœ„ì— ìˆëŠ” BERTì˜ ë…¼ë¬¸ ì´ë¦„ì´ `How to FIne-Tune BERT for Text Classifcation?` ì´ë‹¤. ì§€ê¸ˆ ë‚´ê°€ í•´ì•¼í•  ì§ˆë¬¸ì´ë„ê¹Œ? ë°”ë¡œ ì½ì–´ë³´ì.

![](../../../.gitbook/assets/image%20%281079%29.png)



ë³¸ë¬¸ì—ì„œëŠ” 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•œë‹¤.

![](../../../.gitbook/assets/image%20%281083%29.png)

ê°ê° ì‚´í´ë³´ì.

### Fine-Tuning Strategies

![](../../../.gitbook/assets/image%20%281078%29.png)

ê·¸ë§Œ ì•Œì•„ë³´ì...







