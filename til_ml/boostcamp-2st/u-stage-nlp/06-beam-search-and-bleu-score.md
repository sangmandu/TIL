---
description: '210908'
---

# \(06강\) Beam Search and BLEU score

## 2. Beam search

자연어 생성 모델에서 Test 단계에서 좋은 품질의 생성 결과를 얻게하는 기법이다.

### Greedy decoding

우리는 현재 스텝에서 바로 다음단어를 예측하는데, 매 타임스텝마다 가장 높은 확률을 가지는 단어를 택해 디코딩을 하게된다. 이를 Greedy decoding이라고 한다. 전체 문장을 보고 다음단어를 예측하는 방식이 아닌, 매 타임스텝에서 다음 단어를 예상하는 방식이다.

만약 테스트 단계에서, 단어를 잘못 예측해서 다음 타임스텝부터 예측이 계속 틀리게 된다면 어떻게 해야될까?

### Exhaustive search

![](../../../.gitbook/assets/image%20%281109%29.png)

우리는 이 확률값이 높은 단어를 선택하게 된다. 이 확률은 joint distribution으로 나타내지는데, 현재 우리의 방식은 P\(y1\|x\)가 제일 높은 y1을 택하는 방식이다. 그러나, 전체적으로 봤을 때는 P\(y1\|x\)가 높더라도 y1을 택한 뒤의 확률들은 낮을 수 있고, P\(y2\|x\)가 낮더라도 y2를 택한 뒤에 확률들은 높을 수 있다.

그러나, 모든 경우의 수를 다 확인하려면 vocabulary size V와 time step T에 대하여 $$ V^T $$만큼의 경우의 수를 고려해야 하고, 이는 매우 높은 시간복잡도를 가지게 된다. 그래서 등장하는 차선책법이 Beam Search 이다.

### Beam search

핵심 아이디어는 디코더의 매 time step마다 우리가 정해놓은 k개의 가지수를 고려하는 것. 그리고 이 k개의 candidate 중에서 확률이 가장 높은 것으로 결정한다. k는 beam size라고 부르며 일반적으로 5~10의 값을 가진다.

![](../../../.gitbook/assets/image%20%281112%29.png)

* 계산을 작게 하기 위해 log를 취한다.

모든 경우의 수를 다 따지는 것은 아니기 때문에 가장 최적의 솔루션을 제공한다고 말할 수는 없지만, exhaustive search보다는 더 효율적인 방법이다.

![](../../../.gitbook/assets/image%20%281110%29.png)

* 확률값은 0~1의 값을 가지기 때문에 log 함수는 음수의 값을 가지게 되며 확률이 커질수록 0에 가까운 값을 가지게 된다.
* 이 때는 2의 제곱만큼 점점 가지수가 늘어나는 것이 아니라 항상 beam size만큼의 높은 확률을 가진 leaf에서만 다음 단어를 예측하게 된다. 아래 그림으로 이해할 수 있다.

![](../../../.gitbook/assets/image%20%281117%29.png)

![](../../../.gitbook/assets/image%20%281113%29.png)

* 생성을 끝나는 시점은 &lt;EOS&gt; 를 예측했을 때이다. beam search decoding 방식은 각각의 트리에서 서로 다른 시점에 &lt;EOS&gt; 토큰을 찾게되고 이러한 결과를 저장하게 된다.
* 그러면 빔 서치는 언제 끝날까? 다음과 같은 두 가지 방법이 있다.
  * T라는 최대 타임스텝을 정해서 이 시점까지만 디코딩하도록 한다
  * &lt;EOS&gt; 토큰을 만나 저장된 결과가 최소 n개 이상 있을 경우 멈추도록 한다
* 저장된 결과에서 가장 높은 결합 확률을 가진 가설을 최종 예측값으로 반환한다.
  * 이 때, 각 가설들은 끝나는 시점이 다르기 때문에, 상대적으로 긴 가설은 결합 확률이 낮고, 짧은 가설은 결합 확률이 높다. 
  * 공평하게 비교를 하기 위해서 hypotheses의 길이만큼으로 score를 나누어서 워드 당 평균 확률로 비교하게 된다.



## 3. BLUE score

















