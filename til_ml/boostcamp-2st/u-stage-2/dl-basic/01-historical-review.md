---
description: '210809'
---

# \(01강\) 딥러닝 기본 용어 설명 - Historical Review

## Introduction

딥러닝이라는 학문은 너무 큰 분야라 모든 분야를 다 커버할 수가 없다.

![](../../../../.gitbook/assets/image%20%28789%29.png)

우리 수업에서는, 10번에 걸쳐서 꼭 알아야 하는 내용들만을 다룸

* 교수님이 생각하기에 필수적이라고 생각하는 부분



![](../../../../.gitbook/assets/image%20%28787%29.png)

인공지능

* 인간의 지능을 모방하는 것
* 가장 큰 개념

머신러닝

* 인공지능 안에 있는 분야
* 데이터로 학습을 해서 알고리즘을 구현한 모델

딥러닝

* 머신러닝 안에 있는 분야
* 신경망을 사용하는 모델



#### 딥러닝의 중요요소

* 학습할 데이터
* 데이터를 학습할 모델
* 학습할 모델의 비용함수
* 인자를 조정할 알고리즘

연구를 볼 때 이 4가지를 중점적으로 보면 연구를 잘 이해할 수 있다.



#### Data

데이터는 해결해야할 문제의 유형에 따라 결정된다

* Classification : 이미지를 분류
* Semantic Segmentation : 단순히 이미지를 분류할 뿐만 아니라 각 픽셀별로 어떠한 클래스에 속하는지 분류한다
* Detection : 이미지 내에 클래스에 대해 바운딩박스를 찾는 것
* Pose Estimation : 이미지 내에 2차원 정보 또는 3차원 정보를 찾는 것
* Visual QnA : 이미지를 참고하여 질문이 주어졌을 때 답을 도출해 냈는 것



#### Model

같은 데이터가 주어지고 같은 문제가 주어졌더라도 모델의 종류에 따라 성능이 달라진다.



#### Loss

기준이 되는 비용함수를 정해야 한다

![](../../../../.gitbook/assets/image%20%28786%29.png)

그러나, 각 문제별로 비용함수가 정해져 있는 것은 아니다. 회귀문제를 풀 때 노이즈가 많은 경우에는 에러가 높아지기 때문에 L1 Norm을 사용하는 등의 여러 방법이 있다.

중요한 것은 우리가 풀고자 하는 문제에 대해 Loss 함수가 줄어드는 것이 어떻게 작용되는 지, 그리고 왜 사용하는 지가 중요하다 



#### Optimization Algorithm

![](../../../../.gitbook/assets/image%20%28788%29.png)

신경망의 파라미터를 1차 미분한 정보를 활용한다.

* SGD : 그냥 활용한다
* 그 이외 방법 : 여러 인자들을 곱해서 활용한다

그 외에도 규제화, 드랍 아웃 등을 통해 테스트 데이터에 대한 성능을 높이는 방법들이 있다



## Historical Review

#### 2012 - AlexNet

* 딥러닝을 이용해서 처음으로 수상한 모델



#### 2013 - DQN

* 강화학습방법을 딥러닝에 적용한 것
* 알파고의 시작
* 오늘날의 딥마인드가 있게한 논문



#### 2014 - Encoder / Decoder

* 단어의 연속이 주어졌을 때 다른 언어의 연속으로 바꿔주는 모델



#### 2014 - Adam Optimizer

* 여러 옵티마이저가 있지만 Adam 이 성능이 잘나온다.



#### 2015 - Generative Adversarial Network

* 이미지 생성기와 판별기로 구성되어있따
* 술이 너무 맛없어서 연구를 생각하다가 연구실로 가서 작성한 아이디어

 

#### 2015 - Residual Networks

* 딥러닝의 딥러닝이 있게해준 논문
* 이전에는 신경망의 층이 깊을수록 성능이 안좋다고 했었음
* ResNet이 등장하면서 층을 깊게 쌓아도 성능이 나올 수 있게 되었음
  * 물론 너무 깊으면 다시 성능이 감소한다



#### 2016 - Nothing



#### 2017 - Transformer

* Attention Is All You Need
* 기존의 방법론들을 대체한 모델



#### 2018 - Bert

* Bidirectional Encoder Representations from Transformers
* 딥러닝의 흐름을 바꾼 모델
* 위키피디아 같은 큰 말뭉치를 학습하고 해결하고자 하는 소수의 문제에 접근한다



#### 2019 - BIG Language Models

* GPT-3 같은 Language 모델 등장
  * 다양한 단어와 문장을 만들 수 있음
  * 굉장히 많은 파라미터가 있음
    * 1750억개의 파라미터



#### 2020 - Self Supervised Learning

* 학습 데이터외의 라벨이 없는 Unsupervised 이미지를 학습에 같이 사용하겠다는 아이디어
* 지도학습과 비지도학습의 융합물



