---
description: '210805'
---

# \(AI Math 4강\) 경사하강법 - 매운맛

### 경사하강법으로 선형회귀 계수 구하기



![](../../../../.gitbook/assets/image%20%28748%29.png)

![](../../../../.gitbook/assets/image%20%28741%29.png)

이제 목적식을 최소화하는 베타를 구하는 경사하강법 알고리즘은 다음과 같다

![](../../../../.gitbook/assets/image%20%28750%29.png)

이 때, 목적식 $$ || y - X\beta || _2$$를 최소화하는 것과 $$ || y - X\beta || _2^2$$ 를 최소화 하는 것은 둘 다 같은 문제이다. 따라서 계산을 쉽게 하기 위해 후자를 최소화하는 것으로 선택한다.

![](../../../../.gitbook/assets/image%20%28752%29.png)

2번째 줄 식에서 3번째 줄 식이 되는 과정은 처음에 식을 풀이할 때와 동일. 이렇게 되면 최소화하는 베타를 구하는 경사하강법 알고리즘이 간단해진다.

![](../../../../.gitbook/assets/image%20%28749%29.png)



### 경사하강법은 만능일까?

미분가능하고 볼록한 함수에 대해서 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어있다.

* 그러나 모든 비선형함수가 볼록함수는 아니기 때문에 모든 비선형함수에서 수렴이 항상 보장될 수 없다. 즉, 신경망\(=비선형함수\)에서 경사하강법은 만능은 아니다.



### 확률적 경사하강법

Stochastic gradient descent, 확률적 경사하강법은 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부 활용하여 업데이트한다.

볼록이 아닌 목적식은 SGD를 통해 최적화 할 수 있다

SGD는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는데 도움이 된다. 또, 모든 데이터를 업로드 하면 메모리가 부족한 하드웨어적 문제가 발생한다. 일부를 가지고 사용하면 GPU에서 행렬 연산과 모델 파라미터를 업데이트하는 동안 CPU는 전처리와 GPU에 업로드할 데이터를 준비한다.



