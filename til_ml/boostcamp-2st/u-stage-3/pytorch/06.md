---
description: '210819'
---

# \(06강\) 모델 불러오기

요즘은 Fine tuning이 대세이다.

* 이미 학습된 모델을 가져와서 우리 데이터셋에 맞게 좀 더 학습하고 사용하는 것

### 

### model.save\(\)

* 학습의 결과를 저장하기 위한 함수
* 모델의 형태와 파라미터를 저장할 수 있는 2가지 방법이 있다
* 모델 학습 중간 과정의 저장을 통해 최선의 결과모델을 선택
* 모델을 저장할 때는 pt라는 확장자로 저장한다.

```python
torch.save(model.state_dict(),
            os.path.join(MODEL_PATH, "model.pt")    
```

또는 모델 자체를 pickle로 저장할 수도 있다.

```python
torch.save(model,
            os.path.join(MODEL_PATH, "model_pickle.pt")
```

* `state_dict()` 를 저장하는 대신 `model` 자체를 저장한다.

또, 불러올 때는 다음과 같이 사용하면 된다

```python
new_model = torch.load(os.path.join(MODEL_PATH, 'model.pt'))
```

torchsummary를 사용하면 모델의 정보를 보기 쉽게 볼 수 있다.

```python
from torchsummary import summary
summary(model, (3, 224, 224))
```



### Checkpoints

![](../../../../.gitbook/assets/image%20%28938%29.png)

* 학습의 중간 결과를 저장하여 최선의 결과를 선택한다
* earlystopping 기법이며 loss와 metric 값을 지속적으로 확인하며 저장한다.
  * 일반적으로 epoch, loss, metric을 함께 저장한다

```python
torch.save({
        'epoch': e,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': epoch_loss,
        },
        f"saved/checkpoint_model_{e}_{epoch_loss/len(dataloader)}_\
        {epoch_acc/len(dataloader)}.pt")
        
checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

### 

### Transfer learning

* 다른 데이터셋으로 만든 모델을 현재 데이터에 적용하는 것
* 일반적으로 대용량 데이터셋으로 만들어진 모델로 사용할 수록 성능이 좋다
* 현재 DL에서 가장 일반적인 학습 기법이다
* backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습을 수행함
* NLP는 HuggingFace를 백본으로 굉장히 많이 사용한다



### Freezing

pretrained model을 활용할 때 모델의 일부분을 frozen 시킨다

* 굉장히 많은 기법이 있다

![](../../../../.gitbook/assets/image%20%28937%29.png)









