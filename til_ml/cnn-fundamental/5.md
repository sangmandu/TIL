---
description: '210727'
---

# CNN 모델 구현 및 성능 향상 기본 기법 적용하기

## CIFAR10 데이터세트를 이용하여 CNN 모델 구현 실습 - 01

이미지넷보다 크기가 작아서 학습하는데 오래걸림. 이미지넷은 백만개 정도의 이미지. 그래서 좀 더 크기가 작은 CIFAR 사용. 6만개의 데이터가 있음.

* 이 중 훈련 데이터는 5만개, 테스트 데이터는 1만개

```python
def show_images(images, labels, ncols=8):
    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)
    for i in range(ncols):
        axs[i].imshow(images[i])
        label = labels[i].squeeze()
        axs[i].set_title(NAMES[int(label)])
```

데이터를 이미지로 보여주는 함수.

* figure는 전체 액자를 의미하며 axs는 개별 이미지\(축\)를 의미한다.
* figsize는 액자의 크기.
* nrows는 가로줄 개수
* ncols는 세로줄 개수

라벨 같은 경우는 초기에 \(50000, 1\)과 \(10000, 1\)의 2차원 형태로 존재하므로 1차원으로 스퀴즈 해주는 모습



#### 이미지 전처리

이미지의 각 픽셀값을 255.0 으로 나누어서 0~1사이의 값으로 존재하게 함.

추후에, `sparse categorical crossentropy` 를 알아보기 위해 원-핫 인코딩을 적용하지 않았음



#### CNN 2D

Convolution과 ReLU를 2번 하기 때문에 2D이다. 3번 하면 3D. 이 때 입력 채널은 항상 4차원이어야 한다. \(이미지 개수, 세로, 가로, 채널\)

* 3차원이어도 작동은 하지만, 꼭 4차원으로 입력할 것을 기억하자
* 추후에 predict를 할 때도 4차원으로 입력되어야 한다! 훈련과 테스트의 크기가 동일해야 한다
  * 만약에 1개의 이미지만 predict 할 거라면 1개의 이미지\(3차원\)를 4차원으로 차원 변환 해줘야 함





## CIFAR10 데이터세트를 이용하여 CNN 모델 구현 실습 - 02

```python
x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)

x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)
x = Activation('relu')(x)
```

* Conv2D에서 활성화를 함께 할 수도 있고 3-4번 라인처럼 따로할 수도 있다.



```python
x = MaxPooling2D(pool_size=(2, 2))(x)

x = MaxPooling2D(pool_size=2)(x)

x = MaxPooling2D(2)(x)
```

* 각각의 한줄의 코드는 모두 같은 의미이다.



```python
preds = model.predict(test_images[:32], batch_size=32)
print('예측 결과 shape:', preds.shape)
print('예측 결과:', preds)
```

* 결과는 무조건 2차원 형태의 softmax 결과값이다
* 따라서 예측 클래스는 무조건 `np.argmax` 로 해줘야 한다.

```python
predicted_class = np.argmax(preds, axis=1)
print('예측 클래스 값:', predicted_class)

>>> 예측 클래스 값: [3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6 7 2 4 9 4 2 4 0 9 6 6 5]
```



## CIFAR10 데이터세트를 이용하여 CNN 모델 구현 실습 - 03

필터 사이즈를 3 by 3 으로 할지, 5 by 5 로 할지는 실험적으로 해서 성능을 비교해야 한다. 정해진 골든룰 같은 것은 없다.





## 가중치 초기화\(Weight Initialization\)의 이해와 적용 - 01

#### 좋은 가중치 초기화 조건

* 값이 동일 해서는 안됨
* 충분히 작아야 함
  * 충분히란 얼마만큼?
* 적당한 분산\(또는 표준편차\)을 가져야 함
  * 적당한 양은 어느정도?



```python
numbers = np.random.normal(loc=0.0,scale=1,size=[100, 100])
print(numbers)
print(numbers.mean())
print(numbers.std())
print(numbers.sum())
```

* 표준 정규 분포의 난수를 출력하는 함수이다.
  * loc : location, 평균
  * scale : 표준 편차



## 가중치 초기화\(Weight Initialization\)의 이해와 적용 - 02

#### Xavier Glorot Initialization

![https://www.tutorialexample.com/initialize-tensorflow-weights-using-xavier-initialization-a-beginner-guide-tensorflow-tutorial/](../../.gitbook/assets/image%20%28721%29.png)



#### He Initialization

* Xavier는 Sigmoid에 최적화된 방법이라면 He는 ReLU에 최적화된 방법이다

![](../../.gitbook/assets/image%20%28720%29.png)



그렇지만, 성능은 데이터 바이 데이터. 좋아질 수도 있고 나빠질 수도 있다. 무조건적으로 좋은 것은 아님.

또한, 이후에 배치 정규화를 거치면 따로 가중치 초기화 과정을 거치지 않아도 된다.





## 배치 정규화\(Batch Normalization\) 이해와 적용 - 01

### Feature Scaling

서로 다른 Feature 값들을 동일한 척도로 변환

#### Min-Max Scaling

0에서 1 사이 값으로 변환 : \(X - 최솟값\) / \(최댓값 - 최솟값\)

* \[20, 30, 40\] =&gt; \[0, 0.5, 1\]

#### Z Score 정규화

평균이 0이고 표준 편차가 1인 데이터 세트로 변환 : \(X - 평균\) / 표준편차

* \[20, 30, 40\] =&gt; \[-1.22, 0, 1.22\]
* 평균 : 30, 표준편차 : 8.16

스케일링을 하지 않으면, GD 변화가 타원형으로 이동하게 된다.



### Batch Normalization의 필요성

#### Internal Covariate Shift

신경망 내부의 각 층을 통과할 때마다 입력 데이터의 분포가 조금씩 변경되는 현상

* 많은 뉴런 중 하나가 매우 크거나 작은 값을 가지게 되면 Loss함수가 최적화되기에 어려워진다. 따라서 각 뉴런에서 나오는 값을 스케일링 해줄 필요가 있음.
* 분포가 바뀌면 학습하기가 어려워진다
* 레이어를 거치면서 초기의 데이터 분포가 유지될 필요성을 느낌



## 배치 정규화\(Batch Normalization\) 이해와 적용 - 02

#### Batch Normalization 개요

* 미니 배치의 평균과 분산을 구함
* 각 데이터마다 Z-Score 변환
* 이 후, 스케일링 파라미터를 곱하고 쉬프트 파라미터를 더한다

#### BN의 스케일링과 Shift 파라미터

* 정규화 제약이 너무 제약적이어서 고정적인 값들이 많이 나오게 됨
* 이를 해결하기 위해 적용

일반적으로 BN은 Conv 적용 후, Activation 전에 적용한다

* 따라서 Conv2D에 Activation 빼기.

#### 효과

* 뛰어난 성능 향상 효과
* Regularization 효과
  * 일종의 Dropout 효과이다.
  * 너무 Loss 함수에 휘둘리지 않도록 어느정도의 노이즈 추가 효과를 제공한다
* 가중치 초기화 설정을 크게 신경 쓸 필요가 없어진다.

#### 테스트 데이터 적용시 BN

* 보통은 데이터가 적으므로 정규화 할 데이터가 많지 않음 또는 못함
* 지수 가중 이동 평균을 적용하여 평균과 표준 편차를 구해서 할 수 있다고 함



## 학습 데이터 Shuffle 적용 유무에 따른 모델 성능 비교







## 배치크기 변경에 따른 모델 성능 비교







## 학습율\(Learning Rate\) 동적 변경에 따른 모델 성능 비교







## 필터수와 층\(Layer\) 깊이 변경에 따른 모델 성능 비교







## Global Average Pooling의 이해와 적용







## 가중치 규제\(Weight Regularization\)의 이해와 적용







